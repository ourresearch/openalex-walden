{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Work Embeddings at Scale\n",
    "\n",
    "Generates embeddings for all OpenAlex works using `ai_query` with OpenAI text-embedding-3-small.\n",
    "\n",
    "**Strategy**: Process in batches using SQL, respecting OpenAI rate limits.\n",
    "\n",
    "**Note**: Truncates abstracts to 30K chars (~7500 tokens) to stay under 8192 token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 50000  # Works per batch\n",
    "OUTPUT_TABLE = \"openalex.vector_search.work_embeddings\"\n",
    "SOURCE_TABLE = \"openalex.works.openalex_works\"\n",
    "ENDPOINT_NAME = \"openai-embedding-3-small\"\n",
    "MAX_ABSTRACT_CHARS = 30000  # ~7500 tokens, leaves room for title within 8192 limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current progress\n",
    "progress = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        (SELECT COUNT(*) FROM {OUTPUT_TABLE}) as embedded,\n",
    "        (SELECT COUNT(*) FROM {SOURCE_TABLE} WHERE type != 'dataset' AND abstract IS NOT NULL) as total_with_abstract,\n",
    "        (SELECT COUNT(*) FROM {SOURCE_TABLE} WHERE type != 'dataset') as total_all\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"Progress: {progress['embedded']:,} / {progress['total_with_abstract']:,} works with abstracts\")\n",
    "print(f\"Remaining: {progress['total_with_abstract'] - progress['embedded']:,}\")\n",
    "print(f\"Percent complete: {100 * progress['embedded'] / progress['total_with_abstract']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count works still needing embeddings\n",
    "remaining = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as remaining\n",
    "    FROM {SOURCE_TABLE} w\n",
    "    WHERE w.type != 'dataset'\n",
    "      AND w.abstract IS NOT NULL\n",
    "      AND w.title IS NOT NULL\n",
    "      AND NOT EXISTS (\n",
    "          SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "      )\n",
    "\"\"\").collect()[0]['remaining']\n",
    "\n",
    "print(f\"Works still needing embeddings: {remaining:,}\")\n",
    "print(f\"Estimated batches: {remaining // BATCH_SIZE + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process one batch - with truncation to avoid token limit errors\n",
    "result = spark.sql(f\"\"\"\n",
    "    INSERT INTO {OUTPUT_TABLE}\n",
    "    SELECT \n",
    "        CAST(w.id AS STRING) as work_id,\n",
    "        ai_query(\n",
    "            '{ENDPOINT_NAME}',\n",
    "            CONCAT('Title: ', w.title, '\\n\\nAbstract: ', LEFT(w.abstract, {MAX_ABSTRACT_CHARS}))\n",
    "        ) as embedding,\n",
    "        md5(CONCAT('Title: ', w.title, '\\n\\nAbstract: ', LEFT(w.abstract, {MAX_ABSTRACT_CHARS}))) as text_hash,\n",
    "        w.publication_year,\n",
    "        w.type,\n",
    "        w.open_access.is_oa as is_oa,\n",
    "        true as has_abstract,\n",
    "        current_timestamp() as created_at,\n",
    "        current_timestamp() as updated_at\n",
    "    FROM {SOURCE_TABLE} w\n",
    "    WHERE w.type != 'dataset'\n",
    "      AND w.abstract IS NOT NULL\n",
    "      AND w.title IS NOT NULL\n",
    "      AND NOT EXISTS (\n",
    "          SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "      )\n",
    "    LIMIT {BATCH_SIZE}\n",
    "\"\"\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Batch complete in {elapsed:.1f} seconds\")\n",
    "print(f\"Rate: {BATCH_SIZE / elapsed:.0f} works/second\" if elapsed > 0 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Processing Loop\n",
    "\n",
    "Run this to continuously process batches until complete (or notebook is stopped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_remaining_count():\n",
    "    \"\"\"Get count of works still needing embeddings.\"\"\"\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as remaining\n",
    "        FROM {SOURCE_TABLE} w\n",
    "        WHERE w.type != 'dataset'\n",
    "          AND w.abstract IS NOT NULL\n",
    "          AND w.title IS NOT NULL\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "          )\n",
    "    \"\"\").collect()[0]['remaining']\n",
    "\n",
    "def process_batch():\n",
    "    \"\"\"Process a single batch and return rows inserted.\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {OUTPUT_TABLE}\n",
    "        SELECT \n",
    "            CAST(w.id AS STRING) as work_id,\n",
    "            ai_query(\n",
    "                '{ENDPOINT_NAME}',\n",
    "                CONCAT('Title: ', w.title, '\\n\\nAbstract: ', LEFT(w.abstract, {MAX_ABSTRACT_CHARS}))\n",
    "            ) as embedding,\n",
    "            md5(CONCAT('Title: ', w.title, '\\n\\nAbstract: ', LEFT(w.abstract, {MAX_ABSTRACT_CHARS}))) as text_hash,\n",
    "            w.publication_year,\n",
    "            w.type,\n",
    "            w.open_access.is_oa as is_oa,\n",
    "            true as has_abstract,\n",
    "            current_timestamp() as created_at,\n",
    "            current_timestamp() as updated_at\n",
    "        FROM {SOURCE_TABLE} w\n",
    "        WHERE w.type != 'dataset'\n",
    "          AND w.abstract IS NOT NULL\n",
    "          AND w.title IS NOT NULL\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "          )\n",
    "        LIMIT {BATCH_SIZE}\n",
    "    \"\"\")\n",
    "    return BATCH_SIZE\n",
    "\n",
    "# Main loop\n",
    "total_processed = 0\n",
    "start_time = time.time()\n",
    "batch_num = 0\n",
    "\n",
    "remaining = get_remaining_count()\n",
    "print(f\"Starting continuous processing. {remaining:,} works remaining.\")\n",
    "print(f\"Batch size: {BATCH_SIZE:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "while remaining > 0:\n",
    "    batch_start = time.time()\n",
    "    batch_num += 1\n",
    "    \n",
    "    try:\n",
    "        rows = process_batch()\n",
    "        total_processed += rows\n",
    "        \n",
    "        batch_elapsed = time.time() - batch_start\n",
    "        total_elapsed = time.time() - start_time\n",
    "        rate = total_processed / total_elapsed if total_elapsed > 0 else 0\n",
    "        \n",
    "        # Get updated remaining count every 10 batches (expensive query)\n",
    "        if batch_num % 10 == 0:\n",
    "            remaining = get_remaining_count()\n",
    "        else:\n",
    "            remaining = max(0, remaining - rows)\n",
    "        \n",
    "        eta_hours = remaining / rate / 3600 if rate > 0 else 0\n",
    "        \n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num}: \"\n",
    "              f\"+{rows:,} works in {batch_elapsed:.0f}s | \"\n",
    "              f\"Total: {total_processed:,} | \"\n",
    "              f\"Remaining: {remaining:,} | \"\n",
    "              f\"Rate: {rate:.0f}/s | \"\n",
    "              f\"ETA: {eta_hours:.1f}h\")\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {batch_num}: {e}\")\n",
    "        print(\"Waiting 60s before retry...\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Complete! Processed {total_processed:,} works in {(time.time() - start_time)/3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "    COUNT(*) as total_embeddings,\n",
    "    SUM(CASE WHEN has_abstract THEN 1 ELSE 0 END) as with_abstract,\n",
    "    MIN(created_at) as oldest,\n",
    "    MAX(created_at) as newest,\n",
    "    AVG(SIZE(embedding)) as avg_dims\n",
    "FROM openalex.vector_search.work_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
