{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Work Embeddings at Scale\n",
    "\n",
    "Generates embeddings for all OpenAlex works using `ai_query` with OpenAI text-embedding-3-small.\n",
    "\n",
    "**Token limit handling**: Truncates title to 500 chars and abstract to 5500 chars (~6K total).\n",
    "This guarantees staying under 8192 tokens even for CJK text (1 char â‰ˆ 1 token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 50000  # Works per batch\n",
    "OUTPUT_TABLE = \"openalex.vector_search.work_embeddings\"\n",
    "SOURCE_TABLE = \"openalex.works.openalex_works\"\n",
    "ENDPOINT_NAME = \"openai-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current progress\n",
    "progress = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        (SELECT COUNT(*) FROM {OUTPUT_TABLE}) as embedded,\n",
    "        (SELECT COUNT(*) FROM {SOURCE_TABLE} WHERE type != 'dataset' AND abstract IS NOT NULL) as total_with_abstract\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"Progress: {progress['embedded']:,} / {progress['total_with_abstract']:,} works with abstracts\")\n",
    "print(f\"Remaining: {progress['total_with_abstract'] - progress['embedded']:,}\")\n",
    "print(f\"Percent complete: {100 * progress['embedded'] / progress['total_with_abstract']:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process one batch\n",
    "# Hardcoded truncation: title=500, abstract=5500 chars (safe for all languages)\n",
    "result = spark.sql(f\"\"\"\n",
    "    INSERT INTO {OUTPUT_TABLE}\n",
    "    SELECT \n",
    "        CAST(w.id AS STRING) as work_id,\n",
    "        ai_query(\n",
    "            '{ENDPOINT_NAME}',\n",
    "            CONCAT('Title: ', LEFT(w.title, 500), '\\n\\nAbstract: ', LEFT(w.abstract, 5500))\n",
    "        ) as embedding,\n",
    "        md5(CONCAT('Title: ', LEFT(w.title, 500), '\\n\\nAbstract: ', LEFT(w.abstract, 5500))) as text_hash,\n",
    "        w.publication_year,\n",
    "        w.type,\n",
    "        w.open_access.is_oa as is_oa,\n",
    "        true as has_abstract,\n",
    "        current_timestamp() as created_at,\n",
    "        current_timestamp() as updated_at\n",
    "    FROM {SOURCE_TABLE} w\n",
    "    WHERE w.type != 'dataset'\n",
    "      AND w.abstract IS NOT NULL\n",
    "      AND w.title IS NOT NULL\n",
    "      AND NOT EXISTS (\n",
    "          SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "      )\n",
    "    LIMIT {BATCH_SIZE}\n",
    "\"\"\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Batch complete in {elapsed:.1f} seconds\")\n",
    "print(f\"Rate: {BATCH_SIZE / elapsed:.0f} works/second\" if elapsed > 0 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Processing Loop\n",
    "\n",
    "Run this to continuously process batches until complete (or notebook is stopped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_embedded_count():\n",
    "    \"\"\"Get count of embeddings (fast - small table).\"\"\"\n",
    "    return spark.sql(f\"SELECT COUNT(*) as n FROM {OUTPUT_TABLE}\").collect()[0]['n']\n",
    "\n",
    "def process_batch():\n",
    "    \"\"\"Process a single batch.\"\"\"\n",
    "    # Hardcoded truncation: title=500, abstract=5500 chars\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {OUTPUT_TABLE}\n",
    "        SELECT \n",
    "            CAST(w.id AS STRING) as work_id,\n",
    "            ai_query(\n",
    "                '{ENDPOINT_NAME}',\n",
    "                CONCAT('Title: ', LEFT(w.title, 500), '\\n\\nAbstract: ', LEFT(w.abstract, 5500))\n",
    "            ) as embedding,\n",
    "            md5(CONCAT('Title: ', LEFT(w.title, 500), '\\n\\nAbstract: ', LEFT(w.abstract, 5500))) as text_hash,\n",
    "            w.publication_year,\n",
    "            w.type,\n",
    "            w.open_access.is_oa as is_oa,\n",
    "            true as has_abstract,\n",
    "            current_timestamp() as created_at,\n",
    "            current_timestamp() as updated_at\n",
    "        FROM {SOURCE_TABLE} w\n",
    "        WHERE w.type != 'dataset'\n",
    "          AND w.abstract IS NOT NULL\n",
    "          AND w.title IS NOT NULL\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "          )\n",
    "        LIMIT {BATCH_SIZE}\n",
    "    \"\"\")\n",
    "\n",
    "# Main loop\n",
    "start_time = time.time()\n",
    "batch_num = 0\n",
    "start_count = get_embedded_count()\n",
    "target = 217000000\n",
    "\n",
    "print(f\"Starting from {start_count:,} embeddings\")\n",
    "print(f\"Batch size: {BATCH_SIZE:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "while True:\n",
    "    batch_start = time.time()\n",
    "    batch_num += 1\n",
    "    \n",
    "    try:\n",
    "        process_batch()\n",
    "        batch_elapsed = time.time() - batch_start\n",
    "        \n",
    "        # Check progress every batch\n",
    "        current = get_embedded_count()\n",
    "        added_this_batch = current - start_count - (batch_num - 1) * BATCH_SIZE\n",
    "        \n",
    "        if added_this_batch <= 0 and batch_num > 1:\n",
    "            print(f\"\\nNo new rows added - complete!\")\n",
    "            break\n",
    "        \n",
    "        total_added = current - start_count\n",
    "        total_elapsed = time.time() - start_time\n",
    "        rate = total_added / total_elapsed if total_elapsed > 0 else 0\n",
    "        remaining = target - current\n",
    "        eta_hours = remaining / rate / 3600 if rate > 0 else 0\n",
    "        \n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num}: \"\n",
    "              f\"{batch_elapsed:.0f}s | \"\n",
    "              f\"Total: {current:,} | \"\n",
    "              f\"Rate: {rate:.0f}/s | \"\n",
    "              f\"ETA: {eta_hours:.1f}h\")\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Error: {e}\")\n",
    "        print(\"Waiting 60s before retry...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "print(\"=\"*70)\n",
    "final = get_embedded_count()\n",
    "print(f\"Done! {final:,} total embeddings\")\n",
    "print(f\"Added {final - start_count:,} in {(time.time() - start_time)/3600:.1f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_embeddings,\n",
    "        MIN(created_at) as oldest,\n",
    "        MAX(created_at) as newest\n",
    "    FROM {OUTPUT_TABLE}\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
