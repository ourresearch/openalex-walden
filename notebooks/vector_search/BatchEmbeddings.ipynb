{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Work Embeddings\n",
    "\n",
    "Generates embeddings for all OpenAlex works using OpenAI text-embedding-3-small.\n",
    "\n",
    "Uses OpenAI Python API directly (ai_query only works on SQL warehouses, not job clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openai if needed\n",
    "%pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 100  # OpenAI batch limit\n",
    "BATCHES_PER_COMMIT = 10  # Commit to Delta every N batches\n",
    "OUTPUT_TABLE = \"openalex.vector_search.work_embeddings\"\n",
    "SOURCE_TABLE = \"openalex.works.openalex_works\"\n",
    "MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# Get OpenAI API key from secret scope\n",
    "import os\n",
    "api_key = dbutils.secrets.get(scope=\"openalex\", key=\"openai_api_key\")\n",
    "print(f\"API key loaded (length: {len(api_key)})\")\n",
    "\n",
    "# Quick check - count existing embeddings\n",
    "existing = spark.sql(f\"SELECT COUNT(*) as n FROM {OUTPUT_TABLE}\").collect()[0]['n']\n",
    "print(f\"Existing embeddings: {existing:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import hashlib\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    \"\"\"Get embeddings for a list of texts using OpenAI API.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=MODEL,\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "def get_batch_of_works(limit):\n",
    "    \"\"\"Get a batch of works that need embeddings.\"\"\"\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CAST(w.id AS STRING) as work_id,\n",
    "            w.title,\n",
    "            w.abstract,\n",
    "            w.publication_year,\n",
    "            w.type,\n",
    "            w.open_access.is_oa as is_oa\n",
    "        FROM {SOURCE_TABLE} w\n",
    "        WHERE w.type != 'dataset'\n",
    "          AND w.abstract IS NOT NULL\n",
    "          AND w.title IS NOT NULL\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "          )\n",
    "        LIMIT {limit}\n",
    "    \"\"\").collect()\n",
    "\n",
    "def process_batch(works):\n",
    "    \"\"\"Process a batch of works: get embeddings and return rows to insert.\"\"\"\n",
    "    texts = [f\"Title: {w.title}\\n\\nAbstract: {w.abstract}\" for w in works]\n",
    "    embeddings = get_embeddings(texts)\n",
    "    \n",
    "    rows = []\n",
    "    now = datetime.utcnow()\n",
    "    for work, embedding in zip(works, embeddings):\n",
    "        text = f\"Title: {work.title}\\n\\nAbstract: {work.abstract}\"\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        rows.append((\n",
    "            work.work_id,\n",
    "            embedding,\n",
    "            text_hash,\n",
    "            work.publication_year,\n",
    "            work.type,\n",
    "            work.is_oa,\n",
    "            True,  # has_abstract\n",
    "            now,\n",
    "            now\n",
    "        ))\n",
    "    return rows\n",
    "\n",
    "def insert_rows(all_rows):\n",
    "    \"\"\"Insert rows into Delta table.\"\"\"\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType, BooleanType, TimestampType\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"work_id\", StringType(), False),\n",
    "        StructField(\"embedding\", ArrayType(DoubleType()), False),\n",
    "        StructField(\"text_hash\", StringType(), True),\n",
    "        StructField(\"publication_year\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"is_oa\", BooleanType(), True),\n",
    "        StructField(\"has_abstract\", BooleanType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(all_rows, schema)\n",
    "    df.write.mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "    return len(all_rows)\n",
    "\n",
    "def get_count():\n",
    "    \"\"\"Get current embedding count.\"\"\"\n",
    "    return spark.sql(f\"SELECT COUNT(*) as n FROM {OUTPUT_TABLE}\").collect()[0]['n']\n",
    "\n",
    "# Main loop\n",
    "total_processed = 0\n",
    "start_time = time.time()\n",
    "batch_num = 0\n",
    "last_count = get_count()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting from {last_count:,} embeddings\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Commits every {BATCHES_PER_COMMIT} batches\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "while True:\n",
    "    batch_start = time.time()\n",
    "    batch_num += 1\n",
    "    \n",
    "    try:\n",
    "        # Get works to process\n",
    "        works = get_batch_of_works(BATCH_SIZE)\n",
    "        \n",
    "        if len(works) == 0:\n",
    "            # Flush any remaining rows\n",
    "            if all_rows:\n",
    "                insert_rows(all_rows)\n",
    "            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] No more works to process!\")\n",
    "            break\n",
    "        \n",
    "        # Process batch\n",
    "        rows = process_batch(works)\n",
    "        all_rows.extend(rows)\n",
    "        total_processed += len(rows)\n",
    "        \n",
    "        batch_elapsed = time.time() - batch_start\n",
    "        \n",
    "        # Commit every N batches\n",
    "        if batch_num % BATCHES_PER_COMMIT == 0:\n",
    "            insert_rows(all_rows)\n",
    "            all_rows = []\n",
    "            \n",
    "            new_count = get_count()\n",
    "            total_elapsed = time.time() - start_time\n",
    "            rate = (new_count - last_count) / total_elapsed if total_elapsed > 0 else 0\n",
    "            remaining_est = 217000000 - new_count\n",
    "            eta_hours = remaining_est / rate / 3600 if rate > 0 else 0\n",
    "            \n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num}: \"\n",
    "                  f\"Total: {new_count:,} | \"\n",
    "                  f\"Rate: {rate:.1f}/s | \"\n",
    "                  f\"ETA: {eta_hours:.1f}h\")\n",
    "        else:\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num}: \"\n",
    "                  f\"+{len(rows)} in {batch_elapsed:.1f}s\")\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Error in batch {batch_num}: {e}\")\n",
    "        # Flush what we have\n",
    "        if all_rows:\n",
    "            try:\n",
    "                insert_rows(all_rows)\n",
    "                all_rows = []\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Waiting 60s before retry...\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "\n",
    "print(\"=\"*70)\n",
    "final_count = get_count()\n",
    "print(f\"Complete! Total embeddings: {final_count:,}\")\n",
    "print(f\"Added {final_count - existing:,} in {(time.time() - start_time)/3600:.1f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
