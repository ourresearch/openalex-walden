{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Work Embeddings\n",
    "\n",
    "Generates embeddings for all OpenAlex works using `ai_query` with OpenAI text-embedding-3-small.\n",
    "\n",
    "**Optimized for job execution**: Skips expensive COUNT queries, starts processing immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 1000  # Start small to verify throughput, increase once confirmed working\n",
    "OUTPUT_TABLE = \"openalex.vector_search.work_embeddings\"\n",
    "SOURCE_TABLE = \"openalex.works.openalex_works\"\n",
    "ENDPOINT_NAME = \"openai-embedding-3-small\"\n",
    "\n",
    "# Quick check - just count existing embeddings (small table, fast)\n",
    "existing = spark.sql(f\"SELECT COUNT(*) as n FROM {OUTPUT_TABLE}\").collect()[0]['n']\n",
    "print(f\"Existing embeddings: {existing:,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE:,}\")\n",
    "print(\"Starting continuous processing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def process_batch():\n",
    "    \"\"\"Process a single batch. Returns number of rows inserted.\"\"\"\n",
    "    result = spark.sql(f\"\"\"\n",
    "        INSERT INTO {OUTPUT_TABLE}\n",
    "        SELECT \n",
    "            CAST(w.id AS STRING) as work_id,\n",
    "            ai_query(\n",
    "                '{ENDPOINT_NAME}',\n",
    "                CONCAT('Title: ', w.title, '\\n\\nAbstract: ', w.abstract)\n",
    "            ) as embedding,\n",
    "            md5(CONCAT('Title: ', w.title, '\\n\\nAbstract: ', w.abstract)) as text_hash,\n",
    "            w.publication_year,\n",
    "            w.type,\n",
    "            w.open_access.is_oa as is_oa,\n",
    "            true as has_abstract,\n",
    "            current_timestamp() as created_at,\n",
    "            current_timestamp() as updated_at\n",
    "        FROM {SOURCE_TABLE} w\n",
    "        WHERE w.type != 'dataset'\n",
    "          AND w.abstract IS NOT NULL\n",
    "          AND w.title IS NOT NULL\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM {OUTPUT_TABLE} e WHERE e.work_id = CAST(w.id AS STRING)\n",
    "          )\n",
    "        LIMIT {BATCH_SIZE}\n",
    "    \"\"\")\n",
    "    return BATCH_SIZE\n",
    "\n",
    "def get_count():\n",
    "    \"\"\"Get current embedding count (fast - small table).\"\"\"\n",
    "    return spark.sql(f\"SELECT COUNT(*) as n FROM {OUTPUT_TABLE}\").collect()[0]['n']\n",
    "\n",
    "# Main loop - run until stopped or complete\n",
    "total_processed = 0\n",
    "start_time = time.time()\n",
    "batch_num = 0\n",
    "last_count = get_count()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting from {last_count:,} embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "while True:\n",
    "    batch_start = time.time()\n",
    "    batch_num += 1\n",
    "    \n",
    "    try:\n",
    "        process_batch()\n",
    "        \n",
    "        batch_elapsed = time.time() - batch_start\n",
    "        total_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Check actual count every 10 batches to detect if we're done\n",
    "        if batch_num % 10 == 0:\n",
    "            new_count = get_count()\n",
    "            actual_added = new_count - last_count\n",
    "            if actual_added == 0:\n",
    "                print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] No new rows added - embedding complete!\")\n",
    "                break\n",
    "            last_count = new_count\n",
    "            rate = actual_added / total_elapsed if total_elapsed > 0 else 0\n",
    "            remaining_est = 217000000 - new_count\n",
    "            eta_hours = remaining_est / rate / 3600 if rate > 0 else 0\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num}: \"\n",
    "                  f\"Total: {new_count:,} | \"\n",
    "                  f\"Rate: {rate:.1f}/s | \"\n",
    "                  f\"ETA: {eta_hours:.1f}h\")\n",
    "        else:\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch {batch_num} done in {batch_elapsed:.0f}s\")\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Error in batch {batch_num}: {e}\")\n",
    "        print(\"Waiting 60s before retry...\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "\n",
    "print(\"=\"*70)\n",
    "final_count = get_count()\n",
    "print(f\"Complete! Total embeddings: {final_count:,}\")\n",
    "print(f\"Added {final_count - existing:,} in {(time.time() - start_time)/3600:.1f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
