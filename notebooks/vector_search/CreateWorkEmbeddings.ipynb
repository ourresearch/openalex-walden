{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Work Embeddings for Vector Search\n",
    "\n",
    "Generates text embeddings for all OpenAlex works using OpenAI `text-embedding-3-small`.\n",
    "\n",
    "**Format**: `Title: {title}\\n\\nAbstract: {abstract}`\n",
    "\n",
    "**Output**: `openalex.vector_search.work_embeddings` Delta table\n",
    "\n",
    "**Exclusions**: Works with type='dataset' (non-semantic titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "BATCH_SIZE = 100  # OpenAI allows up to 2048 per request\n",
    "OUTPUT_TABLE = \"openalex.vector_search.work_embeddings\"\n",
    "SOURCE_TABLE = \"openalex.works.openalex_works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create External Model Endpoint (run once)\n",
    "\n",
    "First, create a Databricks Model Serving endpoint for OpenAI embeddings.\n",
    "This only needs to be done once - skip if endpoint already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONCE to create the external model endpoint\n",
    "# The OpenAI API key should be stored in Databricks secrets\n",
    "\n",
    "import mlflow.deployments\n",
    "\n",
    "ENDPOINT_NAME = \"openai-embedding-3-small\"\n",
    "\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# Check if endpoint already exists\n",
    "try:\n",
    "    existing = client.get_endpoint(ENDPOINT_NAME)\n",
    "    print(f\"Endpoint '{ENDPOINT_NAME}' already exists\")\n",
    "except Exception:\n",
    "    # Create new endpoint\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=ENDPOINT_NAME,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"openai-embeddings\",\n",
    "                    \"external_model\": {\n",
    "                        \"name\": \"text-embedding-3-small\",\n",
    "                        \"provider\": \"openai\",\n",
    "                        \"task\": \"llm/v1/embeddings\",\n",
    "                        \"openai_config\": {\n",
    "                            \"openai_api_key\": \"{{secrets/openalex/openai_api_key}}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"rate_limits\": [\n",
    "                {\n",
    "                    \"calls\": 1000,\n",
    "                    \"key\": \"endpoint\",\n",
    "                    \"renewal_period\": \"minute\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created endpoint: {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create output schema and table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create schema if not exists\n",
    "CREATE SCHEMA IF NOT EXISTS openalex.vector_search;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create embeddings table if not exists\n",
    "CREATE TABLE IF NOT EXISTS openalex.vector_search.work_embeddings (\n",
    "    work_id STRING NOT NULL,\n",
    "    embedding ARRAY<FLOAT>,\n",
    "    text_hash STRING,  -- Hash of input text for change detection\n",
    "    publication_year INT,\n",
    "    type STRING,\n",
    "    is_oa BOOLEAN,\n",
    "    has_abstract BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (work_id)\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import hashlib\n",
    "from pyspark.sql.functions import udf, col, concat_ws, lit, md5, when, coalesce\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "# Initialize MLflow client\n",
    "mlflow_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "def format_text_for_embedding(title, abstract):\n",
    "    \"\"\"Format title and abstract for embedding.\"\"\"\n",
    "    parts = []\n",
    "    if title:\n",
    "        parts.append(f\"Title: {title}\")\n",
    "    if abstract:\n",
    "        parts.append(f\"Abstract: {abstract}\")\n",
    "    return \"\\n\\n\".join(parts) if parts else None\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding for a single text.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        response = mlflow_client.predict(\n",
    "            endpoint=ENDPOINT_NAME,\n",
    "            inputs={\"input\": text}\n",
    "        )\n",
    "        return response[\"data\"][0][\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register as UDF for Spark\n",
    "get_embedding_udf = udf(get_embedding, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get works that need embeddings\n",
    "\n",
    "This finds works that either:\n",
    "1. Don't have embeddings yet\n",
    "2. Have changed (title/abstract modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read source works\n# Note: abstract is already a string column (not inverted index)\nworks_df = spark.table(SOURCE_TABLE).filter(\n    # Exclude datasets - their titles are non-semantic\n    col(\"type\") != \"dataset\"\n).select(\n    col(\"id\").cast(\"string\").alias(\"work_id\"),\n    col(\"title\"),\n    col(\"abstract\"),\n    col(\"publication_year\"),\n    col(\"type\"),\n    col(\"open_access.is_oa\").alias(\"is_oa\")\n)\n\nprint(f\"Total works (excluding datasets): {works_df.count():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Abstract is already a string column - no reconstruction needed\n# This cell kept for compatibility but the UDF is not used"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add embedding text (abstract is already a string)\nworks_with_text = works_df.withColumn(\n    \"embedding_text\",\n    when(\n        col(\"abstract\").isNotNull(),\n        concat_ws(\"\\n\\n\", \n            concat_ws(\": \", lit(\"Title\"), col(\"title\")),\n            concat_ws(\": \", lit(\"Abstract\"), col(\"abstract\"))\n        )\n    ).otherwise(\n        concat_ws(\": \", lit(\"Title\"), col(\"title\"))\n    )\n).withColumn(\n    \"text_hash\", md5(col(\"embedding_text\"))\n).withColumn(\n    \"has_abstract\", col(\"abstract\").isNotNull()\n)\n\n# Show sample\nworks_with_text.select(\"work_id\", \"title\", \"has_abstract\", \"embedding_text\").show(5, truncate=80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find works that need new/updated embeddings\n",
    "existing_embeddings = spark.table(OUTPUT_TABLE).select(\"work_id\", \"text_hash\")\n",
    "\n",
    "works_to_embed = works_with_text.join(\n",
    "    existing_embeddings,\n",
    "    on=\"work_id\",\n",
    "    how=\"left_anti\"  # Works not in embeddings table\n",
    ").union(\n",
    "    # Or works where text has changed\n",
    "    works_with_text.alias(\"w\").join(\n",
    "        existing_embeddings.alias(\"e\"),\n",
    "        (col(\"w.work_id\") == col(\"e.work_id\")) & (col(\"w.text_hash\") != col(\"e.text_hash\")),\n",
    "        how=\"inner\"\n",
    "    ).select(\"w.*\")\n",
    ")\n",
    "\n",
    "print(f\"Works needing embeddings: {works_to_embed.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate embeddings in batches\n",
    "\n",
    "Process in batches to manage memory and allow checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, monotonically_increasing_id\n",
    "\n",
    "# Add batch ID for processing\n",
    "RECORDS_PER_BATCH = 10000\n",
    "\n",
    "works_batched = works_to_embed.withColumn(\n",
    "    \"batch_id\", (monotonically_increasing_id() / RECORDS_PER_BATCH).cast(\"int\")\n",
    ")\n",
    "\n",
    "total_batches = works_batched.select(\"batch_id\").distinct().count()\n",
    "print(f\"Total batches: {total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batches\n",
    "# Note: For production, use ai_query() with batch inference for better performance\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{total_batches}...\")\n",
    "    \n",
    "    batch_df = works_batched.filter(col(\"batch_id\") == batch_num)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedded_df = batch_df.withColumn(\n",
    "        \"embedding\", get_embedding_udf(col(\"embedding_text\"))\n",
    "    ).withColumn(\n",
    "        \"created_at\", current_timestamp()\n",
    "    ).withColumn(\n",
    "        \"updated_at\", current_timestamp()\n",
    "    ).select(\n",
    "        \"work_id\",\n",
    "        \"embedding\",\n",
    "        \"text_hash\",\n",
    "        \"publication_year\",\n",
    "        \"type\",\n",
    "        \"is_oa\",\n",
    "        \"has_abstract\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\"\n",
    "    )\n",
    "    \n",
    "    # Write to Delta table (upsert)\n",
    "    embedded_df.write.format(\"delta\").mode(\"append\").saveAsTable(OUTPUT_TABLE)\n",
    "    \n",
    "    print(f\"  Completed batch {batch_num + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "    COUNT(*) as total_embeddings,\n",
    "    SUM(CASE WHEN has_abstract THEN 1 ELSE 0 END) as with_abstract,\n",
    "    SUM(CASE WHEN NOT has_abstract THEN 1 ELSE 0 END) as title_only,\n",
    "    MIN(created_at) as oldest,\n",
    "    MAX(created_at) as newest\n",
    "FROM openalex.vector_search.work_embeddings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check embedding dimensions\n",
    "SELECT \n",
    "    work_id,\n",
    "    SIZE(embedding) as embedding_dims\n",
    "FROM openalex.vector_search.work_embeddings\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Use ai_query for better batch performance\n",
    "\n",
    "For production, `ai_query()` with the external model endpoint provides better batch performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative approach using ai_query (recommended for production)\n# This requires the external model endpoint to be created first\n\n# Example SQL (run in separate notebook or SQL editor):\nsql_query = \"\"\"\nINSERT INTO openalex.vector_search.work_embeddings\nSELECT \n    CAST(id AS STRING) as work_id,\n    ai_query(\n        'openai-embedding-3-small',\n        CONCAT('Title: ', title, COALESCE(CONCAT('\\n\\nAbstract: ', abstract), ''))\n    ) as embedding,\n    md5(CONCAT('Title: ', title, COALESCE(CONCAT('\\n\\nAbstract: ', abstract), ''))) as text_hash,\n    publication_year,\n    type,\n    open_access.is_oa as is_oa,\n    abstract IS NOT NULL as has_abstract,\n    current_timestamp() as created_at,\n    current_timestamp() as updated_at\nFROM openalex.works.openalex_works\nWHERE type != 'dataset'\n  AND title IS NOT NULL\n  AND CAST(id AS STRING) NOT IN (SELECT work_id FROM openalex.vector_search.work_embeddings)\nLIMIT 10000\n\"\"\"\nprint(\"To use ai_query, run the above SQL in a Databricks SQL notebook\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}