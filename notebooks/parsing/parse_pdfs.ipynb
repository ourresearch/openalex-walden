{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd4aefdb-5f24-4da3-b8c9-d61452a48e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This pipeline parses PDFs with grobid.\n",
    "\n",
    "**input**: last 3 days of records from taxicab where type is pdf from table `openalex.taxicab.taxicab_results`\n",
    "\n",
    "**process**: grobid API on ECS\n",
    "\n",
    "**output**: xml from grobid, along with urls and ids into table `openalex.pdf.grobid_processing_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3713fe-bf95-43ba-9b6e-a2a6098a701d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "import time\n",
    "import random\n",
    "from urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BooleanType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c64bf79-f755-42a2-ad1b-dcd53ebafb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GROBID_URL = \"http://grobid-api-load-balancer-1880850154.us-east-1.elb.amazonaws.com/parse\"\n",
    "BATCH_SIZE = 100\n",
    "MAX_WORKERS = 70\n",
    "http_session = requests.Session()\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"POST\"]\n",
    ")\n",
    "\n",
    "adapter = HTTPAdapter(\n",
    "    pool_connections=MAX_WORKERS,\n",
    "    pool_maxsize=MAX_WORKERS*2,\n",
    "    max_retries=retry_strategy\n",
    ")\n",
    "\n",
    "http_session.mount(\"http://\", adapter)\n",
    "http_session.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605a30a7-420e-4f0c-912e-70febfb713cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grobid_results_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"source_pdf_id\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"native_id\", StringType(), True),\n",
    "    StructField(\"native_id_namespace\", StringType(), True),\n",
    "    StructField(\"s3_key\", StringType(), True),\n",
    "    StructField(\"s3_path\", StringType(), True),\n",
    "    StructField(\"xml_content\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d582d935-6056-4a86-b992-e5244470e943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_error_response(pdf_uuid, url, native_id, native_id_namespace, error_message):\n",
    "    \"\"\"Create a standardized error response dictionary\"\"\"\n",
    "    return {\n",
    "        \"id\": None,\n",
    "        \"status\": \"failed\",\n",
    "        \"source_pdf_id\": pdf_uuid,\n",
    "        \"url\": url,\n",
    "        \"native_id\": native_id,\n",
    "        \"native_id_namespace\": native_id_namespace,\n",
    "        \"s3_key\": None,\n",
    "        \"s3_path\": None,\n",
    "        \"xml_content\": None,\n",
    "        \"error_message\": error_message,\n",
    "        \"created_date\": datetime.now(),\n",
    "    }\n",
    "\n",
    "def process_pdf_single(row_data):\n",
    "    \"\"\"Process a single PDF through the GROBID service with improved connection handling\"\"\"\n",
    "    pdf_uuid = row_data.get('source_pdf_id')\n",
    "    url = row_data.get('url')\n",
    "    native_id = row_data.get('native_id')\n",
    "    native_id_namespace = row_data.get('native_id_namespace')\n",
    "    max_retries = 2\n",
    "    \n",
    "    print(f\"Processing PDF {pdf_uuid} from {url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data = {\n",
    "        \"pdf_uuid\": pdf_uuid,\n",
    "        \"url\": url,\n",
    "        \"native_id\": native_id,\n",
    "        \"native_id_namespace\": native_id_namespace\n",
    "    }\n",
    "    \n",
    "    retry_count = 0\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            if retry_count > 0:\n",
    "                jitter = random.uniform(0.1, 1.0)\n",
    "                time.sleep(jitter)\n",
    "                \n",
    "            response = http_session.post(\n",
    "                GROBID_URL,\n",
    "                json=data,\n",
    "                timeout=(30, 120)\n",
    "            )\n",
    "            \n",
    "            if response.status_code in [429, 503, 504]:\n",
    "                retry_count += 1\n",
    "                # exponential backoff with jitter\n",
    "                wait_time = min(2 ** retry_count + random.uniform(0, 1), 60)\n",
    "                print(f\"Service error {response.status_code} for {url}, retrying in {wait_time:.2f} seconds... ({retry_count}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "            if response.status_code >= 400:\n",
    "                error_message = f\"HTTP error: {response.status_code}\"\n",
    "                try:\n",
    "                    response_json = response.json()\n",
    "                    if \"error\" in response_json and response_json[\"error\"] is not None:\n",
    "                        error_message = f\"HTTP error {response.status_code}: {response_json['error']}\"\n",
    "                    else:\n",
    "                        error_message = f\"HTTP error {response.status_code}: {response.text[:200]}\"\n",
    "                except:\n",
    "                    # If not valid JSON, use text\n",
    "                    error_message = f\"HTTP error {response.status_code}: {response.text[:200]}\"\n",
    "                \n",
    "                return create_error_response(\n",
    "                    pdf_uuid, url, native_id, native_id_namespace,\n",
    "                    error_message\n",
    "                )\n",
    "\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            print(f\"GROBID request for {pdf_uuid} took {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # successful response\n",
    "            return {\n",
    "                \"id\": result.get(\"id\"),\n",
    "                \"status\": result.get(\"status\") or \"success\",\n",
    "                \"source_pdf_id\": result.get(\"source_pdf_id\"),\n",
    "                \"url\": url,\n",
    "                \"native_id\": native_id,\n",
    "                \"native_id_namespace\": native_id_namespace,\n",
    "                \"s3_key\": result.get(\"s3_key\"),\n",
    "                \"s3_path\": result.get(\"s3_path\"),\n",
    "                \"xml_content\": result.get(\"xml_content\"),\n",
    "                \"error_message\": None,\n",
    "                \"created_date\": datetime.now()\n",
    "            }\n",
    "                \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            retry_count += 1\n",
    "            error_msg = f\"Connection error: {str(e)}\"\n",
    "            \n",
    "            # If exceeded retries, return error\n",
    "            if retry_count > max_retries:\n",
    "                print(f\"Failed to connect for {url} after {max_retries} retries: {error_msg}\")\n",
    "                return create_error_response(\n",
    "                    pdf_uuid, url, native_id, native_id_namespace,\n",
    "                    error_msg\n",
    "                )\n",
    "            \n",
    "            # longer wait for connection errors\n",
    "            wait_time = min(5 ** retry_count + random.uniform(0, 2), 120)\n",
    "            print(f\"Connection error for {url}, retrying in {wait_time:.2f} seconds... ({retry_count}/{max_retries})\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # if exceeded retries, return error\n",
    "            if retry_count > max_retries:\n",
    "                print(f\"Error processing {url}: {error_msg}\")\n",
    "                return create_error_response(\n",
    "                    pdf_uuid, url, native_id, native_id_namespace,\n",
    "                    error_msg\n",
    "                )\n",
    "            \n",
    "            # wait before retrying with jitter\n",
    "            wait_time = min(2 ** retry_count + random.uniform(0, 1), 60)\n",
    "            print(f\"Error: {error_msg}, retrying in {wait_time:.2f} seconds... ({retry_count}/{max_retries})\")\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011411c2-c98b-4739-8410-4b123a4c0b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_pdfs_with_continuous_batching(candidates_df, write_size=BATCH_SIZE*10):\n",
    "    \"\"\"Process PDFs with continuous batching that doesn't wait for stragglers\"\"\"\n",
    "    \n",
    "    # Convert to pandas for processing\n",
    "    pdf_data = candidates_df.toPandas()\n",
    "    \n",
    "    # Create a list of row data dictionaries\n",
    "    rows = []\n",
    "    for _, row in pdf_data.iterrows():\n",
    "        rows.append({\n",
    "            'source_pdf_id': row['source_pdf_id'],\n",
    "            'url': row['url'],\n",
    "            'native_id': row['native_id'],\n",
    "            'native_id_namespace': row['native_id_namespace']\n",
    "        })\n",
    "    \n",
    "    print(f\"Processing {len(rows)} records with continuous batching\")\n",
    "    print(f\"Will write to database in batches of {write_size} records\")\n",
    "    \n",
    "    # Create a bounded thread pool\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks to the executor without waiting\n",
    "        future_to_row = {executor.submit(process_pdf_single, row): row for row in rows}\n",
    "        \n",
    "        # Process results as they complete rather than waiting for all\n",
    "        completed_count = 0\n",
    "        accumulated_results = []\n",
    "        \n",
    "        for future in as_completed(future_to_row):\n",
    "            row = future_to_row[future]\n",
    "            try:\n",
    "                result = future.result(timeout=180)  # Longer timeout for individual tasks\n",
    "                accumulated_results.append(result)\n",
    "                completed_count += 1\n",
    "                \n",
    "                if completed_count % 50 == 0:\n",
    "                    print(f\"Completed {completed_count}/{len(rows)} records\")\n",
    "                \n",
    "                # Write in larger chunks\n",
    "                if len(accumulated_results) >= write_size:\n",
    "                    print(f\"Writing batch of {len(accumulated_results)} results to database\")\n",
    "                    result_df = pd.DataFrame(accumulated_results)\n",
    "                    spark_df = spark.createDataFrame(result_df, schema=grobid_results_schema)\n",
    "                    write_results_to_table(spark_df)\n",
    "                    accumulated_results = []\n",
    "                    \n",
    "            except TimeoutError:\n",
    "                print(f\"Task timed out for PDF: {row.get('url')}\")\n",
    "                accumulated_results.append({\n",
    "                    \"id\": None,\n",
    "                    \"status\": \"timeout\",\n",
    "                    \"source_pdf_id\": row.get('source_pdf_id'),\n",
    "                    \"url\": row.get('url'),\n",
    "                    \"native_id\": row.get('native_id'),\n",
    "                    \"native_id_namespace\": row.get('native_id_namespace'),\n",
    "                    \"s3_key\": None,\n",
    "                    \"s3_path\": None,\n",
    "                    \"xml_content\": None,\n",
    "                    \"error_message\": \"Task timed out\",\n",
    "                    \"created_date\": datetime.now(),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF {row.get('url')}: {str(e)}\")\n",
    "                accumulated_results.append({\n",
    "                    \"id\": None,\n",
    "                    \"status\": \"error\",\n",
    "                    \"source_pdf_id\": row.get('source_pdf_id'),\n",
    "                    \"url\": row.get('url'),\n",
    "                    \"native_id\": row.get('native_id'),\n",
    "                    \"native_id_namespace\": row.get('native_id_namespace'),\n",
    "                    \"s3_key\": None,\n",
    "                    \"s3_path\": None,\n",
    "                    \"xml_content\": None,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"created_date\": datetime.now(),\n",
    "                })\n",
    "        \n",
    "        # Write any remaining results\n",
    "        if accumulated_results:\n",
    "            print(f\"Writing final batch of {len(accumulated_results)} results to database\")\n",
    "            result_df = pd.DataFrame(accumulated_results)\n",
    "            spark_df = spark.createDataFrame(result_df, schema=grobid_results_schema)\n",
    "            write_results_to_table(spark_df)\n",
    "    \n",
    "    print(f\"Successfully processed {completed_count} records\")\n",
    "    return completed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc02560-eb5d-4e21-bf09-a46b68be0719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_results_to_table(results_df):\n",
    "    \"\"\"Write results to the grobid_processing_results table\"\"\"\n",
    "    if results_df is None or results_df.isEmpty():\n",
    "        print(\"No results to write\")\n",
    "        return\n",
    "    \n",
    "    # create the table if it doesn't exist\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS openalex.pdf.grobid_processing_results (\n",
    "        id STRING,\n",
    "        status STRING,\n",
    "        source_pdf_id STRING,\n",
    "        url STRING,\n",
    "        native_id STRING,\n",
    "        native_id_namespace STRING,\n",
    "        s3_key STRING,\n",
    "        s3_path STRING,\n",
    "        xml_content STRING,\n",
    "        error_message STRING,\n",
    "        created_date TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    results_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"openalex.pdf.grobid_processing_results\")\n",
    "    \n",
    "    print(f\"Successfully appended {results_df.count()} records to grobid_processing_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247fb945-1c9e-4c96-a466-e3706c18779c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting GROBID PDF processing job...\")\n",
    "\n",
    "three_days_ago = datetime.now() - timedelta(days=3)\n",
    "\n",
    "# get existing IDs from grobid_processing_results\n",
    "try:\n",
    "    existing_ids_df = spark.table(\"openalex.pdf.grobid_processing_results\")\n",
    "    existing_ids = existing_ids_df.select(\"source_pdf_id\").distinct()\n",
    "except:\n",
    "    print(\"No existing grobid_processing_results table found. Creating new.\")\n",
    "    existing_ids = spark.createDataFrame([], schema=StructType([StructField(\"source_pdf_id\", StringType(), True)]))\n",
    "\n",
    "# get candidate records from taxicab_results\n",
    "taxicab_df = spark.table(\"openalex.taxicab.taxicab_results\") \\\n",
    "    .filter(\n",
    "        (F.col(\"taxicab_id\").isNotNull()) & \n",
    "        (F.col(\"content_type\").contains(\"pdf\")) &\n",
    "        (F.col(\"processed_date\") >= F.lit(three_days_ago))\n",
    "    )\n",
    "\n",
    "# filter out records that already exist in grobid_processing_results\n",
    "candidates_df = taxicab_df \\\n",
    "    .join(existing_ids, taxicab_df[\"taxicab_id\"] == existing_ids[\"source_pdf_id\"], \"left_anti\") \\\n",
    "    .select(\n",
    "        F.col(\"taxicab_id\").alias(\"source_pdf_id\"),\n",
    "        \"url\",\n",
    "        \"native_id\",\n",
    "        \"native_id_namespace\"\n",
    "    )\n",
    "\n",
    "candidates_df = candidates_df.cache()\n",
    "\n",
    "# get the count of records to process\n",
    "total_records = candidates_df.count()\n",
    "print(f\"Found {total_records} records to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef5c7f0-20f3-4a46-ac57-cce31a2cf59b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if total_records > 0:\n",
    "    write_batch_size = BATCH_SIZE * 10\n",
    "    \n",
    "    print(f\"Using continuous batching method with {MAX_WORKERS} workers\")\n",
    "    print(f\"Will write to database in batches of {write_batch_size} records\")\n",
    "    \n",
    "    # Process with continuous batching\n",
    "    processed_count = process_pdfs_with_continuous_batching(\n",
    "        candidates_df, \n",
    "        write_size=write_batch_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully processed {processed_count} of {total_records} records\")\n",
    "    \n",
    "    # clean up\n",
    "    candidates_df.unpersist()\n",
    "    print(\"GROBID PDF processing job completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"No records to process. Exiting.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parse_pdfs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
