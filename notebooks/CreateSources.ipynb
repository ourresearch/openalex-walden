{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269538ed-30e5-4c65-83e4-573fc5ae04c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install /Volumes/openalex/default/libraries/openalex_dlt_utils-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f435e08-52b6-4f0d-bc5a-b30fd4e34cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import builtins\n",
    "\n",
    "from openalex.utils.environment import *\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTANTS AND SCHEMAS\n",
    "# ============================================================================\n",
    "\n",
    "ID_BUFFER_CROSSREF = 1_000_000_000\n",
    "ID_BUFFER_DATACITE = 2_000_000_000\n",
    "\n",
    "APC_SCHEMA = ArrayType(StructType([\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "]))\n",
    "\n",
    "SOCIETIES_SCHEMA = ArrayType(StructType([\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"organization\", StringType(), True),\n",
    "]))\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_duplicate_issns(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate ISSNs across records, keeping them only in the record with the lowest ID.\n",
    "    \n",
    "    When the same ISSN appears in multiple source records, this function ensures it's kept\n",
    "    only in the record with the lowest ID, removing it from all other records.\n",
    "    \"\"\"\n",
    "    df_exploded = df.withColumn('issn', explode('issns')).filter(col('issn').isNotNull())\n",
    "    \n",
    "    # for each ISSN, find the record with the lowest ID that should keep it\n",
    "    issn_keeper_mapping = (\n",
    "        df_exploded\n",
    "        .groupBy('issn')\n",
    "        .agg(\n",
    "            min('id').alias('keeper_id'),\n",
    "            count('id').alias('record_count')\n",
    "        )\n",
    "        .filter(col('record_count') > 1)\n",
    "        .select('issn', 'keeper_id')\n",
    "    )\n",
    "    \n",
    "    # create a mapping of which ISSNs each record should remove\n",
    "    issns_to_remove = (\n",
    "        df_exploded\n",
    "        .join(issn_keeper_mapping, 'issn')\n",
    "        .filter(col('id') != col('keeper_id'))\n",
    "        .groupBy('id')\n",
    "        .agg(array_sort(collect_set('issn')).alias('issns_to_remove'))\n",
    "    )\n",
    "    \n",
    "    # apply the deduplication by removing duplicate ISSNs from higher ID records\n",
    "    return (\n",
    "        df\n",
    "        .join(issns_to_remove, 'id', 'left')\n",
    "        .withColumn(\n",
    "            'issns_deduplicated',\n",
    "            when(col('issns_to_remove').isNotNull(),\n",
    "                 array_except(col('issns'), col('issns_to_remove'))\n",
    "            ).otherwise(col('issns'))\n",
    "        )\n",
    "        .drop('issns', 'issns_to_remove')\n",
    "        .withColumnRenamed('issns_deduplicated', 'issns')\n",
    "    )\n",
    "\n",
    "\n",
    "def get_max_existing_id(*table_names):\n",
    "    \"\"\"\n",
    "    Get the maximum ID across multiple tables.\n",
    "    \"\"\"\n",
    "    max_ids = []\n",
    "    for table_name in table_names:\n",
    "        try:\n",
    "            # Handle different column names for different tables\n",
    "            if \"sources_from_postgres\" in table_name:\n",
    "                max_id = spark.table(table_name).agg(max(\"id\")).collect()[0][0]\n",
    "            else:\n",
    "                max_id = spark.table(table_name).agg(max(\"openalex_id\")).collect()[0][0]\n",
    "            \n",
    "            if max_id is not None:\n",
    "                max_ids.append(max_id)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return builtins.max(max_ids) if max_ids else 0\n",
    "\n",
    "\n",
    "def create_id_mappings(new_sources, start_id, id_column, order_column):\n",
    "    \"\"\"\n",
    "    Create OpenAlex ID mappings for new sources with deterministic ordering.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        new_sources\n",
    "        .orderBy(order_column)\n",
    "        .withColumn(\"row_num\", row_number().over(Window.orderBy(order_column)))\n",
    "        .withColumn(\"openalex_id\", col(\"row_num\") + lit(start_id - 1))\n",
    "        .withColumn(\"created_date\", current_timestamp())\n",
    "        .select(id_column, \"openalex_id\", \"created_date\")\n",
    "    )\n",
    "\n",
    "\n",
    "def manage_persistent_id_mapping(\n",
    "    sources_df,\n",
    "    persistent_table,\n",
    "    id_column,\n",
    "    id_buffer,\n",
    "    additional_max_tables=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic function to manage persistent ID mappings for sources.\n",
    "    \"\"\"\n",
    "    # Check if persistent mapping exists\n",
    "    try:\n",
    "        existing_mapping = spark.table(persistent_table)\n",
    "        mapping_exists = True\n",
    "    except:\n",
    "        existing_mapping = None\n",
    "        mapping_exists = False\n",
    "    \n",
    "    # Tables to check for max IDs\n",
    "    tables_to_check = [\"openalex.sources.sources_from_postgres\"]\n",
    "    if additional_max_tables:\n",
    "        tables_to_check.extend(additional_max_tables)\n",
    "    \n",
    "    if mapping_exists:\n",
    "        # Find new sources not in existing mapping\n",
    "        new_sources = sources_df.join(existing_mapping, id_column, \"left_anti\")\n",
    "        \n",
    "        if new_sources.count() > 0:\n",
    "            # Get max ID from existing mapping and other tables\n",
    "            max_mapping_id = existing_mapping.agg(max(\"openalex_id\")).collect()[0][0] or 0\n",
    "            max_other_id = get_max_existing_id(*tables_to_check)\n",
    "            next_id = builtins.max(max_mapping_id, max_other_id) + 1\n",
    "            \n",
    "            # Create new mappings\n",
    "            new_mappings = create_id_mappings(new_sources, next_id, id_column, id_column)\n",
    "            final_mapping = existing_mapping.unionByName(new_mappings)\n",
    "        else:\n",
    "            final_mapping = existing_mapping\n",
    "    else:\n",
    "        # Create initial mapping with buffer\n",
    "        max_existing_id = get_max_existing_id(*tables_to_check)\n",
    "        start_id = max_existing_id + id_buffer\n",
    "        \n",
    "        final_mapping = create_id_mappings(sources_df, start_id, id_column, id_column)\n",
    "    \n",
    "    # Save to persistent table\n",
    "    (final_mapping\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .saveAsTable(persistent_table))\n",
    "    \n",
    "    return final_mapping\n",
    "\n",
    "# ============================================================================\n",
    "# DLT PIPELINE TABLES\n",
    "# ============================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"base_sources\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment=\"Sources from original postgresql table with ISSN deduplication.\"\n",
    ")\n",
    "def base_sources():\n",
    "    df = (spark.table(\"openalex.sources.sources_from_postgres\")\n",
    "        .drop(\"is_in_doaj\")\n",
    "        .filter(col(\"merge_into_id\").isNull())\n",
    "        .withColumn(\n",
    "            'issns',\n",
    "            when(col('issns').isNull(), None)\n",
    "            .otherwise(from_json(col('issns'), ArrayType(StringType())))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'alternate_titles',\n",
    "            when(col('alternate_titles').isNull(), None)\n",
    "            .otherwise(from_json(col('alternate_titles'), ArrayType(StringType())))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'apc_prices',\n",
    "            when(col('apc_prices').isNull(), None)\n",
    "            .otherwise(from_json(col('apc_prices'), APC_SCHEMA))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'societies',\n",
    "            when(col('societies').isNull(), None)\n",
    "            .otherwise(from_json(col('societies'), SOCIETIES_SCHEMA))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return remove_duplicate_issns(df)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"crossref_id_mapping\",\n",
    "    comment=\"Stable ID mapping for crossref journals (reads from persistent table)\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def crossref_id_mapping():\n",
    "    \"\"\"Create/maintain mapping between crossref sources and stable OpenAlex IDs.\"\"\"\n",
    "    \n",
    "    crossref_sources = (\n",
    "        spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "        .select(\"issns_concat_id\", \"title\")\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    return manage_persistent_id_mapping(\n",
    "        sources_df=crossref_sources,\n",
    "        persistent_table=\"openalex.sources.crossref_id_mapping_persistent\",\n",
    "        id_column=\"issns_concat_id\",\n",
    "        id_buffer=ID_BUFFER_CROSSREF,\n",
    "        additional_max_tables=None\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"datacite_id_mapping_extended\",\n",
    "    comment=\"Stable ID mapping for unmatched datacite sources (reads from persistent table)\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def datacite_id_mapping_extended():\n",
    "    \"\"\"Create/maintain mapping for datacite sources that don't have existing OpenAlex IDs.\"\"\"\n",
    "    \n",
    "    # Get datacite sources without existing OpenAlex mappings\n",
    "    datacite_sources = spark.read.table(\"openalex.sources.datacite_sources\")\n",
    "    existing_openalex_mapping = spark.read.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "    \n",
    "    unmatched_sources = (\n",
    "        datacite_sources\n",
    "        .join(\n",
    "            existing_openalex_mapping,\n",
    "            datacite_sources[\"id\"] == existing_openalex_mapping[\"datacite_id\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .filter(col(\"openalex_id\").isNull())\n",
    "        .select(datacite_sources[\"id\"].alias(\"datacite_id\"), \"display_name\")\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    return manage_persistent_id_mapping(\n",
    "        sources_df=unmatched_sources,\n",
    "        persistent_table=\"openalex.sources.datacite_id_mapping_extended_persistent\",\n",
    "        id_column=\"datacite_id\",\n",
    "        id_buffer=ID_BUFFER_DATACITE,\n",
    "        additional_max_tables=[\"openalex.sources.crossref_id_mapping_persistent\"]\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "   name=\"crossref_journals_unmatched\",\n",
    "   comment=\"Crossref journals that have NO matching ISSNs with existing base sources\"\n",
    ")\n",
    "def crossref_journals_unmatched():\n",
    "    # get all ISSNs from base sources\n",
    "    base_issns = (dlt.read(\"base_sources\")\n",
    "                 .select(\"id\", explode(\"issns\").alias(\"issn\"))\n",
    "                 .select(\"issn\")\n",
    "                 .distinct())\n",
    "    \n",
    "    # get crossref journals with their exploded ISSNs\n",
    "    crossref_with_issns = (spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "                          .select(\"*\", explode(\"issns\").alias(\"issn\")))\n",
    "    \n",
    "    # find crossref journals that have at least one matching ISSN\n",
    "    crossref_with_matches = (crossref_with_issns\n",
    "                           .join(base_issns, \"issn\", \"inner\")\n",
    "                           .select(\"issns_concat_id\")\n",
    "                           .distinct())\n",
    "    \n",
    "    # find crossref journals that have NO matching ISSNs\n",
    "    crossref_completely_unmatched = (spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "                                   .join(crossref_with_matches, \"issns_concat_id\", \"left_anti\"))\n",
    "    \n",
    "    # join with the stable ID mapping\n",
    "    id_mapping = dlt.read(\"crossref_id_mapping\")\n",
    "    \n",
    "    return (crossref_completely_unmatched\n",
    "           .join(id_mapping, \"issns_concat_id\", \"inner\")\n",
    "           .select(\n",
    "               col(\"openalex_id\").alias(\"id\"),\n",
    "               col(\"title\").alias(\"display_name\"),\n",
    "               col(\"issns\"),\n",
    "               col(\"publisher\"),\n",
    "               lit(False).alias(\"is_oa\"),\n",
    "               lit(\"journal\").alias(\"type\")\n",
    "           )\n",
    "           .withColumn(\"issn\", when(size(col(\"issns\")) > 0, col(\"issns\")[0]).otherwise(lit(None)))\n",
    "    )\n",
    "    \n",
    "@dlt.table(\n",
    "    name=\"datacite_sources_unmatched\",\n",
    "    comment=\"Datacite sources that have NO match existing sources\"\n",
    ")\n",
    "def datacite_sources_unmatched():\n",
    "    df = spark.read.table(\"openalex.sources.datacite_sources\")\n",
    "    df_mapping = spark.read.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "    \n",
    "    # unmatched datacite sources\n",
    "    unmatched_df = df.join(\n",
    "        df_mapping,\n",
    "        col(\"id\") == col(\"datacite_id\"),\n",
    "        \"left\"\n",
    "    ).filter(col(\"openalex_id\").isNull())\n",
    "\n",
    "    # join with the stable ID mapping - alias the openalex_id to avoid ambiguity\n",
    "    id_mapping = dlt.read(\"datacite_id_mapping_extended\").select(\n",
    "        col(\"datacite_id\").alias(\"mapping_datacite_id\"),\n",
    "        col(\"openalex_id\").alias(\"new_openalex_id\"),\n",
    "        col(\"created_date\").alias(\"mapping_created_date\")\n",
    "    )\n",
    "\n",
    "    return (unmatched_df\n",
    "        .join(id_mapping, \n",
    "              unmatched_df[\"id\"] == id_mapping[\"mapping_datacite_id\"], \n",
    "              \"inner\")\n",
    "        .select(\n",
    "            col(\"new_openalex_id\").alias(\"id\"),\n",
    "            col(\"id\").alias(\"datacite_id\"),\n",
    "            col(\"display_name\"),\n",
    "            array_distinct(\n",
    "                array_compact(\n",
    "                    array(\n",
    "                        col(\"issns.issnl\"),\n",
    "                        col(\"issns.print\"),\n",
    "                        col(\"issns.electronic\")\n",
    "                    )\n",
    "                )\n",
    "            ).alias(\"issns\"),\n",
    "            col(\"provider_name\"),\n",
    "            col(\"type\")\n",
    "        )\n",
    "        .withColumn(\"issn\", when(size(col(\"issns\")) > 0, col(\"issns\")[0]).otherwise(lit(None)))\n",
    "        .withColumn(\"publisher\", col(\"provider_name\"))\n",
    "        .withColumn(\"is_oa\", lit(True))\n",
    "        .withColumn(\"type\", \n",
    "            when(col(\"type\") == \"periodical\", \"journal\")\n",
    "            .otherwise(col(\"type\"))\n",
    "        )\n",
    "        .withColumn(\"updated_date\", current_timestamp())\n",
    "        .withColumn(\"created_date\", current_timestamp().cast(\"string\"))\n",
    "        .drop(\"provider_name\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"sources\",\n",
    "    comment=f\"Combined sources with DOAJ status, J-STAGE status, and sample PMH records in {ENV.upper()}\"\n",
    ")\n",
    "def sources():\n",
    "    # combine base sources with unmatched crossref and datacite sources\n",
    "    base_combined = (\n",
    "        dlt.read(\"base_sources\")\n",
    "        .unionByName(\n",
    "            dlt.read(\"crossref_journals_unmatched\"),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "        .unionByName(\n",
    "            dlt.read(\"datacite_sources_unmatched\"),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # get DOAJ ISSNs\n",
    "    doaj = (\n",
    "        spark.table(\"openalex.sources.doaj_from_csv\")\n",
    "        .withColumn(\n",
    "            \"doaj_license_normalized\",\n",
    "            when(col(\"journal_license\") == \"CC BY\", \"cc-by\")\n",
    "            .when(col(\"journal_license\") == \"CC BY-NC\", \"cc-by-nc\")\n",
    "            .when(col(\"journal_license\") == \"CC BY-NC-ND\", \"cc-by-nc-nd\")\n",
    "            .when(col(\"journal_license\") == \"CC BY-NC-SA\", \"cc-by-nc-sa\")\n",
    "            .when(col(\"journal_license\") == \"CC BY-SA\", \"cc-by-sa\")\n",
    "            .when(col(\"journal_license\") == \"CC BY-ND\", \"cc-by-nd\")\n",
    "            .when(col(\"journal_license\") == \"Public domain\", \"public-domain\")\n",
    "            .otherwise(None)  # For multi-license cases or unrecognized licenses\n",
    "        )\n",
    "        .selectExpr(\"explode(issns) as doaj_issn\", \"oa_start_year\", \"doaj_license_normalized\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # get J-STAGE ISSNs\n",
    "    jstage = (\n",
    "        spark.table(\"openalex.sources.jstage_oa\")\n",
    "        .selectExpr(\"explode(issns) as jstage_issn\", \"jstage_oa_start_year\", \"jstage_oa_end_year\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # get scielo issns (all are oa)\n",
    "    scielo = (\n",
    "        spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "        .filter(lower(col(\"publisher\")).startswith(\"scielo\"))\n",
    "        .selectExpr(\"explode(issns) as scielo_issn\", \"publisher as scielo_publisher\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    ojs = (\n",
    "        spark.table(\"openalex.sources.ojs_journals\")\n",
    "        .select(\n",
    "            col(\"issn\").alias(\"ojs_issn\"),\n",
    "            col(\"is_oa\").alias(\"ojs_is_oa\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # new curation process from postgres\n",
    "    approved_curations_window = Window.partitionBy(\"source_id\", \"property\").orderBy(col(\"moderated_date\").desc())\n",
    "    \n",
    "    approved_curations = (\n",
    "        spark.table(\"openalex.curations.approved_curations\")\n",
    "        .filter(col(\"entity\") == \"sources\")\n",
    "        .filter(col(\"status\") == \"approved\")\n",
    "        .withColumn(\"source_id\", regexp_replace(col(\"entity_id\"), \"^S\", \"\").cast(\"long\"))\n",
    "        .withColumn(\"row_num\", row_number().over(approved_curations_window))\n",
    "        .filter(col(\"row_num\") == 1)  # Get most recent curation per source\n",
    "        .select(\n",
    "            col(\"source_id\"),\n",
    "            col(\"property\"),\n",
    "            col(\"property_value\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Pivot curations to get oa_flip_year and is_oa columns\n",
    "    curations_pivoted = (\n",
    "        approved_curations\n",
    "        .groupBy(\"source_id\")\n",
    "        .agg(\n",
    "            max(when(col(\"property\") == \"oa_flip_year\", col(\"property_value\").cast(\"int\"))).alias(\"curation_oa_flip_year\"),\n",
    "            max(when(col(\"property\") == \"is_oa\", col(\"property_value\").cast(\"boolean\"))).alias(\"curation_is_oa\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # get curation requests and convert to high OA rate table format\n",
    "    base_high_oa_rate_issns = (\n",
    "        spark.table(\"openalex.sources.high_oa_rate_issns\")\n",
    "        .select(\"issn_l\", \"oa_year\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Get most recent approved curation per ISSN (even if sets value to false)\n",
    "    window = Window.partitionBy(\"issn\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "\n",
    "    curation_requests = (\n",
    "        spark.table(\"openalex.unpaywall.journal_curation_requests\")\n",
    "        .filter(col(\"approved\") == \"yes\")\n",
    "        .withColumn(\"row_num\", row_number().over(window))\n",
    "        .filter(col(\"row_num\") == 1)\n",
    "        .select(\n",
    "            col(\"issn\").alias(\"issn_l\"),\n",
    "            col(\"new_is_oa\").alias(\"is_oa_high_oa_rate\"),\n",
    "            col(\"new_oa_date\").cast(\"int\").alias(\"high_oa_rate_start_year\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Apply curation overrides â€” if curation exists, it takes full priority\n",
    "    high_oa_rate_issns = (\n",
    "        base_high_oa_rate_issns.alias(\"base\")\n",
    "        .join(curation_requests.alias(\"cur\"), on=\"issn_l\", how=\"outer\")\n",
    "        .select(\n",
    "            coalesce(col(\"cur.issn_l\"), col(\"base.issn_l\")).alias(\"issn_l\"),\n",
    "            when(col(\"cur.is_oa_high_oa_rate\").isNotNull(), col(\"cur.is_oa_high_oa_rate\"))\n",
    "                .otherwise(col(\"base.oa_year\").isNotNull()).alias(\"is_oa_high_oa_rate\"),\n",
    "            when(col(\"cur.issn_l\").isNotNull(), col(\"cur.high_oa_rate_start_year\"))\n",
    "                .otherwise(col(\"base.oa_year\")).alias(\"high_oa_rate_start_year\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # process records with and without ISSNs\n",
    "    sources_with_issns = (\n",
    "        base_combined\n",
    "        .filter(col(\"issns\").isNotNull() & (size(col(\"issns\")) > 0))\n",
    "        .withColumn(\"exploded_issn\", explode(col(\"issns\")))\n",
    "        .join(doaj, col(\"exploded_issn\") == doaj[\"doaj_issn\"], \"left\")\n",
    "        .join(jstage, col(\"exploded_issn\") == jstage[\"jstage_issn\"], \"left\")\n",
    "        .join(scielo, col(\"exploded_issn\") == scielo[\"scielo_issn\"], \"left\")\n",
    "        .join(ojs, col(\"exploded_issn\") == ojs[\"ojs_issn\"], \"left\")\n",
    "        .join(high_oa_rate_issns, col(\"exploded_issn\") == high_oa_rate_issns[\"issn_l\"], \"left\")\n",
    "        .withColumn(\"is_in_doaj\", \n",
    "                    when(doaj[\"doaj_issn\"].isNotNull(), True).otherwise(False))\n",
    "        .withColumn(\"is_in_scielo\", \n",
    "                when(scielo[\"scielo_issn\"].isNotNull(), True).otherwise(False))\n",
    "        .withColumn(\"is_ojs\", \n",
    "                    when(ojs[\"ojs_issn\"].isNotNull(), True).otherwise(False))\n",
    "        .withColumn(\"is_oa_high_oa_rate\", \n",
    "            when(lower(col(\"publisher\")).rlike(\"^(mdpi|academic journals|edorium journals)\"), True)\n",
    "            .when(scielo[\"scielo_issn\"].isNotNull(), True)\n",
    "            .when(ojs[\"ojs_is_oa\"] == True, True)\n",
    "            .otherwise(coalesce(high_oa_rate_issns[\"is_oa_high_oa_rate\"], lit(False))))\n",
    "        .withColumn(\"high_oa_rate_start_year\",\n",
    "            when(lower(col(\"publisher\")).rlike(\"^(mdpi|academic journals|edorium journals)\"), lit(None).cast(\"int\"))\n",
    "            .when(scielo[\"scielo_issn\"].isNotNull(), lit(None).cast(\"int\"))\n",
    "            .when(ojs[\"ojs_is_oa\"] == True, lit(None).cast(\"int\"))\n",
    "            .otherwise(high_oa_rate_issns[\"high_oa_rate_start_year\"]))\n",
    "        .groupBy(\"id\", *[c for c in base_combined.columns if c not in [\"id\", \"issns\"]])\n",
    "        .agg( # make sure the issns are sorted and deduplicated - avoid non-deterministic sorting\n",
    "            array_sort(collect_set(\"exploded_issn\")).alias(\"issns\"),\n",
    "            \n",
    "            # doaj\n",
    "            max(when(col(\"doaj_issn\").isNotNull(), lit(True)).otherwise(lit(False))).alias(\"is_in_doaj\"),\n",
    "            min(col(\"oa_start_year\")).alias(\"is_in_doaj_start_year\"),\n",
    "            first(col(\"doaj_license_normalized\"), ignorenulls=True).alias(\"doaj_license\"),\n",
    "\n",
    "            # scielo\n",
    "            max(when(col(\"scielo_issn\").isNotNull(), lit(True)).otherwise(lit(False))).alias(\"is_in_scielo\"),\n",
    "\n",
    "            # ojs\n",
    "            max(when(col(\"ojs_issn\").isNotNull(), lit(True)).otherwise(lit(False))).alias(\"is_ojs\"),\n",
    "\n",
    "            # j-stage data for later use\n",
    "            max(col(\"jstage_oa_start_year\")).alias(\"jstage_oa_start_year\"),\n",
    "            max(col(\"jstage_oa_end_year\")).alias(\"jstage_oa_end_year\"),\n",
    "            max(when(col(\"jstage_issn\").isNotNull(), lit(True)).otherwise(lit(False))).alias(\"has_jstage_issn\"),\n",
    "\n",
    "            # high oa rate\n",
    "            max(\n",
    "                when(lower(col(\"publisher\")).rlike(\"^(mdpi|academic journals|edorium journals)\"), lit(True))\n",
    "                .when(col(\"scielo_issn\").isNotNull(), lit(True))\n",
    "                .when(col(\"ojs_is_oa\") == True, lit(True))\n",
    "                .otherwise(coalesce(col(\"is_oa_high_oa_rate\"), lit(False)))\n",
    "            ).alias(\"is_oa_high_oa_rate\"),\n",
    "\n",
    "            # high oa rate start year\n",
    "            min(\n",
    "                when(lower(col(\"publisher\")).rlike(\"^(mdpi|academic journals|edorium journals)\"), lit(None).cast(\"int\"))\n",
    "                .when(col(\"scielo_issn\").isNotNull(), lit(None).cast(\"int\"))\n",
    "                .when(col(\"ojs_is_oa\") == True, lit(None).cast(\"int\"))\n",
    "                .otherwise(col(\"high_oa_rate_start_year\"))\n",
    "            ).alias(\"high_oa_rate_start_year\")\n",
    "        )\n",
    "        .drop(\"doaj_issn\", \"jstage_issn\", \"scielo_issn\", \"ojs_issn\", \"issn_l\")\n",
    "        .withColumn(\"issn\", when(size(col(\"issns\")) > 0, col(\"issns\")[0]).otherwise(lit(None)))\n",
    "        .withColumn(\"rank\", row_number().over( # deduplicate by ISSN\n",
    "            Window.partitionBy(\"issn\").orderBy(\n",
    "                size(\"issns\").desc(),\n",
    "                length(\"display_name\").desc(),\n",
    "                col(\"id\").asc()\n",
    "            )\n",
    "        ))\n",
    "        .filter((col(\"issn\").isNull()) | (col(\"rank\") == 1))\n",
    "        .drop(\"rank\")\n",
    "    )\n",
    "    \n",
    "    sources_null_issns = (\n",
    "        base_combined\n",
    "        .filter((col(\"issns\").isNull()) | (size(col(\"issns\")) == 0))\n",
    "        .withColumn(\"is_in_doaj\", lit(False))\n",
    "        .withColumn(\"doaj_license\", lit(None).cast(\"string\"))\n",
    "        .withColumn(\"is_in_scielo\", lit(False))\n",
    "        .withColumn(\"is_ojs\", lit(False))\n",
    "        .withColumn(\"jstage_oa_start_year\", lit(None).cast(\"int\"))\n",
    "        .withColumn(\"jstage_oa_end_year\", lit(None).cast(\"int\"))\n",
    "        .withColumn(\"has_jstage_issn\", lit(False))\n",
    "        .withColumn(\"is_oa_high_oa_rate\", \n",
    "            when(lower(col(\"publisher\")).rlike(\"^(mdpi|academic journals|edorium journals)\"), True)\n",
    "            .otherwise(lit(False)))\n",
    "        .withColumn(\"oa_start_year\", lit(None).cast(\"int\"))\n",
    "        .withColumn(\"high_oa_rate_start_year\", lit(None).cast(\"int\"))\n",
    "        .withColumnRenamed(\"oa_start_year\", \"is_in_doaj_start_year\")\n",
    "        .withColumn(\"datacite_id\", col(\"datacite_id\"))\n",
    "    )\n",
    "    \n",
    "    # combine ISSN and non-ISSN records\n",
    "    sources_with_doaj_jstage_and_oa = (\n",
    "        sources_with_issns\n",
    "        .unionByName(sources_null_issns)\n",
    "        .withColumn(\"rank\", row_number().over(\n",
    "            Window.partitionBy(\"id\").orderBy(\n",
    "                # prefer records that have ISSNs in de-duplication\n",
    "                (col(\"issns\").isNotNull() & (size(col(\"issns\")) > 0)).desc(),\n",
    "                col(\"updated_date\").desc(),\n",
    "                length(\"display_name\").desc(),\n",
    "                col(\"created_date\").desc()\n",
    "            )\n",
    "        ))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .drop(\"rank\", \"has_issns\")\n",
    "    )\n",
    "\n",
    "    sources_with_curations = (\n",
    "        sources_with_doaj_jstage_and_oa\n",
    "        .join(curations_pivoted, sources_with_doaj_jstage_and_oa[\"id\"] == curations_pivoted[\"source_id\"], \"left\")\n",
    "        .withColumn(\"high_oa_rate_start_year\",\n",
    "            when(col(\"curation_is_oa\") == False, lit(None))\n",
    "            .when(col(\"curation_oa_flip_year\").isNotNull(), col(\"curation_oa_flip_year\") + 1)\n",
    "            .otherwise(col(\"high_oa_rate_start_year\")))\n",
    "        .withColumn(\"is_oa_high_oa_rate\",\n",
    "            when(col(\"curation_is_oa\").isNotNull(), col(\"curation_is_oa\"))\n",
    "            .when(col(\"high_oa_rate_start_year\").isNotNull(), lit(True))\n",
    "            .otherwise(col(\"is_oa_high_oa_rate\")))\n",
    "        .drop(\"source_id\", \"curation_oa_flip_year\", \"curation_is_oa\")\n",
    "    )\n",
    "\n",
    "    # update is_oa column based on is_oa_high_oa_rate\n",
    "    sources_with_updated_oa = (\n",
    "        sources_with_curations\n",
    "        .withColumn(\"is_oa\",\n",
    "                    when(col(\"is_in_doaj\") == True, True)\n",
    "                    .when(col(\"is_in_scielo\") == True, True)\n",
    "                    .when(col(\"is_oa_high_oa_rate\") == True, True)\n",
    "                    .when(col(\"is_oa_high_oa_rate\") == False, False)\n",
    "                    .otherwise(col(\"is_oa\")))\n",
    "    )\n",
    "\n",
    "    sources_api_years = (\n",
    "        spark.table(\"openalex.sources.sources_api\")\n",
    "        .select(\"id\", \"first_publication_year\", \"last_publication_year\")\n",
    "    )\n",
    "\n",
    "    sources_with_jstage = (\n",
    "        sources_with_updated_oa\n",
    "        .join(sources_api_years, \"id\", \"left\")\n",
    "        .withColumn(\n",
    "            \"is_fully_open_in_jstage\",\n",
    "            (\n",
    "                col(\"has_jstage_issn\") &\n",
    "                col(\"first_publication_year\").cast(\"int\").isNotNull() &\n",
    "                col(\"last_publication_year\").cast(\"int\").isNotNull() &\n",
    "                col(\"jstage_oa_start_year\").cast(\"int\").isNotNull() &\n",
    "                col(\"jstage_oa_end_year\").cast(\"int\").isNotNull() &\n",
    "                (col(\"jstage_oa_start_year\").cast(\"int\") <= col(\"first_publication_year\").cast(\"int\")) &\n",
    "                (col(\"jstage_oa_end_year\").cast(\"int\") >= col(\"last_publication_year\").cast(\"int\"))\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_oa\",\n",
    "            when(col(\"is_in_doaj\"), True)\n",
    "            .when(col(\"is_fully_open_in_jstage\"), True)\n",
    "            .when(col(\"is_in_scielo\"), True)\n",
    "            .when(col(\"is_oa_high_oa_rate\"), True)\n",
    "            .when(~col(\"is_oa_high_oa_rate\"), False)\n",
    "            .otherwise(col(\"is_oa\"))\n",
    "        )\n",
    "        .drop(\n",
    "            \"first_publication_year\", \"last_publication_year\",\n",
    "            \"has_jstage_issn\", \"jstage_oa_start_year\", \"jstage_oa_end_year\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    # join with endpoint mapping\n",
    "    endpoints_table = (\n",
    "        spark.table(\"openalex.sources.endpoint_mapping\")\n",
    "        .select(\"endpoint_id\", \"sample_pmh_record\")\n",
    "    )\n",
    "\n",
    "    # join with datacite id mapping to get all datacite_ids for a given openalex_id\n",
    "    datacite_id_mapping = (\n",
    "        spark.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "        .groupBy(\"openalex_id\")\n",
    "        .agg(\n",
    "            array_sort(collect_set(\"datacite_id\")).alias(\"all_datacite_ids\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_result = (\n",
    "        sources_with_jstage\n",
    "        .join(\n",
    "            endpoints_table,\n",
    "            sources_with_jstage[\"repository_id\"] == endpoints_table[\"endpoint_id\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .join(\n",
    "            datacite_id_mapping,\n",
    "            sources_with_jstage[\"id\"] == datacite_id_mapping[\"openalex_id\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .withColumn(\"datacite_ids\", \n",
    "            when(col(\"datacite_id\").isNotNull(), \n",
    "                array(col(\"datacite_id\")))\n",
    "            .when(col(\"all_datacite_ids\").isNotNull(), \n",
    "                col(\"all_datacite_ids\"))\n",
    "            .otherwise(array())\n",
    "        )\n",
    "        .withColumn(\"type\",\n",
    "            when(\n",
    "                lower(col(\"display_name\")).contains(\"rxiv\") |\n",
    "                lower(col(\"display_name\")).contains(\"research square\"), \n",
    "                \"repository\"\n",
    "            )\n",
    "            .otherwise(col(\"type\"))\n",
    "        )\n",
    "        # remove \"Deleted Journal\" and other bad journal from sources\n",
    "        .filter(~col(\"id\").isin(4317411217, 4363604846))\n",
    "        .drop(\"endpoint_id\", \"openalex_id\", \"all_datacite_ids\")\n",
    "    )\n",
    "\n",
    "    # add preprints sources, need to move this to separate table\n",
    "    preprints_org = spark.createDataFrame([\n",
    "        {\n",
    "            \"id\": 6309402219,\n",
    "            \"display_name\": \"Preprints.org\",\n",
    "            \"publisher\": \"MDPI AG\",\n",
    "            \"webpage\": \"https://preprints.org\",\n",
    "            \"type\": \"repository\",\n",
    "            \"publisher_id\": 4310310987,\n",
    "            \"is_oa\": True,\n",
    "            \"is_oa_high_oa_rate\": True,\n",
    "            \"is_in_doaj\": False,\n",
    "            \"is_in_scielo\": False,\n",
    "            \"is_ojs\": False,\n",
    "            \"is_fully_open_in_jstage\": False,\n",
    "            \"updated_date\": \"2025-08-12\",\n",
    "            \"created_date\": \"2025-08-12\"\n",
    "        }\n",
    "    ]).withColumn(\"datacite_ids\", array())\n",
    "\n",
    "    return final_result.unionByName(preprints_org, allowMissingColumns=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateSources",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
