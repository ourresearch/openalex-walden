{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269538ed-30e5-4c65-83e4-573fc5ae04c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install /Volumes/openalex/default/libraries/openalex_dlt_utils-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f435e08-52b6-4f0d-bc5a-b30fd4e34cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import builtins\n",
    "\n",
    "from openalex.utils.environment import *\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"base_sources\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment=\"Sources from original postgresql table.\"\n",
    ")\n",
    "def base_sources():\n",
    "    apc_schema = ArrayType(StructType([\n",
    "        StructField(\"price\", IntegerType(), True),\n",
    "        StructField(\"currency\", StringType(), True)\n",
    "    ]))\n",
    "    \n",
    "    societies_schema = ArrayType(StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"organization\", StringType(), True)\n",
    "    ]))\n",
    "\n",
    "    return (spark.table(\"openalex.sources.sources_from_postgres\")\n",
    "        .drop(\"is_in_doaj\")\n",
    "        .withColumn(\n",
    "            'issns',\n",
    "            when(col('issns').isNull(), None)\n",
    "            .otherwise(from_json(col('issns'), ArrayType(StringType())))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'alternate_titles',\n",
    "            when(col('alternate_titles').isNull(), None)\n",
    "            .otherwise(from_json(col('alternate_titles'), ArrayType(StringType())))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'apc_prices',\n",
    "            when(col('apc_prices').isNull(), None)\n",
    "            .otherwise(from_json(col('apc_prices'), apc_schema))\n",
    "        )\n",
    "        .withColumn(\n",
    "            'societies',\n",
    "            when(col('societies').isNull(), None)\n",
    "            .otherwise(from_json(col('societies'), societies_schema))\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"crossref_id_mapping\",\n",
    "    comment=\"Stable ID mapping for crossref journals (reads from persistent table)\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def crossref_id_mapping():\n",
    "    \"\"\"Create/maintain mapping between crossref sources and stable OpenAlex IDs\"\"\"\n",
    "    \n",
    "    persistent_table = \"openalex.sources.crossref_id_mapping_persistent\"\n",
    "    \n",
    "    # check if persistent mapping table already exists\n",
    "    try:\n",
    "        existing_mapping = spark.table(persistent_table)\n",
    "        mapping_exists = True\n",
    "    except:\n",
    "        existing_mapping = None\n",
    "        mapping_exists = False\n",
    "    \n",
    "    crossref_sources = (spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "                       .select(\"issns_concat_id\", \"title\")\n",
    "                       .distinct())\n",
    "    \n",
    "    if mapping_exists:\n",
    "        new_sources = (crossref_sources\n",
    "                      .join(existing_mapping, \"issns_concat_id\", \"left_anti\"))\n",
    "        \n",
    "        if new_sources.count() > 0:\n",
    "            max_mapping_id = existing_mapping.agg(max(\"openalex_id\")).collect()[0][0]\n",
    "            \n",
    "            max_postgres_id = spark.table(\"openalex.sources.sources_from_postgres\").agg(max(\"id\")).collect()[0][0]\n",
    "            \n",
    "            # Use the higher of the two, plus 1 for next available\n",
    "            max_mapping_safe = max_mapping_id if max_mapping_id is not None else 0\n",
    "            max_postgres_safe = max_postgres_id if max_postgres_id is not None else 0\n",
    "            next_id = builtins.max(max_mapping_safe, max_postgres_safe) + 1\n",
    "            \n",
    "            # create new mappings with deterministic ordering\n",
    "            new_mappings = (new_sources\n",
    "                           .orderBy(\"issns_concat_id\")\n",
    "                           .withColumn(\"row_num\", row_number().over(\n",
    "                               Window.orderBy(\"issns_concat_id\")\n",
    "                           ))\n",
    "                           .withColumn(\"openalex_id\", col(\"row_num\") + lit(next_id - 1))\n",
    "                           .withColumn(\"created_date\", current_timestamp())\n",
    "                           .select(\"issns_concat_id\", \"openalex_id\", \"created_date\"))\n",
    "            \n",
    "            final_mapping = existing_mapping.unionByName(new_mappings)\n",
    "        else:\n",
    "            final_mapping = existing_mapping\n",
    "    else:\n",
    "        max_postgres_id = spark.table(\"openalex.sources.sources_from_postgres\").agg(max(\"id\")).collect()[0][0]\n",
    "        \n",
    "        max_postgres_safe = max_postgres_id if max_postgres_id is not None else 0\n",
    "        start_id = max_postgres_safe + 1000000000\n",
    "        \n",
    "        # create initial mapping\n",
    "        final_mapping = (crossref_sources\n",
    "                        .orderBy(\"issns_concat_id\")\n",
    "                        .withColumn(\"row_num\", row_number().over(\n",
    "                            Window.orderBy(\"issns_concat_id\")\n",
    "                        ))\n",
    "                        .withColumn(\"openalex_id\", col(\"row_num\") + lit(start_id - 1))\n",
    "                        .withColumn(\"created_date\", current_timestamp())\n",
    "                        .select(\"issns_concat_id\", \"openalex_id\", \"created_date\"))\n",
    "    \n",
    "    # save to persistent table (survives full refresh)\n",
    "    (final_mapping\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .saveAsTable(persistent_table))\n",
    "    \n",
    "    return final_mapping\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"datacite_id_mapping_extended\",\n",
    "    comment=\"Stable ID mapping for unmatched datacite sources (reads from persistent table)\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def datacite_id_mapping_extended():\n",
    "    \"\"\"Create/maintain mapping for datacite sources that don't have existing OpenAlex IDs\"\"\"\n",
    "    \n",
    "    persistent_table = \"openalex.sources.datacite_id_mapping_extended_persistent\"\n",
    "    \n",
    "    try:\n",
    "        existing_mapping = spark.table(persistent_table)\n",
    "        mapping_exists = True\n",
    "    except:\n",
    "        existing_mapping = None\n",
    "        mapping_exists = False\n",
    "    \n",
    "    # get datacite sources that don't have existing OpenAlex mappings in datacite_to_openalex_mapping table (not extended mapping)\n",
    "    datacite_sources = spark.read.table(\"openalex.sources.datacite_sources\")\n",
    "    existing_openalex_mapping = spark.read.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "    \n",
    "    unmatched_sources = (datacite_sources\n",
    "                        .join(existing_openalex_mapping, \n",
    "                              datacite_sources[\"id\"] == existing_openalex_mapping[\"datacite_id\"], \n",
    "                              \"left\")\n",
    "                        .filter(col(\"openalex_id\").isNull())\n",
    "                        .select(datacite_sources[\"id\"].alias(\"datacite_id\"), \"display_name\")\n",
    "                        .distinct())\n",
    "    \n",
    "    if mapping_exists:\n",
    "        # get sources not in existing extended mapping\n",
    "        new_sources = (unmatched_sources\n",
    "                      .join(existing_mapping, \"datacite_id\", \"left_anti\"))\n",
    "        \n",
    "        if new_sources.count() > 0:\n",
    "            max_mapping_id = existing_mapping.agg(max(\"openalex_id\")).collect()[0][0]\n",
    "            \n",
    "            max_postgres_id = spark.table(\"openalex.sources.sources_from_postgres\").agg(max(\"id\")).collect()[0][0]\n",
    "            \n",
    "            try:\n",
    "                max_crossref_id = spark.table(\"openalex.sources.crossref_id_mapping_persistent\").agg(max(\"openalex_id\")).collect()[0][0]\n",
    "            except:\n",
    "                max_crossref_id = 0\n",
    "            \n",
    "            # use the highest ID found, plus 1 for next available\n",
    "            max_mapping_safe = max_mapping_id if max_mapping_id is not None else 0\n",
    "            max_postgres_safe = max_postgres_id if max_postgres_id is not None else 0\n",
    "            max_crossref_safe = max_crossref_id if max_crossref_id is not None else 0\n",
    "            next_id = builtins.max(max_mapping_safe, max_postgres_safe, max_crossref_safe) + 1\n",
    "            \n",
    "            new_mappings = (new_sources\n",
    "                           .orderBy(\"datacite_id\")\n",
    "                           .withColumn(\"row_num\", row_number().over(\n",
    "                               Window.orderBy(\"datacite_id\")\n",
    "                           ))\n",
    "                           .withColumn(\"openalex_id\", col(\"row_num\") + lit(next_id - 1))\n",
    "                           .withColumn(\"created_date\", current_timestamp())\n",
    "                           .select(\"datacite_id\", \"openalex_id\", \"created_date\"))\n",
    "            \n",
    "            final_mapping = existing_mapping.unionByName(new_mappings)\n",
    "        else:\n",
    "            final_mapping = existing_mapping\n",
    "    else:\n",
    "        max_postgres_id = spark.table(\"openalex.sources.sources_from_postgres\").agg(max(\"id\")).collect()[0][0]\n",
    "        \n",
    "        try:\n",
    "            max_crossref_id = spark.table(\"openalex.sources.crossref_id_mapping_persistent\").agg(max(\"openalex_id\")).collect()[0][0]\n",
    "        except:\n",
    "            max_crossref_id = 0\n",
    "        \n",
    "        # start well above existing IDs - add 2 billion for safety buffer (different from crossref)\n",
    "        max_postgres_safe = max_postgres_id if max_postgres_id is not None else 0\n",
    "        max_crossref_safe = max_crossref_id if max_crossref_id is not None else 0\n",
    "        start_id = builtins.max(max_postgres_safe, max_crossref_safe) + 2000000000\n",
    "        \n",
    "        # create initial mapping\n",
    "        final_mapping = (unmatched_sources\n",
    "                        .orderBy(\"datacite_id\")\n",
    "                        .withColumn(\"row_num\", row_number().over(\n",
    "                            Window.orderBy(\"datacite_id\")\n",
    "                        ))\n",
    "                        .withColumn(\"openalex_id\", col(\"row_num\") + lit(start_id - 1))\n",
    "                        .withColumn(\"created_date\", current_timestamp())\n",
    "                        .select(\"datacite_id\", \"openalex_id\", \"created_date\"))\n",
    "    \n",
    "    # save to persistent table (survives full refresh)\n",
    "    (final_mapping\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .saveAsTable(persistent_table))\n",
    "    \n",
    "    return final_mapping\n",
    "\n",
    "@dlt.table(\n",
    "   name=\"crossref_journals_unmatched\",\n",
    "   comment=\"Crossref journals that have NO matching ISSNs with existing base sources\"\n",
    ")\n",
    "def crossref_journals_unmatched():\n",
    "    # get all ISSNs from base sources\n",
    "    base_issns = (dlt.read(\"base_sources\")\n",
    "                 .select(\"id\", explode(\"issns\").alias(\"issn\"))\n",
    "                 .select(\"issn\")\n",
    "                 .distinct())\n",
    "    \n",
    "    # get crossref journals with their exploded ISSNs\n",
    "    crossref_with_issns = (spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "                          .select(\"*\", explode(\"issns\").alias(\"issn\")))\n",
    "    \n",
    "    # find crossref journals that have at least one matching ISSN\n",
    "    crossref_with_matches = (crossref_with_issns\n",
    "                           .join(base_issns, \"issn\", \"inner\")\n",
    "                           .select(\"issns_concat_id\")\n",
    "                           .distinct())\n",
    "    \n",
    "    # find crossref journals that have NO matching ISSNs\n",
    "    crossref_completely_unmatched = (spark.table(\"openalex.sources.crossref_journals_gold\")\n",
    "                                   .join(crossref_with_matches, \"issns_concat_id\", \"left_anti\"))\n",
    "    \n",
    "    # join with the stable ID mapping\n",
    "    id_mapping = dlt.read(\"crossref_id_mapping\")\n",
    "    \n",
    "    return (crossref_completely_unmatched\n",
    "           .join(id_mapping, \"issns_concat_id\", \"inner\")\n",
    "           .select(\n",
    "               col(\"openalex_id\").alias(\"id\"),\n",
    "               col(\"title\").alias(\"display_name\"),\n",
    "               col(\"issns\"),\n",
    "               col(\"publisher\"),\n",
    "               lit(False).alias(\"is_oa\"),\n",
    "               lit(\"journal\").alias(\"type\")\n",
    "           )\n",
    "           .withColumn(\"issn\", when(size(col(\"issns\")) > 0, col(\"issns\")[0]).otherwise(lit(None)))\n",
    "    )\n",
    "    \n",
    "@dlt.table(\n",
    "    name=\"datacite_sources_unmatched\",\n",
    "    comment=\"Datacite sources that have NO match existing sources\"\n",
    ")\n",
    "def datacite_sources_unmatched():\n",
    "    df = spark.read.table(\"openalex.sources.datacite_sources\")\n",
    "    df_mapping = spark.read.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "    \n",
    "    # unmatched datacite sources\n",
    "    unmatched_df = df.join(\n",
    "        df_mapping,\n",
    "        col(\"id\") == col(\"datacite_id\"),\n",
    "        \"left\"\n",
    "    ).filter(col(\"openalex_id\").isNull())\n",
    "\n",
    "    # join with the stable ID mapping - alias the openalex_id to avoid ambiguity\n",
    "    id_mapping = dlt.read(\"datacite_id_mapping_extended\").select(\n",
    "        col(\"datacite_id\").alias(\"mapping_datacite_id\"),\n",
    "        col(\"openalex_id\").alias(\"new_openalex_id\"),\n",
    "        col(\"created_date\").alias(\"mapping_created_date\")\n",
    "    )\n",
    "\n",
    "    return (unmatched_df\n",
    "        .join(id_mapping, \n",
    "              unmatched_df[\"id\"] == id_mapping[\"mapping_datacite_id\"], \n",
    "              \"inner\")\n",
    "        .select(\n",
    "            col(\"new_openalex_id\").alias(\"id\"),\n",
    "            col(\"id\").alias(\"datacite_id\"),\n",
    "            col(\"display_name\"),\n",
    "            array_distinct(\n",
    "                array_compact(\n",
    "                    array(\n",
    "                        col(\"issns.issnl\"),\n",
    "                        col(\"issns.print\"),\n",
    "                        col(\"issns.electronic\")\n",
    "                    )\n",
    "                )\n",
    "            ).alias(\"issns\"),\n",
    "            col(\"provider_name\"),\n",
    "            col(\"type\")\n",
    "        )\n",
    "        .withColumn(\"issn\", when(size(col(\"issns\")) > 0, col(\"issns\")[0]).otherwise(lit(None)))\n",
    "        .withColumn(\"publisher\", col(\"provider_name\"))\n",
    "        .withColumn(\"is_oa\", lit(True))\n",
    "        .withColumn(\"type\", \n",
    "            when(col(\"type\") == \"periodical\", \"journal\")\n",
    "            .otherwise(col(\"type\"))\n",
    "        )\n",
    "        .withColumn(\"updated_date\", current_timestamp())\n",
    "        .withColumn(\"created_date\", current_timestamp().cast(\"string\"))\n",
    "        .drop(\"provider_name\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"sources\",\n",
    "    comment=f\"Combined sources with DOAJ status and sample PMH records in {ENV.upper()}\"\n",
    ")\n",
    "def sources():\n",
    "    # combine base sources with unmatched crossref and datacite sources\n",
    "    base_combined = (\n",
    "        dlt.read(\"base_sources\")\n",
    "        .unionByName(\n",
    "            dlt.read(\"crossref_journals_unmatched\"),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "        .unionByName(\n",
    "            dlt.read(\"datacite_sources_unmatched\"),\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # get DOAJ ISSNs\n",
    "    doaj = (\n",
    "        spark.table(\"openalex.sources.doaj_from_csv\")\n",
    "        .selectExpr(\"explode(issns) as doaj_issn\", \"oa_start_year\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # get curation requests and convert to high OA rate table format\n",
    "    base_high_oa_rate_issns = (\n",
    "        spark.table(\"openalex.sources.high_oa_rate_issns\")\n",
    "        .select(\"issn_l\", \"oa_year\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Get most recent approved curation per ISSN (even if sets value to false)\n",
    "    window = Window.partitionBy(\"issn\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "\n",
    "    curation_requests = (\n",
    "        spark.table(\"openalex.unpaywall.journal_curation_requests\")\n",
    "        .filter(col(\"approved\") == \"yes\")\n",
    "        .withColumn(\"row_num\", row_number().over(window))\n",
    "        .filter(col(\"row_num\") == 1)\n",
    "        .select(\n",
    "            col(\"issn\").alias(\"issn_l\"),\n",
    "            col(\"new_is_oa\").alias(\"is_oa_high_oa_rate\"),\n",
    "            col(\"new_oa_date\").cast(\"int\").alias(\"high_oa_rate_start_year\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Apply curation overrides — if curation exists, it takes full priority\n",
    "    high_oa_rate_issns = (\n",
    "        base_high_oa_rate_issns.alias(\"base\")\n",
    "        .join(curation_requests.alias(\"cur\"), on=\"issn_l\", how=\"outer\")\n",
    "        .select(\n",
    "            coalesce(col(\"cur.issn_l\"), col(\"base.issn_l\")).alias(\"issn_l\"),\n",
    "            when(col(\"cur.is_oa_high_oa_rate\").isNotNull(), col(\"cur.is_oa_high_oa_rate\"))\n",
    "                .otherwise(col(\"base.oa_year\").isNotNull()).alias(\"is_oa_high_oa_rate\"),\n",
    "            when(col(\"cur.issn_l\").isNotNull(), col(\"cur.high_oa_rate_start_year\"))\n",
    "                .otherwise(col(\"base.oa_year\")).alias(\"high_oa_rate_start_year\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # process records with and without ISSNs\n",
    "    sources_with_issns = (\n",
    "        base_combined\n",
    "        .filter(col(\"issns\").isNotNull() & (size(col(\"issns\")) > 0))\n",
    "        .withColumn(\"exploded_issn\", explode(col(\"issns\")))\n",
    "        .join(doaj, col(\"exploded_issn\") == doaj[\"doaj_issn\"], \"left\")\n",
    "        .join(high_oa_rate_issns, col(\"exploded_issn\") == high_oa_rate_issns[\"issn_l\"], \"left\")\n",
    "        .withColumn(\"is_in_doaj\", \n",
    "                    when(doaj[\"doaj_issn\"].isNotNull(), True).otherwise(False))\n",
    "        .withColumn(\"is_oa_high_oa_rate\", \n",
    "            when((lower(col(\"publisher\")).startswith(\"mdpi\")) & (col(\"is_in_doaj\") == False), True)\n",
    "            .otherwise(coalesce(high_oa_rate_issns[\"is_oa_high_oa_rate\"], lit(False))))\n",
    "        .drop(\"doaj_issn\", \"issn_l\")\n",
    "        .groupBy(\"id\", *[c for c in base_combined.columns if c not in [\"id\", \"issns\"]])\n",
    "        .agg( # make sure the issns are sorted and deduplicated - avoid non-deterministic sorting\n",
    "            array_sort(collect_set(\"exploded_issn\")).alias(\"issns\"),\n",
    "            max(\"is_in_doaj\").alias(\"is_in_doaj\"),\n",
    "            max(\"is_oa_high_oa_rate\").alias(\"is_oa_high_oa_rate\"),\n",
    "            max(\"oa_start_year\").alias(\"oa_start_year\"),\n",
    "            max(\"high_oa_rate_start_year\").alias(\"high_oa_rate_start_year\")\n",
    "        )        \n",
    "        .withColumnRenamed(\"oa_start_year\", \"is_in_doaj_start_year\")\n",
    "        .withColumn(\"rank\", row_number().over( # deduplicate by ISSN\n",
    "            Window.partitionBy(\"issn\").orderBy(\n",
    "                size(\"issns\").desc(),\n",
    "                length(\"display_name\").desc()\n",
    "            )\n",
    "        ))\n",
    "        .filter((col(\"issn\").isNull()) | (col(\"rank\") == 1))\n",
    "        .drop(\"rank\")\n",
    "    )\n",
    "    \n",
    "    sources_null_issns = (\n",
    "        base_combined\n",
    "        .filter((col(\"issns\").isNull()) | (size(col(\"issns\")) == 0))\n",
    "        .withColumn(\"is_in_doaj\", lit(False))\n",
    "        .withColumn(\"is_oa_high_oa_rate\", \n",
    "            when(lower(col(\"publisher\")).startswith(\"mdpi\"), True)\n",
    "            .otherwise(lit(False)))\n",
    "        .withColumn(\"oa_start_year\", lit(None).cast(\"int\"))\n",
    "        .withColumn(\"high_oa_rate_start_year\", lit(None).cast(\"int\"))\n",
    "        .withColumnRenamed(\"oa_start_year\", \"is_in_doaj_start_year\")\n",
    "        .withColumn(\"datacite_id\", col(\"datacite_id\"))\n",
    "    )\n",
    "    \n",
    "    # combine ISSN and non-ISSN records\n",
    "    sources_with_doaj_and_oa = (\n",
    "        sources_with_issns\n",
    "        .unionByName(sources_null_issns)\n",
    "        .withColumn(\"rank\", row_number().over(\n",
    "            Window.partitionBy(\"id\").orderBy(\n",
    "                # prefer records that have ISSNs in de-duplication\n",
    "                (col(\"issns\").isNotNull() & (size(col(\"issns\")) > 0)).desc(),\n",
    "                col(\"updated_date\").desc(),\n",
    "                length(\"display_name\").desc()  # optional: tie-breaker\n",
    "            )\n",
    "        ))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .drop(\"rank\", \"has_issns\")\n",
    "    )\n",
    "\n",
    "    # update is_oa column based on is_oa_high_oa_rate\n",
    "    sources_with_updated_oa = (\n",
    "        sources_with_doaj_and_oa\n",
    "        .withColumn(\"is_oa\",\n",
    "                    when(col(\"is_in_doaj\") == True, True)\n",
    "                    .when(col(\"is_oa_high_oa_rate\") == True, True)\n",
    "                    .when(col(\"is_oa_high_oa_rate\") == False, False)\n",
    "                    .otherwise(col(\"is_oa\")))\n",
    "    )\n",
    "    \n",
    "    # join with endpoint mapping\n",
    "    endpoints_table = (\n",
    "        spark.table(\"openalex.sources.endpoint_mapping\")\n",
    "        .select(\"endpoint_id\", \"sample_pmh_record\")\n",
    "    )\n",
    "\n",
    "    # join with datacite id mapping to get all datacite_ids for a given openalex_id\n",
    "    datacite_id_mapping = (\n",
    "        spark.table(\"openalex.sources.datacite_to_openalex_mapping\")\n",
    "        .groupBy(\"openalex_id\")\n",
    "        .agg(\n",
    "            collect_set(\"datacite_id\").alias(\"all_datacite_ids\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_result = (\n",
    "        sources_with_updated_oa\n",
    "        .join(\n",
    "            endpoints_table,\n",
    "            sources_with_updated_oa[\"repository_id\"] == endpoints_table[\"endpoint_id\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .join(\n",
    "            datacite_id_mapping,\n",
    "            sources_with_updated_oa[\"id\"] == datacite_id_mapping[\"openalex_id\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .withColumn(\"datacite_ids\", \n",
    "            when(col(\"datacite_id\").isNotNull(), \n",
    "                array(col(\"datacite_id\")))\n",
    "            .when(col(\"all_datacite_ids\").isNotNull(), \n",
    "                col(\"all_datacite_ids\"))\n",
    "            .otherwise(array())\n",
    "        )\n",
    "        # remove \"Deleted Journal\" and DOAJ source records from upstream\n",
    "        .filter(~col(\"id\").isin(4317411217, 4306401280))\n",
    "        .drop(\"endpoint_id\", \"openalex_id\", \"all_datacite_ids\")\n",
    "    )\n",
    "    return final_result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateSources",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
