{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "\"\"\"\nSync Vector Embeddings to Elasticsearch\n\nBulk updates works-v32 documents with vector_embedding field from the\nwork_embeddings_v2 table. Uses partial document updates to add embeddings\nwithout overwriting existing fields.\n\nRun modes:\n- is_full_sync=true: Load all 217M embeddings (initial load, ~4 hours)\n- is_full_sync=false: Load only recent embeddings (for ongoing sync)\n\"\"\"\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom elasticsearch import Elasticsearch, helpers\nimport logging\n\nlogging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')\nlog = logging.getLogger(__name__)\n\nELASTIC_INDEX = \"works-v32\"\nELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n\nIS_FULL_SYNC = dbutils.widgets.get(\"is_full_sync\").lower() == \"true\"\n\nprint(f\"IS_FULL_SYNC: {IS_FULL_SYNC}\")\nprint(f\"Target index: {ELASTIC_INDEX}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load embeddings from Databricks table\n",
    "# work_id is stored as string (e.g. \"3043519958\"), embedding is array<double> (1024 dims)\n",
    "\n",
    "if IS_FULL_SYNC:\n",
    "    SQL_QUERY = \"\"\"\n",
    "    SELECT work_id, embedding\n",
    "    FROM openalex.vector_search.work_embeddings_v2\n",
    "    \"\"\"\n",
    "else:\n",
    "    # For incremental sync, we'd need to track which embeddings are new\n",
    "    # For now, just use full sync for initial load\n",
    "    SQL_QUERY = \"\"\"\n",
    "    SELECT work_id, embedding\n",
    "    FROM openalex.vector_search.work_embeddings_v2\n",
    "    \"\"\"\n",
    "\n",
    "df = spark.sql(SQL_QUERY)\n",
    "\n",
    "# Get count for progress tracking\n",
    "total_count = df.count()\n",
    "print(f\"Total embeddings to sync: {total_count:,}\")\n",
    "\n",
    "# Repartition for parallel processing\n",
    "# ~217M rows / 10K per partition = 21,700 partitions\n",
    "# Use 8000 partitions for manageable parallelism\n",
    "if IS_FULL_SYNC:\n",
    "    df = df.repartitionByRange(8000, \"work_id\")\n",
    "    print(f\"Repartitioned to {df.rdd.getNumPartitions()} partitions\")\n",
    "else:\n",
    "    df = df.repartition(100)\n",
    "    print(f\"Using {df.rdd.getNumPartitions()} partitions for incremental sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Schema for logging results\n",
    "log_schema = StructType([\n",
    "    StructField(\"partition_id\", IntegerType(), True),\n",
    "    StructField(\"updated_count\", IntegerType(), True),\n",
    "    StructField(\"error_count\", IntegerType(), True),\n",
    "    StructField(\"not_found_count\", IntegerType(), True),\n",
    "    StructField(\"errors\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "def generate_update_actions(partition):\n",
    "    \"\"\"\n",
    "    Generate ES bulk update actions for partial document updates.\n",
    "    Uses _update action to add vector_embedding without touching other fields.\n",
    "    \"\"\"\n",
    "    for row in partition:\n",
    "        # Build the OpenAlex ID format: https://openalex.org/W{work_id}\n",
    "        doc_id = f\"https://openalex.org/W{row.work_id}\"\n",
    "        \n",
    "        # Convert embedding from list of doubles to list of floats\n",
    "        # ES dense_vector expects float32, PySpark gives us float64\n",
    "        embedding = [float(x) for x in row.embedding]\n",
    "        \n",
    "        yield {\n",
    "            \"_op_type\": \"update\",\n",
    "            \"_index\": ELASTIC_INDEX,\n",
    "            \"_id\": doc_id,\n",
    "            \"doc\": {\n",
    "                \"vector_embedding\": embedding\n",
    "            },\n",
    "            # Don't fail if document doesn't exist (some works may not be in ES yet)\n",
    "            \"doc_as_upsert\": False\n",
    "        }\n",
    "\n",
    "def send_partition_to_elastic(partition, partition_id):\n",
    "    \"\"\"\n",
    "    Send a partition of embeddings to Elasticsearch using bulk update.\n",
    "    \"\"\"\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "    \n",
    "    updated_count = 0\n",
    "    error_count = 0\n",
    "    not_found_count = 0\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        for success, info in helpers.parallel_bulk(\n",
    "            client,\n",
    "            generate_update_actions(partition),\n",
    "            chunk_size=500,\n",
    "            thread_count=4,\n",
    "            queue_size=10,\n",
    "            raise_on_error=False\n",
    "        ):\n",
    "            if success:\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                error_info = info.get(\"update\", {})\n",
    "                status = error_info.get(\"status\", 0)\n",
    "                \n",
    "                # 404 = document not found in ES (work not synced yet)\n",
    "                if status == 404:\n",
    "                    not_found_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    if len(errors) < 10:\n",
    "                        errors.append(str(info)[:500])\n",
    "                        \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        errors.append(f\"Bulk error: {str(e)[:500]}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "    \n",
    "    yield {\n",
    "        \"partition_id\": partition_id,\n",
    "        \"updated_count\": updated_count,\n",
    "        \"error_count\": error_count,\n",
    "        \"not_found_count\": not_found_count,\n",
    "        \"errors\": errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Execute the sync\n",
    "print(f\"Starting embedding sync to {ELASTIC_INDEX}...\")\n",
    "\n",
    "logs_rdd = df.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, partition: send_partition_to_elastic(partition, idx)\n",
    ")\n",
    "\n",
    "logs_df = spark.createDataFrame(logs_rdd, log_schema)\n",
    "\n",
    "# Cache and collect stats\n",
    "logs_df.cache()\n",
    "partition_count = logs_df.count()\n",
    "\n",
    "print(f\"Processed {partition_count} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate results\n",
    "stats = logs_df.agg(\n",
    "    F.sum(\"updated_count\").alias(\"total_updated\"),\n",
    "    F.sum(\"error_count\").alias(\"total_errors\"),\n",
    "    F.sum(\"not_found_count\").alias(\"total_not_found\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\n=== Sync Complete ===\")\n",
    "print(f\"Total updated: {stats.total_updated:,}\")\n",
    "print(f\"Total errors: {stats.total_errors:,}\")\n",
    "print(f\"Total not found (work not in ES): {stats.total_not_found:,}\")\n",
    "\n",
    "# Show sample errors if any\n",
    "if stats.total_errors > 0:\n",
    "    print(\"\\nSample errors:\")\n",
    "    error_sample = logs_df.filter(F.size(\"errors\") > 0).select(\"errors\").limit(5).collect()\n",
    "    for row in error_sample:\n",
    "        for err in row.errors[:2]:\n",
    "            print(f\"  - {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Refresh index and verify\n",
    "try:\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "    \n",
    "    if client.indices.exists(index=ELASTIC_INDEX):\n",
    "        client.indices.refresh(index=ELASTIC_INDEX)\n",
    "        print(f\"Refreshed index {ELASTIC_INDEX}\")\n",
    "        \n",
    "        # Count documents with embeddings\n",
    "        result = client.count(\n",
    "            index=ELASTIC_INDEX,\n",
    "            body={\"query\": {\"exists\": {\"field\": \"vector_embedding\"}}}\n",
    "        )\n",
    "        print(f\"Documents with vector_embedding: {result['count']:,}\")\n",
    "        \n",
    "        total_docs = client.count(index=ELASTIC_INDEX)['count']\n",
    "        print(f\"Total documents: {total_docs:,}\")\n",
    "        print(f\"Coverage: {result['count'] / total_docs * 100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test kNN search with a sample query\n",
    "print(\"\\nTesting kNN search...\")\n",
    "\n",
    "try:\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "    \n",
    "    # Get a sample embedding to use as query\n",
    "    sample = spark.sql(\"\"\"\n",
    "        SELECT work_id, embedding\n",
    "        FROM openalex.vector_search.work_embeddings_v2\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    query_vector = [float(x) for x in sample.embedding]\n",
    "    \n",
    "    # Run kNN search\n",
    "    result = client.search(\n",
    "        index=ELASTIC_INDEX,\n",
    "        body={\n",
    "            \"knn\": {\n",
    "                \"field\": \"vector_embedding\",\n",
    "                \"query_vector\": query_vector,\n",
    "                \"k\": 5,\n",
    "                \"num_candidates\": 50\n",
    "            },\n",
    "            \"_source\": [\"id\", \"title\"]\n",
    "        },\n",
    "        size=5\n",
    "    )\n",
    "    \n",
    "    print(f\"Query work_id: {sample.work_id}\")\n",
    "    print(f\"\\nTop 5 similar works:\")\n",
    "    for hit in result['hits']['hits']:\n",
    "        print(f\"  {hit['_score']:.4f}: {hit['_source'].get('title', 'N/A')[:80]}\")\n",
    "        \n",
    "finally:\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}