{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Embeddings to Parquet\n",
    "\n",
    "Exports the 217M embeddings from `openalex.vector_search.work_embeddings_v2` to S3 as Parquet.\n",
    "\n",
    "This is a safe prep step that doesn't touch Elasticsearch. The Parquet files can then be\n",
    "used by `sync_embeddings_to_es.ipynb` to bulk load embeddings into ES.\n",
    "\n",
    "**Expected runtime**: 2-4 hours\n",
    "**Output**: `s3://openalex-ingest/embeddings/work_embeddings_v2/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SOURCE_TABLE = \"openalex.vector_search.work_embeddings_v2\"\n",
    "S3_OUTPUT_PATH = \"s3://openalex-ingest/embeddings/work_embeddings_v2\"\n",
    "NUM_PARTITIONS = 1000  # ~217K rows per partition for parallel processing\n",
    "\n",
    "print(f\"Source: {SOURCE_TABLE}\")\n",
    "print(f\"Output: {S3_OUTPUT_PATH}\")\n",
    "print(f\"Partitions: {NUM_PARTITIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source table\n",
    "print(\"Loading embeddings table...\")\n",
    "t0 = time.time()\n",
    "\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "\n",
    "# Get count\n",
    "total_count = df.count()\n",
    "print(f\"Total embeddings: {total_count:,}\")\n",
    "print(f\"Count took {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check schema\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Sample row\n",
    "print(\"\\nSample row:\")\n",
    "sample = df.limit(1).collect()[0]\n",
    "print(f\"  work_id: {sample.work_id}\")\n",
    "print(f\"  embedding dims: {len(sample.embedding)}\")\n",
    "print(f\"  embedding type: {type(sample.embedding[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition for efficient parallel writes\n",
    "# Using range partitioning on work_id for even distribution\n",
    "print(f\"Repartitioning to {NUM_PARTITIONS} partitions...\")\n",
    "t0 = time.time()\n",
    "\n",
    "df_partitioned = df.repartitionByRange(NUM_PARTITIONS, \"work_id\")\n",
    "\n",
    "print(f\"Actual partitions: {df_partitioned.rdd.getNumPartitions()}\")\n",
    "print(f\"Repartition took {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet\n",
    "print(f\"\\nWriting to {S3_OUTPUT_PATH}...\")\n",
    "print(f\"Started at: {datetime.now(timezone.utc).isoformat()}\")\n",
    "t0 = time.time()\n",
    "\n",
    "(\n",
    "    df_partitioned\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(S3_OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nWrite complete!\")\n",
    "print(f\"Elapsed: {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)\")\n",
    "print(f\"Throughput: {total_count/elapsed:,.0f} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the output\n",
    "print(\"Verifying output...\")\n",
    "\n",
    "df_verify = spark.read.parquet(S3_OUTPUT_PATH)\n",
    "verify_count = df_verify.count()\n",
    "\n",
    "print(f\"Source count: {total_count:,}\")\n",
    "print(f\"Output count: {verify_count:,}\")\n",
    "\n",
    "if verify_count == total_count:\n",
    "    print(\"\\n✓ Counts match! Export successful.\")\n",
    "else:\n",
    "    print(f\"\\n✗ COUNT MISMATCH! Missing {total_count - verify_count:,} rows\")\n",
    "    raise Exception(\"Export verification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show file stats\n",
    "print(\"\\nOutput file stats:\")\n",
    "files = dbutils.fs.ls(S3_OUTPUT_PATH.replace(\"s3://\", \"s3a://\"))\n",
    "parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "total_size_gb = sum(f.size for f in parquet_files) / (1024**3)\n",
    "\n",
    "print(f\"  Files: {len(parquet_files)}\")\n",
    "print(f\"  Total size: {total_size_gb:.2f} GB\")\n",
    "print(f\"\\nReady for bulk load to Elasticsearch!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
