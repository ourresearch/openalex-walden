{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97465bf0-7bca-409c-abe6-a9b47ebf3f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e64c8f-aae1-4169-8167-e3359c79bfd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc20bab9-7ede-47d5-a520-5ae15e692446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "-- need legacy.mag_advanced_field_of_study_children,\n",
    "-- legacy.mag_advanced_field_of_study_extended_attributes\n",
    "\n",
    "WITH exploded AS (\n",
    "    SELECT id as work_id, cited_by_count, explode(concepts) as concept\n",
    "    FROM openalex.works.openalex_works\n",
    "),\n",
    "-- one row per (work_id, keyword_id)\n",
    "dedup AS (\n",
    "  SELECT work_id, cited_by_count, concept\n",
    "  FROM exploded\n",
    "  QUALIFY row_number() OVER (PARTITION BY work_id, concept.id ORDER BY work_id, concept.id) = 1\n",
    "),\n",
    "-- Aggregate on unique keywords\n",
    "aggregated_counts AS (\n",
    "  SELECT\n",
    "    concept.id as concept_id,\n",
    "    concept.display_name as display_name,\n",
    "    count(DISTINCT work_id) as works_count,\n",
    "    sum(cited_by_count) as cited_by_count\n",
    "  FROM dedup\n",
    "  GROUP BY 1, 2\n",
    "),\n",
    "concepts_json AS (\n",
    "  SELECT \n",
    "    get_json_object(wikipedia_json, '$.query.pages[0]') as wikipedia_data, \n",
    "    get_json_object(wikidata_json, \n",
    "      CONCAT('$.entities.', json_object_keys(get_json_object(wikidata_json, '$.entities'))[0])\n",
    "    ) as wikidata_data,\n",
    "    *\n",
    "  FROM openalex.common.concepts\n",
    "  WHERE wikidata_id IS NOT NULL AND merge_into_id IS NULL\n",
    "),\n",
    "concepts_parsed AS (\n",
    "  SELECT\n",
    "    get_json_object(wikipedia_data, '$.title') as wikipedia_title,\n",
    "    get_json_object(wikipedia_data, '$.original.source') as image_url,\n",
    "    get_json_object(wikipedia_data, '$.thumbnail.source') as image_thumbnail_url,\n",
    "    get_json_object(wikipedia_data, '$.terms.description[0]') as description,\n",
    "    get_json_object(wikipedia_data, '$.pageprops.wikibase_item') as raw_wikidata_id,\n",
    "    wikidata_data, -- extract international data (probably from_json followed by transform_keys)\n",
    "    *\n",
    "  FROM concepts_json\n",
    ")\n",
    "-- Join with the common keywords table to get metadata\n",
    "SELECT\n",
    "  concept_id as _id,\n",
    "  STRUCT(\n",
    "    ac.concept_id as id,\n",
    "    con.wikidata_id as wikidata,\n",
    "    ac.display_name,\n",
    "    con.level,\n",
    "    con.description,\n",
    "    ac.works_count,\n",
    "    ac.cited_by_count,\n",
    "    NULL AS summary_stats,\n",
    "    named_struct(\n",
    "      \"openalex\", con.concept_id,\n",
    "      \"wikidata\", con.wikidata_id,\n",
    "      \"wikipedia\", CONCAT(\"https://en.wikipedia.org/wiki/\", \n",
    "        replace(lower(ac.display_name), \" \", \"_\")),\n",
    "      \"umls_aui\", NULL,\n",
    "      \"umls_cui\", NULL,\n",
    "      \"mag\", NULL\n",
    "    ) as ids,\n",
    "    con.image_url,\n",
    "    con.image_thumbnail_url,\n",
    "    NULL as international,\n",
    "    NULL as ancestors,\n",
    "    NULL as related_concepts,\n",
    "    NULL as counts_by_year,\n",
    "    CONCAT(\"https://api.openalex.org/works?filter=concepts.id:\", con.concept_id) AS works_api_url,\n",
    "    updated_date,\n",
    "    created_date\n",
    "  ) as _source\n",
    "FROM aggregated_counts ac\n",
    "JOIN concepts_parsed con USING (concept_id)\"\"\")\n",
    "\n",
    "df.cache()\n",
    "print(f\"Number of concepts: {df.count()}\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7753993f-8651-43d7-b145-4d882aff169a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "\n",
    "ELASTIC_INDEX = \"concepts-v8\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "\n",
    "client = Elasticsearch(\n",
    "    hosts = [ELASTIC_URL],\n",
    "    request_timeout = 180,\n",
    "    max_retries = 5,\n",
    "    retry_on_timeout = True\n",
    ")\n",
    "\n",
    "def actions_from_spark(rows, op_type = \"index\"):\n",
    "    for row in rows:\n",
    "        yield {\n",
    "            \"_op_type\": op_type,\n",
    "            \"_index\": ELASTIC_INDEX,\n",
    "            \"_id\": row._id,\n",
    "            \"_source\": row._source.asDict(True)\n",
    "        }\n",
    "\n",
    "# Delete old index\n",
    "if client.indices.exists(index=ELASTIC_INDEX):\n",
    "    client.indices.delete(index=ELASTIC_INDEX)\n",
    "\n",
    "rows = df.collect()\n",
    "print(f\"Concepts count: {len(rows)}\")\n",
    "\n",
    "ok = fail = 0\n",
    "for success, info in helpers.streaming_bulk(client, actions_from_spark(rows),\n",
    "    chunk_size=1000, request_timeout=60, max_retries=3):\n",
    "    if success:\n",
    "        ok += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "\n",
    "print(f\"Indexed ok={ok}, failed={fail}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_concepts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
