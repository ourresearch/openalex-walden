{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b5a0cb-d716-47b6-b870-63b0b96939d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2661e04f-0703-4713-bfa3-6890341ee2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"table_name\": \"openalex.awards.awards_api\",\n",
    "    \"index_name\": \"awards-v3\"\n",
    "}\n",
    "\n",
    "IS_FULL_SYNC = dbutils.widgets.get(\"is_full_sync\").lower() == \"true\"\n",
    "print(f\"IS_FULL_SYNC: {IS_FULL_SYNC}\")\n",
    "\n",
    "def send_partition_to_elastic(partition, index_name):\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        max_retries=3,\n",
    "        request_timeout=180\n",
    "    )\n",
    "    \n",
    "    def generate_actions(op_type = \"index\"):\n",
    "        for row in partition:\n",
    "            yield {\n",
    "                \"_op_type\": op_type,\n",
    "                \"_index\": CONFIG[\"index_name\"],\n",
    "                \"_id\": row.id,\n",
    "                \"_source\": row._source.asDict(True)\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        count = 0\n",
    "        for success, info in helpers.parallel_bulk(\n",
    "            client, \n",
    "            generate_actions(), \n",
    "            chunk_size=500,\n",
    "            thread_count=4\n",
    "        ):\n",
    "            count += 1\n",
    "            if not success:\n",
    "                print(f\"FAILED TO INDEX: {info}\")\n",
    "                raise Exception(f\"Failed to index document: {info}\")\n",
    "        \n",
    "        print(f\"Successfully indexed {count} total documents to {index_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log.error(f\"Error indexing documents to {index_name}: {e}\", stack_info=True, exc_info=True)\n",
    "        print(f\"Error indexing documents to {index_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2099dd-f16c-42e4-8e01-5aca7efc34aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53192c77-0348-4cfe-af7f-7b4be0e5dbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.table(CONFIG[\"table_name\"])\n",
    "\n",
    "if not IS_FULL_SYNC:\n",
    "    two_days_ago = (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    df = df.filter(F.col(\"updated_date\") >= two_days_ago)\n",
    "    print(f\"Incremental sync: filtering to updated_date >= {two_days_ago}\")\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/G\"), F.col(\"id\")))\n",
    "    .select(\"id\", F.struct(F.col(\"*\")).alias(\"_source\"))\n",
    ")\n",
    "\n",
    "\n",
    "num_partitions = 96 if IS_FULL_SYNC else 8\n",
    "df = df.repartition(num_partitions)\n",
    "print(f\"Total records to process: {df.count()}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660c9e5a-17f0-4e9c-b931-0b8ed8cd03c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca5877e-6a05-4917-8e46-36a85d5314d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\n=== Processing {CONFIG['table_name']} ===\")\n",
    "\n",
    "try:\n",
    "    def send_partition_wrapper(partition):\n",
    "        return send_partition_to_elastic(\n",
    "            partition,\n",
    "            CONFIG['index_name']\n",
    "        )\n",
    "    \n",
    "    df.foreachPartition(send_partition_wrapper)\n",
    "    \n",
    "    print(f\"Completed indexing {CONFIG['table_name']} to {CONFIG['index_name']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to process {CONFIG['table_name']}: {e}\")\n",
    "    log.error(f\"Failed to process {CONFIG['table_name']}: {e}\", stack_info=True, exc_info=True)\n",
    "\n",
    "print(\"\\nIndexing operation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c84211f-6831-46a4-8b54-27d71925f3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        max_retries=3,\n",
    "        request_timeout=180\n",
    "    )\n",
    "\n",
    "client.indices.refresh(index=CONFIG['index_name'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6502428237283308,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_awards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}