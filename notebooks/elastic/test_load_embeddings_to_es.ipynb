{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Load Embeddings to ES\n",
    "\n",
    "Incremental test of loading embeddings to Elasticsearch.\n",
    "Use the `limit` widget to control how many records to load.\n",
    "\n",
    "**Incremental test plan:**\n",
    "1. 1,000 records - validate it works\n",
    "2. 1,000,000 records - test throughput\n",
    "3. 10,000,000 records - stress test\n",
    "4. Full load (217M) - production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.17.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import functions as F\nfrom elasticsearch import Elasticsearch, helpers\nimport time\n\n# Configuration - read from Databricks table (not S3, to avoid IAM issues)\nEMBEDDINGS_TABLE = \"openalex.vector_search.work_embeddings_v2\"\nELASTIC_INDEX = \"works-v32\"\nELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n\n# Widget for limit - default 1000 for first test\ndbutils.widgets.text(\"limit\", \"1000\", \"Number of records to load\")\nLIMIT = int(dbutils.widgets.get(\"limit\"))\n\nprint(f\"Loading {LIMIT:,} embeddings from {EMBEDDINGS_TABLE}\")\nprint(f\"Target index: {ELASTIC_INDEX}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load embeddings from Databricks table with limit\nprint(f\"Reading {LIMIT:,} records from table...\")\nt0 = time.time()\n\ndf = spark.table(EMBEDDINGS_TABLE).limit(LIMIT)\n\n# Force evaluation to get actual count\nactual_count = df.count()\nprint(f\"Loaded {actual_count:,} records in {time.time() - t0:.1f}s\")\n\n# Show sample\nprint(\"\\nSample record:\")\nsample = df.first()\nprint(f\"  work_id: {sample.work_id}\")\nprint(f\"  embedding dims: {len(sample.embedding)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect to driver for small batches, or use partition-based processing for larger ones\nif LIMIT <= 100000:\n    # For small tests, collect to driver and bulk update\n    print(f\"Collecting {actual_count:,} records to driver...\")\n    records = df.collect()\n    print(f\"Collected {len(records):,} records\")\n    use_partitions = False\nelse:\n    # For larger tests, use partition-based processing\n    print(f\"Using partition-based processing for {actual_count:,} records\")\n    # Repartition for parallel processing\n    num_partitions = max(100, actual_count // 10000)  # ~10K per partition\n    df = df.repartition(num_partitions)\n    print(f\"Repartitioned to {df.rdd.getNumPartitions()} partitions\")\n    records = None\n    use_partitions = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\nimport time\n\ndef generate_update_actions(records_iter, index_name):\n    \"\"\"Generate ES bulk update actions.\"\"\"\n    for row in records_iter:\n        doc_id = f\"https://openalex.org/W{row.work_id}\"\n        embedding = [float(x) for x in row.embedding]\n        \n        yield {\n            \"_op_type\": \"update\",\n            \"_index\": index_name,\n            \"_id\": doc_id,\n            \"doc\": {\n                \"vector_embedding\": embedding\n            },\n            \"doc_as_upsert\": False\n        }\n\ndef send_partition_to_elastic(partition, partition_id):\n    \"\"\"Send a partition of embeddings to ES using bulk update.\"\"\"\n    from elasticsearch import Elasticsearch, helpers\n    \n    client = Elasticsearch(\n        hosts=[ELASTIC_URL],\n        request_timeout=180,\n        max_retries=3,\n        retry_on_timeout=True,\n    )\n    \n    updated_count = 0\n    error_count = 0\n    not_found_count = 0\n    errors = []\n    \n    try:\n        for success, info in helpers.parallel_bulk(\n            client,\n            generate_update_actions(partition, ELASTIC_INDEX),\n            chunk_size=500,\n            thread_count=4,\n            raise_on_error=False\n        ):\n            if success:\n                updated_count += 1\n            else:\n                error_info = info.get(\"update\", {})\n                if error_info.get(\"status\") == 404:\n                    not_found_count += 1\n                else:\n                    error_count += 1\n                    if len(errors) < 5:\n                        errors.append(str(info)[:200])\n    except Exception as e:\n        error_count += 1\n        errors.append(f\"Error: {str(e)[:200]}\")\n    finally:\n        client.close()\n    \n    yield {\n        \"partition_id\": partition_id,\n        \"updated_count\": updated_count,\n        \"error_count\": error_count,\n        \"not_found_count\": not_found_count,\n        \"errors\": errors\n    }\n\nif use_partitions:\n    # Partition-based processing for large datasets\n    print(f\"\\nBulk updating via partitions...\")\n    t0 = time.time()\n    \n    log_schema = StructType([\n        StructField(\"partition_id\", IntegerType(), True),\n        StructField(\"updated_count\", IntegerType(), True),\n        StructField(\"error_count\", IntegerType(), True),\n        StructField(\"not_found_count\", IntegerType(), True),\n        StructField(\"errors\", ArrayType(StringType()), True)\n    ])\n    \n    logs_rdd = df.rdd.mapPartitionsWithIndex(\n        lambda idx, partition: send_partition_to_elastic(partition, idx)\n    )\n    \n    logs_df = spark.createDataFrame(logs_rdd, log_schema)\n    logs_df.cache()\n    \n    stats = logs_df.agg(\n        F.sum(\"updated_count\").alias(\"total_updated\"),\n        F.sum(\"error_count\").alias(\"total_errors\"),\n        F.sum(\"not_found_count\").alias(\"total_not_found\")\n    ).collect()[0]\n    \n    elapsed = time.time() - t0\n    print(f\"\\n=== Results ===\")\n    print(f\"Success: {stats.total_updated:,}\")\n    print(f\"Not found (work not in ES): {stats.total_not_found:,}\")\n    print(f\"Errors: {stats.total_errors:,}\")\n    print(f\"Time: {elapsed:.1f}s\")\n    print(f\"Rate: {stats.total_updated/elapsed:,.0f} docs/sec\")\n    \n    # Show sample errors if any\n    if stats.total_errors > 0:\n        print(\"\\nSample errors:\")\n        error_sample = logs_df.filter(F.size(\"errors\") > 0).select(\"errors\").limit(3).collect()\n        for row in error_sample:\n            for err in row.errors[:2]:\n                print(f\"  - {err}\")\n                \n    success_count = stats.total_updated\n    not_found_count = stats.total_not_found\n    error_count = stats.total_errors\n\nelse:\n    # Driver-based processing for small datasets\n    print(f\"\\nBulk updating {len(records):,} records to ES...\")\n    t0 = time.time()\n    \n    client = Elasticsearch(\n        hosts=[ELASTIC_URL],\n        request_timeout=180,\n        max_retries=3,\n        retry_on_timeout=True,\n    )\n    \n    success_count = 0\n    error_count = 0\n    not_found_count = 0\n    \n    for success, info in helpers.parallel_bulk(\n        client,\n        generate_update_actions(records, ELASTIC_INDEX),\n        chunk_size=500,\n        thread_count=4,\n        raise_on_error=False\n    ):\n        if success:\n            success_count += 1\n        else:\n            error_info = info.get(\"update\", {})\n            if error_info.get(\"status\") == 404:\n                not_found_count += 1\n            else:\n                error_count += 1\n                if error_count <= 5:\n                    print(f\"Error: {info}\")\n    \n    elapsed = time.time() - t0\n    print(f\"\\n=== Results ===\")\n    print(f\"Success: {success_count:,}\")\n    print(f\"Not found (work not in ES): {not_found_count:,}\")\n    print(f\"Errors: {error_count:,}\")\n    print(f\"Time: {elapsed:.1f}s\")\n    print(f\"Rate: {success_count/elapsed:,.0f} docs/sec\")\n    \n    client.close()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh index and verify\n",
    "print(\"\\nRefreshing index and verifying...\")\n",
    "\n",
    "client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "\n",
    "# Refresh to make new docs searchable\n",
    "client.indices.refresh(index=ELASTIC_INDEX)\n",
    "\n",
    "# Count documents with embeddings\n",
    "result = client.count(\n",
    "    index=ELASTIC_INDEX,\n",
    "    body={\"query\": {\"exists\": {\"field\": \"vector_embedding\"}}}\n",
    ")\n",
    "print(f\"Documents with vector_embedding: {result['count']:,}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test kNN search\nprint(\"\\nTesting kNN search...\")\n\nclient = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n\n# Get a sample embedding to use as query\nif records is not None:\n    sample_embedding = [float(x) for x in records[0].embedding]\n    sample_work_id = records[0].work_id\nelse:\n    # Get a sample from the dataframe\n    sample = df.limit(1).collect()[0]\n    sample_embedding = [float(x) for x in sample.embedding]\n    sample_work_id = sample.work_id\n\n# Run kNN search\nt0 = time.time()\nresult = client.search(\n    index=ELASTIC_INDEX,\n    body={\n        \"knn\": {\n            \"field\": \"vector_embedding\",\n            \"query_vector\": sample_embedding,\n            \"k\": 5,\n            \"num_candidates\": 50\n        },\n        \"_source\": [\"id\", \"title\"]\n    },\n    size=5\n)\nlatency = (time.time() - t0) * 1000\n\nprint(f\"Query work_id: W{sample_work_id}\")\nprint(f\"Latency: {latency:.0f}ms\")\nprint(f\"\\nTop 5 similar works:\")\nfor hit in result['hits']['hits']:\n    title = hit['_source'].get('title', 'N/A')\n    if title:\n        title = title[:70] + \"...\" if len(title) > 70 else title\n    print(f\"  {hit['_score']:.4f}: {title}\")\n\nclient.close()\n\nprint(f\"\\n✓ Test complete! kNN search working.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kNN with pre-filter\n",
    "print(\"\\nTesting kNN with pre-filter (is_oa:true)...\")\n",
    "\n",
    "client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "\n",
    "t0 = time.time()\n",
    "result = client.search(\n",
    "    index=ELASTIC_INDEX,\n",
    "    body={\n",
    "        \"knn\": {\n",
    "            \"field\": \"vector_embedding\",\n",
    "            \"query_vector\": sample_embedding,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 50,\n",
    "            \"filter\": {\n",
    "                \"term\": {\"is_oa\": True}\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"title\", \"is_oa\"]\n",
    "    },\n",
    "    size=5\n",
    ")\n",
    "latency = (time.time() - t0) * 1000\n",
    "\n",
    "print(f\"Latency: {latency:.0f}ms\")\n",
    "print(f\"\\nTop 5 similar OA works:\")\n",
    "all_oa = True\n",
    "for hit in result['hits']['hits']:\n",
    "    title = hit['_source'].get('title', 'N/A')\n",
    "    is_oa = hit['_source'].get('is_oa', False)\n",
    "    if not is_oa:\n",
    "        all_oa = False\n",
    "    if title:\n",
    "        title = title[:60] + \"...\" if len(title) > 60 else title\n",
    "    print(f\"  {hit['_score']:.4f} [OA:{is_oa}]: {title}\")\n",
    "\n",
    "client.close()\n",
    "\n",
    "if all_oa:\n",
    "    print(f\"\\n✓ Pre-filter working! All results are OA.\")\n",
    "else:\n",
    "    print(f\"\\n✗ Warning: Some results are not OA - filter may not be working correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}