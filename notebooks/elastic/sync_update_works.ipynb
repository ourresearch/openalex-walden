{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104bf6c9-96bc-4a41-a4ba-c50d12c4084f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c44ab9-06bf-4229-91b0-3a50bd763254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import logging\n",
    "import json\n",
    "\n",
    "ELASTIC_INDEX = \"works-v26\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "\n",
    "UPDATE_FIELDS_ONLY = dbutils.widgets.get(\"update_fields_only\").lower()\n",
    "\n",
    "if UPDATE_FIELDS_ONLY.strip() == '':\n",
    "    dbutils.notebook.exit(\"Please provide a comma-separated list of fields to update.\")\n",
    "    \n",
    "print(\"UPDATE_FIELDS_ONLY: \" + UPDATE_FIELDS_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff580d2-f6cb-4ffd-bfc3-c947f7900fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SQL_QUERY = f\"\"\"SELECT id, {UPDATE_FIELDS_ONLY}\n",
    "FROM openalex.works.openalex_works\"\"\"\n",
    "# WHERE array_contains(flatten(authorships.institutions.id), 'https://openalex.org/I141945490')\"\"\"\n",
    "# WHERE id = 2151103935\"\"\"\n",
    "print(SQL_QUERY)\n",
    "df = (spark.sql(SQL_QUERY)\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    ")\n",
    "\n",
    "FIELDS = [f.strip().lower() for f in UPDATE_FIELDS_ONLY.split(',')]\n",
    "if \"concepts\" in FIELDS:\n",
    "    df = df.withColumn(\n",
    "        \"concepts\",\n",
    "        F.transform(\n",
    "            F.col(\"concepts\"),\n",
    "            lambda c: F.struct(\n",
    "                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n",
    "                c.wikidata.alias(\"wikidata\"),\n",
    "                c.display_name.alias(\"display_name\"),\n",
    "                c.level.alias(\"level\"),\n",
    "                c.score.alias(\"score\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "df = df.withColumn(\"doc\", F.struct(FIELDS)).select(\"id\", \"doc\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875a5517-e669-4d17-ad7c-b5961ef7b4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c83d03-9157-4377-8154-4ddf8d646ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_prepared_actions(partition, parsing_errors, op_type = \"update\"):\n",
    "    for row in partition:\n",
    "        try:\n",
    "            yield {\n",
    "                \"_op_type\": op_type,\n",
    "                \"_index\": ELASTIC_INDEX,\n",
    "                \"_id\": row.id,\n",
    "                \"doc\": row.doc.asDict(True)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            parsing_errors.append({\"row_id\": row.id, \"error\": str(e)})             \n",
    "\n",
    "def send_partition_to_elastic(partition, partition_id, op_type=\"update\"):\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "\n",
    "    indexed_count = 0\n",
    "    parsing_errors = []\n",
    "    indexing_errors = []\n",
    "    skipped_count = 0\n",
    "    op_type = op_type.lower()\n",
    "\n",
    "    try:\n",
    "        for success, info in helpers.parallel_bulk(client,\n",
    "                generate_prepared_actions(partition, parsing_errors, op_type),\n",
    "                chunk_size=500, thread_count=4, queue_size=10\n",
    "            ):\n",
    "\n",
    "            if success:\n",
    "                indexed_count += 1\n",
    "            else:\n",
    "                error_info = info.get(op_type, {})\n",
    "                status = error_info.get(\"status\", 0)\n",
    "\n",
    "                # âœ… Skip 409 (document already exists)\n",
    "                if status == 409:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                id_url = error_info.get('_id')\n",
    "                row_id = None\n",
    "                if id_url:\n",
    "                    try:\n",
    "                        row_id = int(id_url.replace(\"https://openalex.org/W\", \"\"))\n",
    "                    except ValueError:\n",
    "                        row_id = -1\n",
    "\n",
    "                indexing_errors.append({\n",
    "                    \"row_id\": row_id,\n",
    "                    \"error\": str(info)[:1000]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        indexing_errors.append({\n",
    "            \"row_id\": None,\n",
    "            \"error\": \"Parallel Bulk Error: \" + str(e)[:1000]\n",
    "        })\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "    log_entry = {\n",
    "        \"index_name\": ELASTIC_INDEX,\n",
    "        \"partition_id\": partition_id,\n",
    "        \"success\": len(indexing_errors) == 0,\n",
    "        \"indexed_count\": indexed_count,\n",
    "        \"skipped_count\": skipped_count,\n",
    "        \"parsing_error_count\": len(parsing_errors),\n",
    "        \"indexing_error_count\": len(indexing_errors),\n",
    "        \"message\": f\"{indexed_count} records indexed. {skipped_count} skipped. {len(parsing_errors)} parsing errors. {len(indexing_errors)} ES errors.\",\n",
    "        \"parsing_errors\": parsing_errors[:1000],\n",
    "        \"indexing_errors\": indexing_errors[:1000],\n",
    "    }\n",
    "\n",
    "    yield log_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efaaa97-04af-4398-89a8-e27e5b95eff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute Sync with `mapPartitionsWithIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48733138-6d67-472b-ae77-2672aa2e315e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_input = spark.read.table(\"openalex.works.works_api_sync\")\n",
    "import pprint # Import the pretty-print library for clean output\n",
    "log_schema = StructType([\n",
    "    StructField(\"index_name\", StringType(), True),\n",
    "    StructField(\"partition_id\", IntegerType(), True),\n",
    "    StructField(\"indexed_count\", IntegerType(), True),\n",
    "    StructField(\"skipped_count\", IntegerType(), True),\n",
    "    StructField(\"parsing_error_count\", IntegerType(), True),\n",
    "    StructField(\"indexing_error_count\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"parsing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    ),\n",
    "    StructField(\n",
    "        \"indexing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "logs_rdd = df.rdd.mapPartitionsWithIndex(\n",
    "    lambda partition_idx, partition: send_partition_to_elastic(partition, partition_idx, \"update\")\n",
    ")\n",
    "logs_df = spark.createDataFrame(logs_rdd, log_schema)\n",
    "\n",
    "log_count = logs_df.count() # mapPartitionsWithIndex is lazy, so force it to run\n",
    "print(f\"Processed {log_count} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3459f92a-4875-4d8f-ae9b-f180a6b30e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # delete index if exists\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "    \n",
    "    if client.indices.exists(index=ELASTIC_INDEX):\n",
    "        client.indices.refresh(index=ELASTIC_INDEX)\n",
    "        print(f\"Refreshed index {ELASTIC_INDEX}\")\n",
    "        print(f\"{client.count(index=ELASTIC_INDEX)['count']} documents in {ELASTIC_INDEX}\")\n",
    "    else:\n",
    "        print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "finally:\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7510455811838486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_update_works",
   "widgets": {
    "update_fields_only": {
     "currentValue": "",
     "nuid": "d223c8a8-0d71-421c-a28d-0eb694213c7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "update_fields_only",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "",
      "name": "update_fields_only",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
