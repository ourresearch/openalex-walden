{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104bf6c9-96bc-4a41-a4ba-c50d12c4084f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c44ab9-06bf-4229-91b0-3a50bd763254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "ELASTIC_INDEX = \"works-v31\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "MAX_LENGTH = 32000  # Slightly below the 32766 limit\n",
    "\n",
    "IS_FULL_SYNC = dbutils.widgets.get(\"is_full_sync\").lower() == \"true\" # default is incremental\n",
    "\n",
    "print(f\"IS_FULL_SYNC: {IS_FULL_SYNC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set replicas to 0 for faster bulk indexing during full sync\n",
    "if IS_FULL_SYNC:\n",
    "    try:\n",
    "        client = Elasticsearch(\n",
    "            hosts=[ELASTIC_URL],\n",
    "            request_timeout=180,\n",
    "            max_retries=5,\n",
    "            retry_on_timeout=True\n",
    "        )\n",
    "        if client.indices.exists(index=ELASTIC_INDEX):\n",
    "            client.indices.put_settings(index=ELASTIC_INDEX, body={\n",
    "                \"index\": {\"number_of_replicas\": 0}\n",
    "            })\n",
    "            print(f\"Set replicas to 0 on {ELASTIC_INDEX} for full sync\")\n",
    "        else:\n",
    "            print(f\"Index {ELASTIC_INDEX} does not exist yet - will create with default settings\")\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0aa700d-33aa-4bb2-8158-6921ddba8c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff580d2-f6cb-4ffd-bfc3-c947f7900fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "SQL_QUERY = \"\"\"SELECT * FROM openalex.works.openalex_works\nWHERE updated_date >= current_date() - INTERVAL 2 days\n\"\"\"\nCOUNT_QUERY = \"\"\"SELECT COUNT(*) as cnt FROM openalex.works.openalex_works\nWHERE updated_date >= current_date() - INTERVAL 2 days\n\"\"\"\n\nif (IS_FULL_SYNC):\n    SQL_QUERY = \"\"\"SELECT * FROM openalex.works.openalex_works\"\"\"\n    COUNT_QUERY = None  # Full sync doesn't need count-based optimization\n\n# Get record count BEFORE loading data (lightweight SQL count, no transformations)\nrecord_count = None\nif not IS_FULL_SYNC and COUNT_QUERY:\n    record_count = spark.sql(COUNT_QUERY).collect()[0].cnt\n    print(f\"Record count for daily sync: {record_count:,}\")\n\ndf = (\n    spark.sql(SQL_QUERY)\n    .withColumn(\"display_name\", F.col(\"title\"))\n    # First cast to date/timestamp\n    .withColumn(\"created_date\", F.to_timestamp(\"created_date\"))\n    .withColumn(\"updated_date\", F.to_timestamp(\"updated_date\"))\n    .withColumn(\"publication_date\", F.to_date(\"publication_date\"))\n    .withColumn(\n        \"concepts\",\n        F.transform(\n            F.col(\"concepts\"),\n            lambda c: F.struct(\n                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n                c.wikidata.alias(\"wikidata\"),\n                c.display_name.alias(\"display_name\"),\n                c.level.alias(\"level\"),\n                c.score.alias(\"score\")\n            )\n        )\n    )\n    # Apply range checks using BETWEEN\n    .withColumn(\n        \"created_date\",\n        F.when(\n            F.col(\"created_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n            F.col(\"created_date\")\n        ).otherwise(F.lit(None).cast(\"timestamp\"))\n    )\n    .withColumn(\n        \"updated_date\",\n        F.when(\n            F.col(\"updated_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n            F.col(\"updated_date\")\n        ).otherwise(F.lit(None).cast(\"timestamp\"))\n    )\n    .withColumn(\n        \"publication_date\",\n        F.when(\n            F.col(\"publication_date\").between(F.lit(\"1000-01-01\"), F.lit(\"2050-12-31\")),\n            F.col(\"publication_date\")\n        ).otherwise(F.lit(None).cast(\"date\"))\n    )\n    .filter(F.col(\"id\").isNotNull())\n)\n\n# Dynamic partitioning based on record volume\n# Only apply partition optimization for non-full syncs\nif not IS_FULL_SYNC and record_count is not None:\n    # Calculate optimal partition count:\n    # - Small updates (<500k): use fewer partitions for efficiency\n    # - Medium updates (500k-5M): moderate partitions  \n    # - Large updates (5M-20M): many partitions like full sync\n    # - Very large updates (>20M): use repartitionByRange for even distribution\n    RECORDS_PER_PARTITION = 10000  # Target ~10k records per partition\n    \n    if record_count < 2_000_000:\n        # Small daily update - coalesce to reduce overhead\n        optimal_partitions = max(64, record_count // RECORDS_PER_PARTITION)\n        df = df.coalesce(optimal_partitions)\n        print(f\"Small update: coalesced to {optimal_partitions} partitions\")\n    elif record_count < 10_000_000:\n        # Medium daily update - use more partitions\n        optimal_partitions = max(1024, record_count // RECORDS_PER_PARTITION)\n        df = df.repartition(optimal_partitions)\n        print(f\"Medium update: repartitioned to {optimal_partitions} partitions\")\n    elif record_count < 20_000_000:\n        # Large daily update - repartition for better distribution\n        optimal_partitions = min(4096, record_count // RECORDS_PER_PARTITION)\n        df = df.repartition(optimal_partitions)\n        print(f\"Large update: repartitioned to {optimal_partitions} partitions\")\n    else:\n        # Very large update - use repartitionByRange like full sync\n        df = df.repartitionByRange(8096, \"id\")\n        print(f\"Very large update: using repartitionByRange with 8096 partitions\")\n\nprint(f\"SQL query:\\n{SQL_QUERY}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27962577-7f39-4672-b6b9-18201d2b2d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def truncate_abstract_index_string(raw_json: str, max_bytes: int = 32760) -> str:\n",
    "    \"\"\"\n",
    "    Truncate inverted index JSON by finding a safe cutoff point.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not raw_json:\n",
    "            return None\n",
    "            \n",
    "        encoded = raw_json.encode('utf-8')\n",
    "        if len(encoded) <= max_bytes:\n",
    "            return raw_json\n",
    "            \n",
    "        safe_bytes = max_bytes - 100\n",
    "        truncated = encoded[:safe_bytes].decode('utf-8', errors='ignore')\n",
    "        \n",
    "        last_complete_array = -1\n",
    "        for pattern in ['],\"', '],']:\n",
    "            pos = truncated.rfind(pattern)\n",
    "            if pos > last_complete_array:\n",
    "                last_complete_array = pos\n",
    "                \n",
    "        if last_complete_array == -1:\n",
    "            return '{}'\n",
    "            \n",
    "        if truncated[last_complete_array:last_complete_array+3] == '],\"':\n",
    "            result = truncated[:last_complete_array+1] + '}'\n",
    "        else:\n",
    "            result = truncated[:last_complete_array+1] + '}'\n",
    "            \n",
    "        if result.count('{') != result.count('}'):\n",
    "            return '{}'\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def sanitize_name(col_name: str):\n",
    "  \"\"\"\n",
    "  Cleans a string column by removing unwanted characters and normalizing whitespace.\n",
    "  Handles multilingual text by preserving letters, numbers, punctuation, and symbols from all Unicode scripts.\n",
    "\n",
    "  Args:\n",
    "    col_name: The name of the column to sanitize.\n",
    "\n",
    "  Returns:\n",
    "    A PySpark Column object with the cleaning transformations applied.\n",
    "  \"\"\"\n",
    "  # Pattern to match any character that is NOT a letter, number, punctuation, symbol, or space (in any language).\n",
    "  # Usually they show up due to encoding changes (from windows-1251, etc)\n",
    "  unwanted_chars_pattern = r\"[^\\p{L}\\p{N}\\p{P}\\p{S}\\p{Z}]\"\n",
    "  # Pattern to match one or more whitespace characters.\n",
    "  multiple_spaces_pattern = r\"\\s+\"\n",
    "\n",
    "  return F.trim( # trim to a single space\n",
    "      F.regexp_replace( \n",
    "          F.regexp_replace(F.col(col_name), unwanted_chars_pattern, \"\"), # replace with empty string (risk - having them as word boundaries)\n",
    "          multiple_spaces_pattern, \" \" # collapse the 2+ spaces\n",
    "      )\n",
    "  )\n",
    "\n",
    "def sanitize_string(col_name: str, max_len: int = 32000):\n",
    "    return F.when(F.col(col_name).isNotNull(), F.substring(F.col(col_name), 1, max_len)).otherwise(None)\n",
    "\n",
    "empty_sdg_array = F.array().cast(\"array<struct<id:string,display_name:string,score:double>>\")\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    "    .withColumn(\"publication_year\", F.coalesce(\n",
    "        F.col(\"publication_year\"),\n",
    "        F.year(F.col(\"publication_date\"))\n",
    "    )) # remove setting 1800 on NULL\n",
    "    .withColumn(\"publication_year\", F.year(\"publication_date\"))\n",
    "    .withColumn(\"title\", sanitize_name(\"title\"))\n",
    "    .withColumn(\"display_name\", sanitize_name(\"display_name\"))\n",
    "    .withColumn(\"ids\", \n",
    "        F.transform_values(\"ids\",\n",
    "            lambda k, v: F.when(k == \"doi\", \n",
    "                    F.concat(F.lit(\"https://doi.org/\"),v)).otherwise(v)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"doi\", sanitize_string(\"doi\"))\n",
    "    .withColumn(\"language\", sanitize_string(\"language\"))\n",
    "    .withColumn(\"type\", sanitize_string(\"type\"))\n",
    "    .withColumn(\"abstract\", sanitize_string(\"abstract\"))\n",
    "    .withColumn(\"referenced_works\", \n",
    "                F.expr(\"transform(referenced_works, x -> 'https://openalex.org/W' || x)\"))\n",
    "    .withColumn(\"referenced_works_count\",\n",
    "                F.when(F.col(\"referenced_works\").isNotNull(), F.size(\"referenced_works\")).otherwise(0))\n",
    "    .withColumn(\"abstract_inverted_index\", truncate_abstract_index_string(F.col(\"abstract_inverted_index\")))\n",
    "    .withColumn(\"open_access\", F.struct(\n",
    "        F.col(\"open_access.is_oa\"),\n",
    "        sanitize_string(\"open_access.oa_status\").alias(\"oa_status\"),\n",
    "        F.lit(False).cast(\"boolean\").alias(\"any_repository_has_fulltext\"),\n",
    "        F.col(\"open_access.oa_url\")\n",
    "    ))\n",
    "    # Build full authorships first, then truncate for the limited version\n",
    "    .withColumn(\"authorships_full\", F.expr(\"\"\"\n",
    "        transform(authorships, x -> named_struct(\n",
    "            'affiliations', x.affiliations,\n",
    "            'author', x.author,\n",
    "            'author_position', substring(x.author_position, 1, 32000),\n",
    "            'countries', x.countries,\n",
    "            'raw_author_name', substring(x.raw_author_name, 1, 32000),\n",
    "            'is_corresponding', x.is_corresponding,\n",
    "            'raw_affiliation_strings', transform(x.raw_affiliation_strings, aff -> substring(aff, 1, 32000)),\n",
    "            'institutions', x.institutions\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"authorships\", F.slice(F.col(\"authorships_full\"), 1, 100))\n",
    "    .withColumn(\"locations\", F.expr(\"\"\"\n",
    "        transform(locations, x -> named_struct(\n",
    "            'is_oa', x.is_oa,\n",
    "            'is_published', x.version = 'publishedVersion',\n",
    "            'landing_page_url', substring(x.landing_page_url, 1, 32000),\n",
    "            'pdf_url', substring(x.pdf_url, 1, 32000),\n",
    "            'source', x.source,\n",
    "            'raw_source_name', x.raw_source_name,\n",
    "            'raw_type', x.raw_type,\n",
    "            'native_id', x.native_id,\n",
    "            'provenance', x.provenance,\n",
    "            'license', x.license,\n",
    "            'license_id', x.license_id,\n",
    "            'version', x.version,\n",
    "            'is_accepted', x.is_accepted\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    # limit to a reasonable number (they go up to 130) - mainly for xpac\n",
    "    .withColumn(\"concepts\", F.slice(F.col(\"concepts\"), 1, 40))\n",
    "    .withColumn(\"has_fulltext\", F.col(\"fulltext\").isNotNull())\n",
    "    .withColumn(\"_source\", F.struct(\n",
    "        F.col(\"id\"),\n",
    "        F.col(\"doi\"),\n",
    "        F.col(\"title\"),\n",
    "        F.col(\"display_name\"),\n",
    "        F.col(\"ids\"),\n",
    "        # move this to openalex_works at some point if it is useful in other contexts\n",
    "        F.expr(\"\"\"\n",
    "            array_sort(\n",
    "                array_distinct(\n",
    "                    array_compact(\n",
    "                        flatten(\n",
    "                            TRANSFORM(locations, loc ->\n",
    "                                CASE\n",
    "                                WHEN loc.provenance IN ('crossref', 'pubmed', 'datacite')\n",
    "                                    THEN array(loc.provenance, IF(loc.source.is_in_doaj, 'doaj', NULL))\n",
    "                                WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:arxiv.org%'\n",
    "                                    THEN array('arxiv')\n",
    "                                WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:doaj.org/%'\n",
    "                                    THEN array('doaj')\n",
    "                                WHEN loc.provenance = 'mag' AND lower(loc.source.display_name) = 'pubmed'\n",
    "                                    THEN array('pubmed')\n",
    "                                ELSE array()\n",
    "                                END\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \"\"\").alias(\"indexed_in\"),\n",
    "        F.col(\"publication_date\"),\n",
    "        F.col(\"publication_year\"),\n",
    "        F.col(\"language\"),\n",
    "        F.col(\"type\"),\n",
    "        F.coalesce(F.col(\"authorships\"), F.lit([])).alias(\"authorships\"),\n",
    "        F.coalesce(F.col(\"authorships_full\"), F.lit([])).alias(\"authorships_full\"),\n",
    "        F.col(\"authors_count\"),\n",
    "        F.coalesce(F.col(\"corresponding_author_ids\"), F.lit([])).alias(\"corresponding_author_ids\"),\n",
    "        F.coalesce(F.col(\"corresponding_institution_ids\"), F.lit([])).alias(\"corresponding_institution_ids\"),\n",
    "        F.col(\"primary_topic\"),\n",
    "        F.col(\"topics\"),\n",
    "        F.col(\"keywords\"),\n",
    "        F.col(\"concepts\"),\n",
    "        F.col(\"locations\"),\n",
    "        F.col(\"locations_count\"),\n",
    "        F.col(\"primary_location\"),\n",
    "        F.col(\"best_oa_location\"),\n",
    "        F.coalesce(F.col(\"sustainable_development_goals\"), empty_sdg_array).alias(\"sustainable_development_goals\"),\n",
    "        F.col(\"awards\"),\n",
    "        F.col(\"funders\"),\n",
    "        F.col(\"institutions\"),\n",
    "        F.col(\"countries_distinct_count\"),\n",
    "        F.col(\"institutions_distinct_count\"),\n",
    "        F.col(\"open_access\"),\n",
    "        F.col(\"is_paratext\"),\n",
    "        F.col(\"is_retracted\"),\n",
    "        F.col(\"is_xpac\"),\n",
    "        F.col(\"biblio\"),\n",
    "        F.col(\"abstract\"),\n",
    "        F.col(\"referenced_works\"),\n",
    "        F.col(\"referenced_works_count\"),\n",
    "        F.coalesce(F.col(\"related_works\"), F.lit([])).alias(\"related_works\"),\n",
    "        F.col(\"abstract_inverted_index\"),\n",
    "        F.col(\"cited_by_count\"),\n",
    "        F.col(\"counts_by_year\"),\n",
    "        F.col(\"apc_list\"),\n",
    "        F.col(\"apc_paid\"),\n",
    "        F.col(\"fwci\"),\n",
    "        F.col(\"citation_normalized_percentile\"),\n",
    "        F.col(\"cited_by_percentile_year\"),\n",
    "        F.coalesce(F.col(\"mesh\"), F.lit([])).alias(\"mesh\"),\n",
    "        F.col(\"has_abstract\"),\n",
    "        F.col(\"has_content\"),\n",
    "        F.col(\"fulltext\"),\n",
    "        F.col(\"has_fulltext\"),\n",
    "        F.col(\"created_date\"),\n",
    "        F.col(\"updated_date\"),\n",
    "        F.current_timestamp().alias(\"indexed_timestamp\")\n",
    "    ))\n",
    "    .select(\"id\", \"_source\")\n",
    ")\n",
    "\n",
    "# Record count was already printed in cell-5 using lightweight SQL COUNT\n",
    "# No need to call df_transformed.count() here - it causes OOM on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "875a5517-e669-4d17-ad7c-b5961ef7b4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c83d03-9157-4377-8154-4ddf8d646ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_schema = StructType([\n",
    "    StructField(\"index_name\", StringType(), True),\n",
    "    StructField(\"run_id\", StringType(), True),\n",
    "    StructField(\"partition_id\", IntegerType(), True),\n",
    "    StructField(\"indexed_count\", IntegerType(), True),\n",
    "    StructField(\"skipped_count\", IntegerType(), True),\n",
    "    StructField(\"parsing_error_count\", IntegerType(), True),\n",
    "    StructField(\"indexing_error_count\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"parsing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    ),\n",
    "    StructField(\n",
    "        \"indexing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "\n",
    "])\n",
    "\n",
    "@dataclass\n",
    "class DatabricksEnvInfo:\n",
    "    job_id: str = None\n",
    "    run_id: str = None\n",
    "    command_run_id: str = None\n",
    "    notebook_path: str = None\n",
    "    user: str = None\n",
    "\n",
    "def get_databricks_env_info() -> DatabricksEnvInfo:\n",
    "    attr = json.loads(\n",
    "        dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()\n",
    "    )[\"attributes\"]\n",
    "    return DatabricksEnvInfo(\n",
    "        command_run_id = attr.get(\"commandRunId\"),\n",
    "        job_id = attr.get(\"jobId\"),\n",
    "        run_id = attr.get(\"currentRunId\"),\n",
    "        notebook_path = attr.get(\"notebook_path\"),\n",
    "        user = attr.get(\"user\"),\n",
    "    )\n",
    "\n",
    "dbx_env = get_databricks_env_info()\n",
    "\n",
    "def get_run_identifier():\n",
    "    if dbx_env.job_id and dbx_env.run_id:\n",
    "        return f\"{dbx_env.job_id}-{dbx_env.run_id}\"\n",
    "    else:\n",
    "        return f\"{dbx_env.user}-{dbx_env.command_run_id}\"\n",
    "    \n",
    "def generate_prepared_actions(partition, parsing_errors, op_type = \"index\"):\n",
    "    for row in partition:\n",
    "        try:\n",
    "            yield {\n",
    "                \"_op_type\": op_type,\n",
    "                \"_index\": ELASTIC_INDEX,\n",
    "                \"_id\": row.id,\n",
    "                \"_source\": row._source.asDict(True)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            parsing_errors.append({\"row_id\": row.id, \"error\": str(e)})             \n",
    "\n",
    "def send_partition_to_elastic(partition, partition_id, op_type=\"create\"):\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "\n",
    "    indexed_count = 0\n",
    "    parsing_errors = []\n",
    "    indexing_errors = []\n",
    "    skipped_count = 0\n",
    "    op_type = op_type.lower()\n",
    "\n",
    "    try:\n",
    "        for success, info in helpers.parallel_bulk(client,\n",
    "                generate_prepared_actions(partition, parsing_errors, op_type),\n",
    "                chunk_size=500, thread_count=4, queue_size=10\n",
    "            ):\n",
    "\n",
    "            if success:\n",
    "                indexed_count += 1\n",
    "            else:\n",
    "                error_info = info.get(op_type, {})\n",
    "                status = error_info.get(\"status\", 0)\n",
    "\n",
    "                # âœ… Skip 409 (document already exists)\n",
    "                if status == 409:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                id_url = error_info.get('_id')\n",
    "                row_id = None\n",
    "                if id_url:\n",
    "                    try:\n",
    "                        row_id = int(id_url.replace(\"https://openalex.org/W\", \"\"))\n",
    "                    except ValueError:\n",
    "                        row_id = -1\n",
    "\n",
    "                indexing_errors.append({\n",
    "                    \"row_id\": row_id,\n",
    "                    \"error\": str(info)[:1000]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        indexing_errors.append({\n",
    "            \"row_id\": None,\n",
    "            \"error\": \"Parallel Bulk Error: \" + str(e)[:1000]\n",
    "        })\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "    log_entry = {\n",
    "        \"index_name\": ELASTIC_INDEX,\n",
    "        \"run_id\": get_run_identifier(),\n",
    "        \"partition_id\": partition_id,\n",
    "        \"success\": len(indexing_errors) == 0,\n",
    "        \"indexed_count\": indexed_count,\n",
    "        \"skipped_count\": skipped_count,\n",
    "        \"parsing_error_count\": len(parsing_errors),\n",
    "        \"indexing_error_count\": len(indexing_errors),\n",
    "        \"message\": f\"{indexed_count} records indexed. {skipped_count} skipped. {len(parsing_errors)} parsing errors. {len(indexing_errors)} ES errors.\",\n",
    "        \"parsing_errors\": parsing_errors[:1000],\n",
    "        \"indexing_errors\": indexing_errors[:1000],\n",
    "    }\n",
    "\n",
    "    yield log_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6efaaa97-04af-4398-89a8-e27e5b95eff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute Sync with `mapPartitionsWithIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48733138-6d67-472b-ae77-2672aa2e315e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logs_rdd = df_transformed.rdd.mapPartitionsWithIndex(\n",
    "    lambda partition_idx, partition: send_partition_to_elastic(partition, partition_idx, \"index\")\n",
    ")\n",
    "logs_df = spark.createDataFrame(logs_rdd, log_schema)\n",
    "\n",
    "log_count = logs_df.count() # mapPartitionsWithIndex is lazy, so force it to run\n",
    "print(f\"Processed {log_count} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3459f92a-4875-4d8f-ae9b-f180a6b30e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # refresh index if exists\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "    \n",
    "    if client.indices.exists(index=ELASTIC_INDEX):\n",
    "        client.indices.refresh(index=ELASTIC_INDEX)\n",
    "        print(f\"Refreshed index {ELASTIC_INDEX}\")\n",
    "        print(f\"{client.count(index=ELASTIC_INDEX)['count']} documents in {ELASTIC_INDEX}\")\n",
    "        \n",
    "        # Restore replicas after full sync\n",
    "        if IS_FULL_SYNC:\n",
    "            client.indices.put_settings(index=ELASTIC_INDEX, body={\n",
    "                \"index\": {\"number_of_replicas\": 1}\n",
    "            })\n",
    "            print(f\"Restored replicas to 1 on {ELASTIC_INDEX}\")\n",
    "    else:\n",
    "        print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "finally:\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5061607116769740,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_works",
   "widgets": {
    "delete_index": {
     "currentValue": "false",
     "nuid": "76eabd73-36bb-4de9-8dac-a93515afffda",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "delete_index",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": "",
      "name": "delete_index",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "is_full_sync": {
     "currentValue": "false",
     "nuid": "caa87eee-7e67-41ba-96ff-1a7cf35763a3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "is_full_sync",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": "",
      "name": "is_full_sync",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "op_type": {
     "currentValue": "index",
     "nuid": "fb961533-e7c1-4948-9c3c-e96d85e00ace",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "index",
      "label": "",
      "name": "op_type",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "index",
      "label": "",
      "name": "op_type",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "oreo_only": {
     "currentValue": "false",
     "nuid": "610d3f51-2b8a-4f0f-8592-3e24f7e3d89d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "oreo_only",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": "",
      "name": "oreo_only",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "repartition": {
     "currentValue": "false",
     "nuid": "d223c8a8-0d71-421c-a28d-0eb694213c7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "repartition",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": "",
      "name": "repartition",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}