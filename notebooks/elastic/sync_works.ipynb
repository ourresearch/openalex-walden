{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104bf6c9-96bc-4a41-a4ba-c50d12c4084f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c44ab9-06bf-4229-91b0-3a50bd763254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "ELASTIC_INDEX = \"works-v26\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "MAX_LENGTH = 32000  # Slightly below the 32766 limit\n",
    "\n",
    "OP_TYPE = dbutils.widgets.get(\"op_type\")\n",
    "DELETE_INDEX = dbutils.widgets.get(\"delete_index\").lower() == \"true\" # default is false\n",
    "REPARTITION = dbutils.widgets.get(\"repartition\").lower() == \"true\" # default is false\n",
    "IS_FULL_SYNC = dbutils.widgets.get(\"is_full_sync\").lower() == \"true\" # default is incremental\n",
    "OREO_ONLY = dbutils.widgets.get(\"oreo_only\").lower() == \"true\"\n",
    "\n",
    "print(f\"OP_TYPE: {OP_TYPE}\")\n",
    "print(f\"DELETE_INDEX: {DELETE_INDEX}\")\n",
    "print(f\"REPARTITION: {REPARTITION}\")\n",
    "print(f\"IS_FULL_SYNC: {IS_FULL_SYNC}\")\n",
    "print(f\"OREO_ONLY: {OREO_ONLY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c33ea0-e672-4580-8bc4-bdbfc74c422d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete Index if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93a6bc4-1be1-43f7-a5f3-89646403b060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if DELETE_INDEX:\n",
    "  print(f\"WARNING: Deleting {ELASTIC_INDEX}...\")\n",
    "  try:\n",
    "    # delete index if exists\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True\n",
    "    )\n",
    "    if client.indices.exists(index=ELASTIC_INDEX):\n",
    "        client.indices.delete(index=ELASTIC_INDEX)\n",
    "        print(f\"Deleted index {ELASTIC_INDEX}\")\n",
    "    else:\n",
    "        print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "  finally:\n",
    "      client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0aa700d-33aa-4bb2-8158-6921ddba8c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff580d2-f6cb-4ffd-bfc3-c947f7900fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SQL_QUERY = \"\"\"SELECT *\n",
    "                FROM openalex.works.openalex_works\n",
    "                WHERE openalex_works.created_date >= current_date() - INTERVAL 2 days \n",
    "                OR openalex_works.updated_date >= current_date() - INTERVAL 2 days\n",
    "              \"\"\"\n",
    "if (IS_FULL_SYNC):\n",
    "    SQL_QUERY = \"\"\"SELECT * EXCEPT (fulltext) FROM openalex.works.openalex_works\"\"\"\n",
    "\n",
    "if (OREO_ONLY):\n",
    "    SQL_QUERY = \"\"\"SELECT * FROM openalex.works.openalex_works_oreo\"\"\"\n",
    "\n",
    "df = (\n",
    "    # TEST (to avoid RDD accessing Parquet files directly via spark.read.table)\n",
    "    spark.sql(SQL_QUERY)\n",
    "    # spark.read.table(\"openalex.works.openalex_works\")\n",
    "    .withColumn(\"display_name\", F.col(\"title\"))\n",
    "    # First cast to date/timestamp\n",
    "    .withColumn(\"created_date\", F.to_timestamp(\"created_date\"))\n",
    "    .withColumn(\"updated_date\", F.to_timestamp(\"updated_date\"))\n",
    "    .withColumn(\"publication_date\", F.to_date(\"publication_date\"))\n",
    "    .withColumn(\n",
    "        \"concepts\",\n",
    "        F.transform(\n",
    "            F.col(\"concepts\"),\n",
    "            lambda c: F.struct(\n",
    "                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n",
    "                c.wikidata.alias(\"wikidata\"),\n",
    "                c.display_name.alias(\"display_name\"),\n",
    "                c.level.alias(\"level\"),\n",
    "                c.score.alias(\"score\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Apply range checks using BETWEEN\n",
    "    .withColumn(\n",
    "        \"created_date\",\n",
    "        F.when(\n",
    "            F.col(\"created_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"created_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"updated_date\",\n",
    "        F.when(\n",
    "            F.col(\"updated_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"updated_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"publication_date\",\n",
    "        F.when(\n",
    "            F.col(\"publication_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"publication_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27962577-7f39-4672-b6b9-18201d2b2d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def truncate_abstract_index_string(raw_json: str, max_bytes: int = 32760) -> str:\n",
    "    try:\n",
    "        if not raw_json:\n",
    "            return None\n",
    "\n",
    "        if len(raw_json) <= (max_bytes // 4):\n",
    "            return raw_json\n",
    "\n",
    "        encoded = raw_json.encode('utf-8')\n",
    "        if len(encoded) <= max_bytes:\n",
    "            return raw_json\n",
    "\n",
    "        truncated = encoded[:max_bytes].decode('utf-8', errors='ignore')\n",
    "        last_bracket = truncated.rfind(']')\n",
    "        if last_bracket == -1:\n",
    "            return None\n",
    "\n",
    "        return truncated[:last_bracket + 1] + '}'\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def sanitize_string(col_name: str, max_len: int = 32000):\n",
    "    return F.when(F.col(col_name).isNotNull(), F.substring(F.col(col_name), 1, max_len)).otherwise(None)\n",
    "\n",
    "empty_sdg_array = F.array().cast(\"array<struct<id:string,display_name:string,score:double>>\")\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    "    .withColumn(\"publication_year\", F.when(F.col(\"publication_date\").isNotNull(), F.year(\"publication_date\")).otherwise(1800))\n",
    "    .withColumn(\"title\", sanitize_string(\"title\"))\n",
    "    .withColumn(\"display_name\", sanitize_string(\"display_name\"))\n",
    "    .withColumn(\"ids\", \n",
    "        F.transform_values(\"ids\",\n",
    "            lambda k, v: F.when(k == \"doi\", \n",
    "                    F.concat(F.lit(\"https://doi.org/\"),v)).otherwise(v)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"doi\", sanitize_string(\"doi\"))\n",
    "    .withColumn(\"language\", sanitize_string(\"language\"))\n",
    "    .withColumn(\"type\", sanitize_string(\"type\"))\n",
    "    .withColumn(\"abstract\", sanitize_string(\"abstract\"))\n",
    "    .withColumn(\"referenced_works\", \n",
    "                F.expr(\"transform(referenced_works, x -> 'https://openalex.org/W' || x)\"))\n",
    "    .withColumn(\"referenced_works_count\", \n",
    "                F.when(F.col(\"referenced_works\").isNotNull(), F.size(\"referenced_works\")).otherwise(0))\n",
    "    .withColumn(\"abstract_inverted_index\", truncate_abstract_index_string(F.col(\"abstract_inverted_index\")))\n",
    "    .withColumn(\"open_access\", F.struct(\n",
    "        F.col(\"open_access.is_oa\"),\n",
    "        sanitize_string(\"open_access.oa_status\").alias(\"oa_status\"),\n",
    "        F.lit(None).cast(\"boolean\").alias(\"any_repository_has_fulltext\")\n",
    "    ))\n",
    "    .withColumn(\"authorships\", F.expr(\"\"\"\n",
    "        transform(authorships, x -> named_struct(\n",
    "            'affiliations', x.affiliations,\n",
    "            'author', x.author,\n",
    "            'author_position', substring(x.author_position, 1, 32000),\n",
    "            'countries', x.countries,\n",
    "            'raw_author_name', substring(x.raw_author_name, 1, 32000),\n",
    "            'is_corresponding', x.is_corresponding,\n",
    "            'raw_affiliation_strings', transform(x.raw_affiliation_strings, aff -> substring(aff, 1, 32000)),\n",
    "            'institutions', x.institutions\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"locations\", F.expr(\"\"\"\n",
    "        transform(locations, x -> named_struct(\n",
    "            'is_oa', x.is_oa,\n",
    "            'is_published', x.version = 'publishedVersion',\n",
    "            'landing_page_url', substring(x.landing_page_url, 1, 32000),\n",
    "            'pdf_url', substring(x.pdf_url, 1, 32000),\n",
    "            'source', x.source,\n",
    "            'native_id', x.native_id,\n",
    "            'provenance', x.provenance,\n",
    "            'license', x.license,\n",
    "            'license_id', x.license_id\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    # limit to a reasonable number (they go up to 130) - mainly for xpac\n",
    "    .withColumn(\"concepts\", F.slice(F.col(\"concepts\"), 1, 40))\n",
    "    .withColumn(\"_source\", F.struct(\n",
    "        F.col(\"id\"),\n",
    "        F.col(\"doi\"),\n",
    "        F.col(\"title\"),\n",
    "        F.col(\"display_name\"),\n",
    "        F.col(\"ids\"),\n",
    "        # move this to openalex_works at some point if it is useful in other contexts\n",
    "        F.expr(\"\"\"\n",
    "            array_sort(\n",
    "                array_distinct(\n",
    "                    array_compact(\n",
    "                        flatten(\n",
    "                            TRANSFORM(locations, loc ->\n",
    "                                CASE\n",
    "                                WHEN loc.provenance IN ('crossref', 'pubmed', 'datacite')\n",
    "                                    THEN array(loc.provenance, IF(loc.source.is_in_doaj, 'doaj', NULL))\n",
    "                                WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:arxiv.org%'\n",
    "                                    THEN array('arxiv')\n",
    "                                WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:doaj.org/%'\n",
    "                                    THEN array('doaj')\n",
    "                                WHEN loc.provenance = 'mag' AND lower(loc.source.display_name) = 'pubmed'\n",
    "                                    THEN array('pubmed')\n",
    "                                ELSE array()\n",
    "                                END\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \"\"\").alias(\"indexed_in\"),\n",
    "        F.col(\"publication_date\"),\n",
    "        F.col(\"publication_year\"),\n",
    "        F.col(\"language\"),\n",
    "        F.col(\"type\"),\n",
    "        F.col(\"authorships\"),\n",
    "        F.coalesce(F.col(\"corresponding_author_ids\"), F.lit([])).alias(\"corresponding_author_ids\"),\n",
    "        F.coalesce(F.col(\"corresponding_institution_ids\"), F.lit([])).alias(\"corresponding_institution_ids\"),\n",
    "        F.col(\"primary_topic\"),\n",
    "        F.col(\"topics\"),\n",
    "        F.col(\"keywords\"),\n",
    "        F.col(\"concepts\"),\n",
    "        F.col(\"locations\"),\n",
    "        F.col(\"locations_count\"),\n",
    "        F.col(\"primary_location\"),\n",
    "        F.col(\"best_oa_location\"),\n",
    "        F.coalesce(F.col(\"sustainable_development_goals\"), empty_sdg_array).alias(\"sustainable_development_goals\"),\n",
    "        F.col(\"grants\"),\n",
    "        F.col(\"awards\"),\n",
    "        F.col(\"open_access\"),\n",
    "        F.col(\"is_paratext\"),\n",
    "        F.col(\"is_retracted\"),\n",
    "        F.col(\"is_xpac\"),\n",
    "        F.col(\"biblio\"),\n",
    "        F.col(\"abstract\"),\n",
    "        F.col(\"referenced_works\"),\n",
    "        F.col(\"referenced_works_count\"),\n",
    "        F.coalesce(F.col(\"related_works\"), F.lit([])).alias(\"related_works\"),\n",
    "        F.col(\"abstract_inverted_index\"),\n",
    "        F.col(\"cited_by_count\"),\n",
    "        F.col(\"counts_by_year\"),\n",
    "        F.col(\"apc_list\"),\n",
    "        F.col(\"apc_paid\"),\n",
    "        F.coalesce(F.col(\"fwci\"),F.lit(0)).alias(\"fwci\"),\n",
    "        F.col(\"citation_normalized_percentile\"),\n",
    "        F.col(\"cited_by_percentile_year\"),\n",
    "        F.coalesce(F.col(\"mesh\"), F.lit([])).alias(\"mesh\"),\n",
    "        F.col(\"has_abstract\"),\n",
    "        F.col(\"created_date\"),\n",
    "        F.col(\"updated_date\"),\n",
    "        F.current_timestamp().alias(\"indexed_timestamp\")\n",
    "    ))\n",
    "    .select(\"id\", \"_source\")\n",
    ")\n",
    "# print(f\"Transformed {df_transformed.count()} records.\")\n",
    "display(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875a5517-e669-4d17-ad7c-b5961ef7b4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c83d03-9157-4377-8154-4ddf8d646ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_schema = StructType([\n",
    "    StructField(\"index_name\", StringType(), True),\n",
    "    StructField(\"run_id\", StringType(), True),\n",
    "    StructField(\"partition_id\", IntegerType(), True),\n",
    "    StructField(\"indexed_count\", IntegerType(), True),\n",
    "    StructField(\"skipped_count\", IntegerType(), True),\n",
    "    StructField(\"parsing_error_count\", IntegerType(), True),\n",
    "    StructField(\"indexing_error_count\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"parsing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    ),\n",
    "    StructField(\n",
    "        \"indexing_errors\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"row_id\", LongType(), True),\n",
    "                StructField(\"error\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "@dataclass\n",
    "class DatabricksEnvInfo:\n",
    "    job_id: str = None\n",
    "    run_id: str = None\n",
    "    command_run_id: str = None\n",
    "    notebook_path: str = None\n",
    "    user: str = None\n",
    "\n",
    "def get_databricks_env_info() -> DatabricksEnvInfo:\n",
    "    attr = json.loads(\n",
    "        dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()\n",
    "    )[\"attributes\"]\n",
    "    return DatabricksEnvInfo(\n",
    "        command_run_id = attr.get(\"commandRunId\"),\n",
    "        job_id = attr.get(\"jobId\"),\n",
    "        run_id = attr.get(\"currentRunId\"),\n",
    "        notebook_path = attr.get(\"notebook_path\"),\n",
    "        user = attr.get(\"user\"),\n",
    "    )\n",
    "\n",
    "dbx_env = get_databricks_env_info()\n",
    "\n",
    "def get_run_identifier():\n",
    "    if dbx_env.job_id and dbx_env.run_id:\n",
    "        return f\"{dbx_env.job_id}-{dbx_env.run_id}\"\n",
    "    else:\n",
    "        return f\"{dbx_env.user}-{dbx_env.command_run_id}\"\n",
    "    \n",
    "def generate_prepared_actions(partition, parsing_errors, op_type = \"index\"):\n",
    "    for row in partition:\n",
    "        try:\n",
    "            yield {\n",
    "                \"_op_type\": op_type,\n",
    "                \"_index\": ELASTIC_INDEX,\n",
    "                \"_id\": row.id,\n",
    "                \"_source\": row._source.asDict(True)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            parsing_errors.append({\"row_id\": row.id, \"error\": str(e)})             \n",
    "\n",
    "def send_partition_to_elastic(partition, partition_id, op_type=\"create\"):\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "\n",
    "    indexed_count = 0\n",
    "    parsing_errors = []\n",
    "    indexing_errors = []\n",
    "    skipped_count = 0\n",
    "    op_type = op_type.lower()\n",
    "\n",
    "    try:\n",
    "        for success, info in helpers.parallel_bulk(client,\n",
    "                generate_prepared_actions(partition, parsing_errors, op_type),\n",
    "                chunk_size=500, thread_count=4, queue_size=10\n",
    "            ):\n",
    "\n",
    "            if success:\n",
    "                indexed_count += 1\n",
    "            else:\n",
    "                error_info = info.get(op_type, {})\n",
    "                status = error_info.get(\"status\", 0)\n",
    "\n",
    "                # âœ… Skip 409 (document already exists)\n",
    "                if status == 409:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                id_url = error_info.get('_id')\n",
    "                row_id = None\n",
    "                if id_url:\n",
    "                    try:\n",
    "                        row_id = int(id_url.replace(\"https://openalex.org/W\", \"\"))\n",
    "                    except ValueError:\n",
    "                        row_id = -1\n",
    "\n",
    "                indexing_errors.append({\n",
    "                    \"row_id\": row_id,\n",
    "                    \"error\": str(info)[:1000]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        indexing_errors.append({\n",
    "            \"row_id\": None,\n",
    "            \"error\": \"Parallel Bulk Error: \" + str(e)[:1000]\n",
    "        })\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "    log_entry = {\n",
    "        \"index_name\": ELASTIC_INDEX,\n",
    "        \"run_id\": get_run_identifier(),\n",
    "        \"partition_id\": partition_id,\n",
    "        \"success\": len(indexing_errors) == 0,\n",
    "        \"indexed_count\": indexed_count,\n",
    "        \"skipped_count\": skipped_count,\n",
    "        \"parsing_error_count\": len(parsing_errors),\n",
    "        \"indexing_error_count\": len(indexing_errors),\n",
    "        \"message\": f\"{indexed_count} records indexed. {skipped_count} skipped. {len(parsing_errors)} parsing errors. {len(indexing_errors)} ES errors.\",\n",
    "        \"parsing_errors\": parsing_errors[:1000],\n",
    "        \"indexing_errors\": indexing_errors[:1000],\n",
    "    }\n",
    "\n",
    "    yield log_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6926e0b9-6671-4a38-bfe7-91e206bbe175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Re-balance partitions to avoid skew and driver OOM (FULL run only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2cdd9fc-9fbb-4f1b-9326-1283351babea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# takes too long - only if absolutely necessary\n",
    "if REPARTITION:\n",
    "    df_transformed = df_transformed.repartitionByRange(8096, \"id\")\n",
    "# Trigger the shuffle once while caching the dataset for reliability and to avoid re-computation during MapPartitionsWithIndex\n",
    "# can instead just write to openalex.works.works_api?\n",
    "# df_transformed.write.mode(\"overwrite\").saveAsTable(\"openalex.works.works_api_sync\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efaaa97-04af-4398-89a8-e27e5b95eff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute Sync with `mapPartitionsWithIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48733138-6d67-472b-ae77-2672aa2e315e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_input = spark.read.table(\"openalex.works.works_api_sync\")\n",
    "import pprint # Import the pretty-print library for clean output\n",
    "\n",
    "if (OREO_ONLY):\n",
    "    indexed_count = 0\n",
    "    # --- Your existing setup code remains the same ---\n",
    "    df_transformed_rows = df_transformed.collect()\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "\n",
    "    # --- Refined bulk indexing with detailed error reporting ---\n",
    "    indexed_count = 0\n",
    "    failed_docs = []\n",
    "\n",
    "    # Use streaming_bulk with a small chunk_size\n",
    "    for success, info in helpers.streaming_bulk(\n",
    "        client,\n",
    "        generate_prepared_actions(df_transformed_rows, \"index\"), # Assuming the generator is simplified\n",
    "        chunk_size=100 # Process in batches of 10\n",
    "    ):\n",
    "        if success:\n",
    "            indexed_count += 1\n",
    "        else:\n",
    "            # This block now correctly parses the error info\n",
    "            action, result = info.popitem()\n",
    "            doc_id = result.get(\"_id\", \"[unknown_id]\")\n",
    "            error_details = result.get(\"error\", {})\n",
    "            \n",
    "            # Print a clear error message\n",
    "            print(\"---\" * 15)\n",
    "            print(f\"ðŸ’¥ FAILED to index document ID: {doc_id}\")\n",
    "            pprint.pprint(error_details)\n",
    "            print(\"---\" * 15)\n",
    "            \n",
    "            # Keep track of failed documents\n",
    "            failed_docs.append(doc_id)\n",
    "        if (indexed_count % 100 == 0):\n",
    "            print(f\"Indexed {indexed_count} documents so far...\")\n",
    "\n",
    "    client.close()\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n--- BULK OPERATION COMPLETE ---\")\n",
    "    print(f\"Successfully indexed: {indexed_count} documents\")\n",
    "    if failed_docs:\n",
    "        print(f\"Failed to index: {len(failed_docs)} documents\")\n",
    "        print(f\"Failed document IDs: {failed_docs}\")\n",
    "else:\n",
    "    logs_rdd = df_transformed.rdd.mapPartitionsWithIndex(\n",
    "        lambda partition_idx, partition: send_partition_to_elastic(partition, partition_idx, OP_TYPE)\n",
    "    )\n",
    "    logs_df = spark.createDataFrame(logs_rdd, log_schema)\n",
    "\n",
    "    log_count = logs_df.count() # mapPartitionsWithIndex is lazy, so force it to run\n",
    "    print(f\"Processed {log_count} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3459f92a-4875-4d8f-ae9b-f180a6b30e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # delete index if exists\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=60)\n",
    "    \n",
    "    if client.indices.exists(index=ELASTIC_INDEX):\n",
    "        client.indices.refresh(index=ELASTIC_INDEX)\n",
    "        print(f\"Refreshed index {ELASTIC_INDEX}\")\n",
    "        print(f\"{client.count(index=ELASTIC_INDEX)['count']} documents in {ELASTIC_INDEX}\")\n",
    "    else:\n",
    "        print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95f480a-d2b8-47ba-a721-0e5b4ed249d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7510455811838486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_works",
   "widgets": {
    "delete_index": {
     "currentValue": "false",
     "nuid": "76eabd73-36bb-4de9-8dac-a93515afffda",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "delete_index",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "",
      "name": "delete_index",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "is_full_sync": {
     "currentValue": "true",
     "nuid": "caa87eee-7e67-41ba-96ff-1a7cf35763a3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "is_full_sync",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "",
      "name": "is_full_sync",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "op_type": {
     "currentValue": "index",
     "nuid": "fb961533-e7c1-4948-9c3c-e96d85e00ace",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "index",
      "label": "",
      "name": "op_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "index",
      "label": "",
      "name": "op_type",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "oreo_only": {
     "currentValue": "true",
     "nuid": "610d3f51-2b8a-4f0f-8592-3e24f7e3d89d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "oreo_only",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "",
      "name": "oreo_only",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "repartition": {
     "currentValue": "false",
     "nuid": "d223c8a8-0d71-421c-a28d-0eb694213c7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "",
      "name": "repartition",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "",
      "name": "repartition",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
