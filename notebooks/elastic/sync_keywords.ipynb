{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b5a0cb-d716-47b6-b870-63b0b96939d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb2099dd-f16c-42e4-8e01-5aca7efc34aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53192c77-0348-4cfe-af7f-7b4be0e5dbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "WITH exploded AS (\n",
    "    SELECT id as work_id, cited_by_count, explode(keywords) as keyword\n",
    "    FROM openalex.works.openalex_works\n",
    "),\n",
    "-- one row per (work_id, keyword_id)\n",
    "dedup AS (\n",
    "  SELECT work_id, cited_by_count, keyword\n",
    "  FROM exploded\n",
    "  QUALIFY row_number() OVER (PARTITION BY work_id, keyword.id ORDER BY work_id, keyword.id) = 1\n",
    "),\n",
    "-- Aggregate on unique keywords\n",
    "aggregated_counts AS (\n",
    "  SELECT\n",
    "    keyword.id as id,\n",
    "    keyword.display_name as display_name,\n",
    "    count(DISTINCT work_id) as works_count,\n",
    "    sum(cited_by_count) as cited_by_count\n",
    "  FROM dedup\n",
    "  GROUP BY 1, 2\n",
    ")\n",
    "-- Join with the common keywords table to get metadata\n",
    "SELECT\n",
    "  ac.id as id,\n",
    "  STRUCT(\n",
    "    ac.id,\n",
    "    ac.display_name,\n",
    "    ac.works_count,\n",
    "    ac.cited_by_count,\n",
    "    CONCAT(\"https://api.openalex.org/works?filter=keywords.id:keywords/\", kw.keyword_id) AS works_api_url,\n",
    "    kw.updated_datetime AS updated_date,\n",
    "    date(kw.created_datetime) as created_date\n",
    "  ) as _source\n",
    "FROM aggregated_counts ac\n",
    "JOIN openalex.common.keywords kw\n",
    "  ON kw.keyword_id = replace(ac.id, 'https://openalex.org/keywords/', '')\"\"\")\n",
    "\n",
    "rows = df.collect()\n",
    "\n",
    "print(f\"Keywords count: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca5877e-6a05-4917-8e46-36a85d5314d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "\n",
    "ELASTIC_INDEX = \"keywords-v1\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "\n",
    "client = Elasticsearch(\n",
    "    hosts = [ELASTIC_URL],\n",
    "    request_timeout = 180,\n",
    "    max_retries = 5,\n",
    "    retry_on_timeout = True\n",
    ")\n",
    "\n",
    "def actions_from_spark(rows, op_type = \"index\"):\n",
    "    for row in rows:\n",
    "        yield {\n",
    "            \"_op_type\": op_type,\n",
    "            \"_index\": ELASTIC_INDEX,\n",
    "            \"_id\": row.id,\n",
    "            \"_source\": row._source.asDict(True)\n",
    "        }\n",
    "\n",
    "# Delete old index\n",
    "if client.indices.exists(index=ELASTIC_INDEX):\n",
    "    client.indices.delete(index=ELASTIC_INDEX)\n",
    "\n",
    "ok = fail = 0\n",
    "for success, info in helpers.streaming_bulk(client, actions_from_spark(rows),\n",
    "    chunk_size=2000, request_timeout=60, max_retries=3):\n",
    "    if success:\n",
    "        ok += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "\n",
    "print(f\"Indexed ok={ok}, failed={fail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c84211f-6831-46a4-8b54-27d71925f3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.indices.refresh(index=ELASTIC_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f977f9c-4d20-4ad7-9abb-e230c311cd5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.count(index=ELASTIC_INDEX)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_keywords",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
