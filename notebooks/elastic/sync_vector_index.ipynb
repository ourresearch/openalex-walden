{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install elasticsearch==8.19.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sync Vector Index (works-vectors-v1)\n",
    "\n",
    "Bulk loads embeddings + 14 flat filter fields into a dedicated lightweight\n",
    "ES index optimized for kNN vector search (12 shards vs 72 on works-v33).\n",
    "\n",
    "Two-phase semantic search: kNN here returns IDs → mget full docs from works-v33.\n",
    "\n",
    "Run modes:\n",
    "- is_full_sync=true: Load all ~413M embeddings (initial load, ~6-8 hours)\n",
    "- is_full_sync=false: Load recent updates only (daily incremental)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "ELASTIC_INDEX = \"works-vectors-v1\"\n",
    "ELASTIC_URL = dbutils.secrets.get(scope=\"elastic\", key=\"elastic_url\")\n",
    "\n",
    "IS_FULL_SYNC = dbutils.widgets.get(\"is_full_sync\").lower() == \"true\"\n",
    "\n",
    "print(f\"IS_FULL_SYNC: {IS_FULL_SYNC}\")\n",
    "print(f\"Target index: {ELASTIC_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set replicas to 0 for faster bulk indexing during full sync\n",
    "if IS_FULL_SYNC:\n",
    "    try:\n",
    "        client = Elasticsearch(\n",
    "            hosts=[ELASTIC_URL],\n",
    "            request_timeout=180,\n",
    "            max_retries=5,\n",
    "            retry_on_timeout=True\n",
    "        )\n",
    "        if client.indices.exists(index=ELASTIC_INDEX):\n",
    "            client.indices.put_settings(index=ELASTIC_INDEX, body={\n",
    "                \"index\": {\n",
    "                    \"number_of_replicas\": 0,\n",
    "                    \"refresh_interval\": \"-1\"\n",
    "                }\n",
    "            })\n",
    "            print(f\"Set replicas=0, refresh=-1 on {ELASTIC_INDEX} for full sync\")\n",
    "        else:\n",
    "            print(f\"Index {ELASTIC_INDEX} does not exist\")\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "\n",
    "Join work_embeddings_v2 with openalex_works to get the 14 filter fields.\n",
    "Flatten authorships into arrays: author_ids, institution_ids, country_codes, funder_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\n# Batch processing configuration\nSTAGING_TABLE = \"openalex.vector_search.vector_sync_staging\"\nCHECKPOINT_TABLE = \"openalex.vector_search.vector_sync_checkpoint\"\nNUM_BATCHES = 200 if IS_FULL_SYNC else 10\n\n# --- Checkpoint table ---\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {CHECKPOINT_TABLE} (\n        batch_id INT, indexed_count LONG, skipped_count LONG,\n        error_count LONG, completed_at TIMESTAMP\n    )\n\"\"\")\ncompleted_rows = spark.sql(f\"SELECT batch_id FROM {CHECKPOINT_TABLE}\").collect()\ncompleted_batches = {row.batch_id for row in completed_rows}\n\n# --- Staging table (materialized join, partitioned by batch_id) ---\nstaging_exists = spark.catalog.tableExists(STAGING_TABLE)\n\nif IS_FULL_SYNC and staging_exists and len(completed_batches) > 0:\n    # Resuming an interrupted full sync — reuse existing staging data\n    total_staged = spark.sql(f\"SELECT COUNT(*) FROM {STAGING_TABLE}\").collect()[0][0]\n    print(f\"RESUMING full sync: {len(completed_batches)}/{NUM_BATCHES} batches done\")\n    print(f\"Staging table has {total_staged:,} records\")\nelse:\n    # Fresh start — clear checkpoint and rebuild staging\n    spark.sql(f\"TRUNCATE TABLE {CHECKPOINT_TABLE}\")\n    completed_batches = set()\n\n    if IS_FULL_SYNC:\n        SQL_QUERY = \"\"\"\n        SELECT\n          concat('https://openalex.org/W', e.work_id) as id,\n          e.embedding,\n          w.publication_year,\n          lower(w.type) as type,\n          w.open_access.is_oa as is_oa,\n          lower(w.language) as language,\n          array_compact(transform(w.authorships, a -> a.author.id)) as author_ids,\n          array_distinct(array_compact(flatten(\n            transform(w.authorships, a -> transform(a.institutions, i -> i.id))\n          ))) as institution_ids,\n          array_distinct(array_compact(flatten(\n            transform(w.authorships, a -> transform(a.institutions, i -> lower(i.country_code)))\n          ))) as country_codes,\n          w.is_retracted,\n          w.primary_location.source.id as source_id,\n          w.cited_by_count,\n          array_compact(transform(coalesce(w.funders, array()), f -> f.id)) as funder_ids,\n          w.fulltext IS NOT NULL as has_fulltext,\n          w.has_abstract,\n          w.primary_location.license_id as license_id\n        FROM openalex.vector_search.work_embeddings_v2 e\n        JOIN openalex.works.openalex_works w ON e.work_id = CAST(w.id AS STRING)\n        \"\"\"\n    else:\n        SQL_QUERY = \"\"\"\n        SELECT\n          concat('https://openalex.org/W', e.work_id) as id,\n          e.embedding,\n          w.publication_year,\n          lower(w.type) as type,\n          w.open_access.is_oa as is_oa,\n          lower(w.language) as language,\n          array_compact(transform(w.authorships, a -> a.author.id)) as author_ids,\n          array_distinct(array_compact(flatten(\n            transform(w.authorships, a -> transform(a.institutions, i -> i.id))\n          ))) as institution_ids,\n          array_distinct(array_compact(flatten(\n            transform(w.authorships, a -> transform(a.institutions, i -> lower(i.country_code)))\n          ))) as country_codes,\n          w.is_retracted,\n          w.primary_location.source.id as source_id,\n          w.cited_by_count,\n          array_compact(transform(coalesce(w.funders, array()), f -> f.id)) as funder_ids,\n          w.fulltext IS NOT NULL as has_fulltext,\n          w.has_abstract,\n          w.primary_location.license_id as license_id\n        FROM openalex.vector_search.work_embeddings_v2 e\n        JOIN openalex.works.openalex_works w ON e.work_id = CAST(w.id AS STRING)\n        WHERE w.updated_date >= current_date() - INTERVAL 2 days\n        \"\"\"\n\n    print(f\"Running query and writing staging table ({NUM_BATCHES} batches)...\")\n    staging_start = time.time()\n\n    df = spark.sql(SQL_QUERY)\n    df = df.withColumn(\"batch_id\", F.abs(F.hash(\"id\")) % F.lit(NUM_BATCHES))\n\n    if staging_exists:\n        spark.sql(f\"DROP TABLE {STAGING_TABLE}\")\n    df.write.format(\"delta\").partitionBy(\"batch_id\").saveAsTable(STAGING_TABLE)\n\n    total_staged = spark.sql(f\"SELECT COUNT(*) FROM {STAGING_TABLE}\").collect()[0][0]\n    staging_min = (time.time() - staging_start) / 60\n    print(f\"Staging complete: {total_staged:,} records in {staging_min:.1f}min\")\n\nremaining = NUM_BATCHES - len(completed_batches)\nprint(f\"\\nReady: {remaining} batches to process\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Load Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_schema = StructType([\n",
    "    StructField(\"partition_id\", IntegerType(), True),\n",
    "    StructField(\"indexed_count\", IntegerType(), True),\n",
    "    StructField(\"skipped_count\", IntegerType(), True),\n",
    "    StructField(\"error_count\", IntegerType(), True),\n",
    "    StructField(\"errors\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "def generate_actions(partition, errors_list):\n",
    "    \"\"\"Generate ES bulk index actions for the vector index.\"\"\"\n",
    "    for row in partition:\n",
    "        try:\n",
    "            embedding = [float(x) for x in row.embedding]\n",
    "\n",
    "            doc = {\n",
    "                \"id\": row.id,\n",
    "                \"vector_embedding\": embedding,\n",
    "                \"publication_year\": row.publication_year,\n",
    "                \"type\": row.type,\n",
    "                \"is_oa\": row.is_oa if row.is_oa is not None else False,\n",
    "                \"language\": row.language,\n",
    "                \"author_ids\": list(row.author_ids) if row.author_ids else [],\n",
    "                \"institution_ids\": list(row.institution_ids) if row.institution_ids else [],\n",
    "                \"country_codes\": list(row.country_codes) if row.country_codes else [],\n",
    "                \"is_retracted\": row.is_retracted if row.is_retracted is not None else False,\n",
    "                \"source_id\": row.source_id,\n",
    "                \"cited_by_count\": row.cited_by_count or 0,\n",
    "                \"funder_ids\": list(row.funder_ids) if row.funder_ids else [],\n",
    "                \"has_fulltext\": row.has_fulltext if row.has_fulltext is not None else False,\n",
    "                \"has_abstract\": row.has_abstract if row.has_abstract is not None else False,\n",
    "                \"license_id\": row.license_id,\n",
    "            }\n",
    "\n",
    "            yield {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": ELASTIC_INDEX,\n",
    "                \"_id\": row.id,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "        except Exception as e:\n",
    "            errors_list.append(f\"Parse error for {getattr(row, 'id', '?')}: {str(e)[:200]}\")\n",
    "\n",
    "\n",
    "def send_partition_to_elastic(partition, partition_id):\n",
    "    \"\"\"Send a partition of docs to Elasticsearch.\"\"\"\n",
    "    client = Elasticsearch(\n",
    "        hosts=[ELASTIC_URL],\n",
    "        request_timeout=180,\n",
    "        max_retries=5,\n",
    "        retry_on_timeout=True,\n",
    "        http_compress=True,\n",
    "    )\n",
    "\n",
    "    indexed_count = 0\n",
    "    skipped_count = 0\n",
    "    errors = []\n",
    "\n",
    "    try:\n",
    "        for success, info in helpers.parallel_bulk(\n",
    "            client,\n",
    "            generate_actions(partition, errors),\n",
    "            chunk_size=500,\n",
    "            thread_count=4,\n",
    "            queue_size=10,\n",
    "            raise_on_error=False\n",
    "        ):\n",
    "            if success:\n",
    "                indexed_count += 1\n",
    "            else:\n",
    "                error_info = info.get(\"index\", {})\n",
    "                status = error_info.get(\"status\", 0)\n",
    "                if status == 409:\n",
    "                    skipped_count += 1\n",
    "                else:\n",
    "                    if len(errors) < 10:\n",
    "                        errors.append(str(info)[:500])\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Bulk error: {str(e)[:500]}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "    yield {\n",
    "        \"partition_id\": partition_id,\n",
    "        \"indexed_count\": indexed_count,\n",
    "        \"skipped_count\": skipped_count,\n",
    "        \"error_count\": len(errors),\n",
    "        \"errors\": errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Bulk Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process batches with checkpointing + progress\nbatches_todo = sorted(b for b in range(NUM_BATCHES) if b not in completed_batches)\n\nif not batches_todo:\n    print(\"All batches already completed! Nothing to do.\")\nelse:\n    # Prior progress from checkpoint\n    prior_stats = spark.sql(f\"\"\"\n        SELECT COALESCE(SUM(indexed_count), 0) as indexed,\n               COALESCE(SUM(error_count), 0) as errors\n        FROM {CHECKPOINT_TABLE}\n    \"\"\").collect()[0]\n    prior_indexed = prior_stats.indexed\n\n    print(f\"=== Starting bulk load to {ELASTIC_INDEX} ===\")\n    print(f\"Batches: {len(batches_todo)} remaining of {NUM_BATCHES}\")\n    if prior_indexed > 0:\n        print(f\"Previously indexed: {prior_indexed:,}\")\n    print()\n\n    start_time = time.time()\n    session_indexed = 0\n    session_errors = 0\n\n    for i, batch_id in enumerate(batches_todo):\n        batch_start = time.time()\n\n        # Read this batch from staging table (fast — partitioned by batch_id)\n        batch_df = spark.read.table(STAGING_TABLE).filter(F.col(\"batch_id\") == batch_id)\n\n        # Send to Elasticsearch\n        batch_rdd = batch_df.rdd.mapPartitionsWithIndex(\n            lambda idx, part: send_partition_to_elastic(part, idx)\n        )\n        batch_logs = spark.createDataFrame(batch_rdd, log_schema)\n\n        # Aggregate batch results\n        stats = batch_logs.agg(\n            F.sum(\"indexed_count\").alias(\"indexed\"),\n            F.sum(\"skipped_count\").alias(\"skipped\"),\n            F.sum(\"error_count\").alias(\"errors\")\n        ).collect()[0]\n\n        batch_indexed = stats.indexed or 0\n        batch_skipped = stats.skipped or 0\n        batch_errors = stats.errors or 0\n\n        # Checkpoint — survives cluster restarts\n        spark.sql(f\"\"\"\n            INSERT INTO {CHECKPOINT_TABLE}\n            VALUES ({batch_id}, {batch_indexed}, {batch_skipped}, {batch_errors}, current_timestamp())\n        \"\"\")\n\n        # Update session totals\n        session_indexed += batch_indexed\n        session_errors += batch_errors\n        total_indexed = prior_indexed + session_indexed\n\n        # Rate and ETA\n        elapsed = time.time() - start_time\n        batch_time = time.time() - batch_start\n        rate = session_indexed / elapsed if elapsed > 0 else 0\n        batches_done = i + 1\n        batches_left = len(batches_todo) - batches_done\n        avg_batch_time = elapsed / batches_done\n        eta_min = (batches_left * avg_batch_time) / 60\n\n        print(\n            f\"[{batches_done}/{len(batches_todo)}] \"\n            f\"batch {batch_id}: +{batch_indexed:,} ({batch_time:.0f}s) | \"\n            f\"Total: {total_indexed:,} | \"\n            f\"{rate:,.0f}/s | \"\n            f\"ETA: {eta_min:.0f}min\"\n        )\n\n        # Show sample errors if any\n        if batch_errors > 0:\n            error_rows = batch_logs.filter(F.size(\"errors\") > 0).select(\"errors\").limit(3).collect()\n            for row in error_rows:\n                for err in row.errors[:1]:\n                    print(f\"  ERROR: {err[:300]}\")\n\n    total_elapsed = (time.time() - start_time) / 60\n    print(f\"\\n=== Session Complete ===\")\n    print(f\"Session: {session_indexed:,} indexed, {session_errors:,} errors in {total_elapsed:.1f}min\")\n    print(f\"All sessions: {prior_indexed + session_indexed:,} total indexed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary from checkpoint table\nsummary = spark.sql(f\"\"\"\n    SELECT\n        COUNT(*) as batches_done,\n        COALESCE(SUM(indexed_count), 0) as total_indexed,\n        COALESCE(SUM(skipped_count), 0) as total_skipped,\n        COALESCE(SUM(error_count), 0) as total_errors,\n        MIN(completed_at) as first_batch_at,\n        MAX(completed_at) as last_batch_at\n    FROM {CHECKPOINT_TABLE}\n\"\"\").collect()[0]\n\nprint(f\"=== Sync Summary (from checkpoint) ===\")\nprint(f\"Batches completed: {summary.batches_done}/{NUM_BATCHES}\")\nprint(f\"Total indexed: {summary.total_indexed:,}\")\nprint(f\"Total skipped: {summary.total_skipped:,}\")\nprint(f\"Total errors:  {summary.total_errors:,}\")\nif summary.first_batch_at and summary.last_batch_at:\n    print(f\"Time span: {summary.first_batch_at} → {summary.last_batch_at}\")\n\nif summary.batches_done < NUM_BATCHES:\n    missing = NUM_BATCHES - summary.batches_done\n    print(f\"\\nWARNING: {missing} batches not yet completed. Re-run cell 8 to resume.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Post-sync: refresh, set refresh_interval, verify doc count, clean up\n# NOTE: Do NOT force merge here — run graduated merge manually (see PLAN.md)\n# NOTE: Do NOT set replicas=1 — keep at 0 permanently (9.3TB, rebuildable in ~4h)\ntry:\n    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n\n    if client.indices.exists(index=ELASTIC_INDEX):\n        # Refresh\n        client.indices.refresh(index=ELASTIC_INDEX)\n        print(f\"Refreshed index {ELASTIC_INDEX}\")\n\n        # Doc count\n        total_docs = client.count(index=ELASTIC_INDEX)['count']\n        print(f\"Total documents: {total_docs:,}\")\n\n        # Set refresh interval (but keep replicas=0)\n        if IS_FULL_SYNC:\n            client.indices.put_settings(index=ELASTIC_INDEX, body={\n                \"index\": {\n                    \"refresh_interval\": \"30s\"\n                }\n            })\n            print(f\"Set refresh_interval=30s on {ELASTIC_INDEX} (replicas stay at 0)\")\n    else:\n        print(f\"Index {ELASTIC_INDEX} does not exist\")\nfinally:\n    client.close()\n\n# Clean up staging and checkpoint tables\nprint(f\"\\nCleaning up temp tables...\")\nspark.sql(f\"DROP TABLE IF EXISTS {STAGING_TABLE}\")\nspark.sql(f\"DROP TABLE IF EXISTS {CHECKPOINT_TABLE}\")\nprint(\"Staging and checkpoint tables dropped.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kNN search with a sample query\n",
    "print(\"Testing kNN search...\")\n",
    "\n",
    "try:\n",
    "    client = Elasticsearch(hosts=[ELASTIC_URL], request_timeout=180)\n",
    "\n",
    "    # Get a sample embedding to use as query\n",
    "    sample = spark.sql(\"\"\"\n",
    "        SELECT work_id, embedding\n",
    "        FROM openalex.vector_search.work_embeddings_v2\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()[0]\n",
    "\n",
    "    query_vector = [float(x) for x in sample.embedding]\n",
    "\n",
    "    # Run kNN search on the vector index\n",
    "    result = client.search(\n",
    "        index=ELASTIC_INDEX,\n",
    "        body={\n",
    "            \"knn\": {\n",
    "                \"field\": \"vector_embedding\",\n",
    "                \"query_vector\": query_vector,\n",
    "                \"k\": 5,\n",
    "                \"num_candidates\": 50\n",
    "            },\n",
    "            \"_source\": [\"id\", \"publication_year\", \"type\", \"cited_by_count\"]\n",
    "        },\n",
    "        size=5\n",
    "    )\n",
    "\n",
    "    print(f\"Query work_id: {sample.work_id}\")\n",
    "    print(f\"\\nTop 5 similar works:\")\n",
    "    for hit in result['hits']['hits']:\n",
    "        src = hit['_source']\n",
    "        print(f\"  {hit['_score']:.4f}: {src.get('id', 'N/A')} ({src.get('type', '?')}, {src.get('publication_year', '?')}, cited: {src.get('cited_by_count', 0)})\")\n",
    "\n",
    "    # Test with a filter\n",
    "    filtered_result = client.search(\n",
    "        index=ELASTIC_INDEX,\n",
    "        body={\n",
    "            \"knn\": {\n",
    "                \"field\": \"vector_embedding\",\n",
    "                \"query_vector\": query_vector,\n",
    "                \"k\": 5,\n",
    "                \"num_candidates\": 50,\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\"term\": {\"is_oa\": True}},\n",
    "                            {\"range\": {\"publication_year\": {\"gte\": 2020}}}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"id\", \"publication_year\", \"is_oa\"]\n",
    "        },\n",
    "        size=5\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFiltered (is_oa=true, year>=2020):\")\n",
    "    for hit in filtered_result['hits']['hits']:\n",
    "        src = hit['_source']\n",
    "        print(f\"  {hit['_score']:.4f}: {src.get('id', 'N/A')} ({src.get('publication_year', '?')}, is_oa={src.get('is_oa', '?')})\")\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}