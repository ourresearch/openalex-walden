-- Databricks notebook source
-- MAGIC %python
-- MAGIC %pip install elasticsearch==8.19.0

-- COMMAND ----------

-- %sql
-- -- PREPARE authors_api table
-- CREATE OR REPLACE TABLE openalex.authors.authors_api AS
-- SELECT 
--     id as original_id,
--     CONCAT('https://openalex.org/A', id) as id,
--     display_name,
--     created_date,
--     updated_date
-- FROM openalex.authors.authors_from_postgres

-- COMMAND ----------

-- MAGIC %python
-- MAGIC import uuid
-- MAGIC from datetime import datetime
-- MAGIC from pyspark.sql import functions as F
-- MAGIC from elasticsearch import Elasticsearch, helpers
-- MAGIC import logging
-- MAGIC import json
-- MAGIC
-- MAGIC logging.basicConfig(level=logging.WARNING, format='[%(asctime)s]: %(message)s')
-- MAGIC log = logging.getLogger(__name__)
-- MAGIC
-- MAGIC ELASTIC_URL = dbutils.secrets.get(scope="elastic", key="elastic_url")
-- MAGIC
-- MAGIC CONFIG = {
-- MAGIC     "table_name": "openalex.authors.authors_api",
-- MAGIC     "index_name": "authors-v16"
-- MAGIC }
-- MAGIC
-- MAGIC def send_partition_to_elastic(partition, index_name):
-- MAGIC     client = Elasticsearch(
-- MAGIC         hosts=[ELASTIC_URL],
-- MAGIC         max_retries=3,
-- MAGIC     )
-- MAGIC     
-- MAGIC     def generate_actions():
-- MAGIC         for row in partition:
-- MAGIC             doc = row.asDict()
-- MAGIC             
-- MAGIC             for key, value in doc.items():
-- MAGIC                 if isinstance(value, str) and value.startswith('[') and value.endswith(']'):
-- MAGIC                     try:
-- MAGIC                         parsed_value = json.loads(value)
-- MAGIC                         if isinstance(parsed_value, list):
-- MAGIC                             doc[key] = parsed_value
-- MAGIC                     except (json.JSONDecodeError, ValueError):
-- MAGIC                         pass
-- MAGIC                 elif hasattr(value, 'asDict'):
-- MAGIC                     doc[key] = value.asDict()
-- MAGIC                 elif isinstance(value, list) and len(value) > 0 and hasattr(value[0], 'asDict'):
-- MAGIC                     doc[key] = [item.asDict() if hasattr(item, 'asDict') else item for item in value]
-- MAGIC             
-- MAGIC             entity_id = doc['id']
-- MAGIC             
-- MAGIC             yield {
-- MAGIC                 "_index": index_name,
-- MAGIC                 "_id": entity_id,
-- MAGIC                 "_source": doc
-- MAGIC             }
-- MAGIC     
-- MAGIC     try:
-- MAGIC         count = 0
-- MAGIC         for success, info in helpers.parallel_bulk(
-- MAGIC             client, 
-- MAGIC             generate_actions(), 
-- MAGIC             chunk_size=500,
-- MAGIC             thread_count=4
-- MAGIC         ):
-- MAGIC             count += 1
-- MAGIC             if not success:
-- MAGIC                 print(f"FAILED TO INDEX: {info}")
-- MAGIC                 raise Exception(f"Failed to index document: {info}")
-- MAGIC             elif count % 1000 == 0:
-- MAGIC                 msg = f"Indexed {count} documents to {index_name}..."
-- MAGIC                 log.warning(msg)
-- MAGIC                 print(msg)
-- MAGIC         
-- MAGIC         print(f"Successfully indexed {count} total documents to {index_name}")
-- MAGIC         
-- MAGIC     except Exception as e:
-- MAGIC         log.error(f"Error indexing documents to {index_name}: {e}", stack_info=True, exc_info=True)
-- MAGIC         print(f"Error indexing documents to {index_name}: {e}")
-- MAGIC
-- MAGIC print(f"\n=== Processing {CONFIG['table_name']} ===")
-- MAGIC
-- MAGIC try:
-- MAGIC     df = spark.read.table(CONFIG['table_name'])
-- MAGIC     
-- MAGIC     def send_partition_wrapper(partition):
-- MAGIC         return send_partition_to_elastic(
-- MAGIC             partition, 
-- MAGIC             CONFIG['index_name']
-- MAGIC         )
-- MAGIC     
-- MAGIC     df.foreachPartition(send_partition_wrapper)
-- MAGIC     
-- MAGIC     print(f"Completed indexing {CONFIG['table_name']} to {CONFIG['index_name']}")
-- MAGIC     
-- MAGIC except Exception as e:
-- MAGIC     print(f"Failed to process {CONFIG['table_name']}: {e}")
-- MAGIC     log.error(f"Failed to process {CONFIG['table_name']}: {e}", stack_info=True, exc_info=True)
-- MAGIC
-- MAGIC print("\nIndexing operation completed!")
