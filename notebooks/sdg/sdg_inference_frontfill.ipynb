{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizerFast\n",
        "import os\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SRC_SAVED_MODEL = \"/Volumes/openalex/works/models/sdg/saved_model\"\n",
        "TOKENIZER_PATH = \"/Volumes/openalex/works/models/sdg/tokenizer\"\n",
        "\n",
        "goal_names = {\n",
        "    \"Goal 1\": \"No poverty\",\n",
        "    \"Goal 2\": \"Zero hunger\",\n",
        "    \"Goal 3\": \"Good health and well-being\",\n",
        "    \"Goal 4\": \"Quality Education\",\n",
        "    \"Goal 5\": \"Gender equality\",\n",
        "    \"Goal 6\": \"Clean water and sanitation\",\n",
        "    \"Goal 7\": \"Affordable and clean energy\",\n",
        "    \"Goal 8\": \"Decent work and economic growth\",\n",
        "    \"Goal 9\": \"Industry, innovation and infrastructure\",\n",
        "    \"Goal 10\": \"Reduced inequalities\",\n",
        "    \"Goal 11\": \"Sustainable cities and communities\",\n",
        "    \"Goal 12\": \"Responsible consumption and production\",\n",
        "    \"Goal 13\": \"Climate action\",\n",
        "    \"Goal 14\": \"Life below water\",\n",
        "    \"Goal 15\": \"Life in Land\",\n",
        "    \"Goal 16\": \"Peace, Justice and strong institutions\",\n",
        "    \"Goal 17\": \"Partnerships for the goals\"\n",
        "}\n",
        "\n",
        "class ModelCache:\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    predict_fn = None\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls):\n",
        "        if cls.model is not None:\n",
        "            return\n",
        "\n",
        "        cls.model = tf.saved_model.load(SRC_SAVED_MODEL)\n",
        "        cls.tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
        "        cls.predict_fn = cls.model.signatures['serving_default']\n",
        "\n",
        "    @classmethod\n",
        "    def predict(cls, text):\n",
        "        \"\"\"Predict SDG scores for arbitrary text string\"\"\"\n",
        "        if cls.model is None:\n",
        "            cls.load()\n",
        "        \n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Tokenize\n",
        "            enc = cls.tokenizer(\n",
        "                text.lower(),\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"tf\"\n",
        "            )\n",
        "            \n",
        "            # Call SavedModel\n",
        "            out = cls.predict_fn(\n",
        "                input_ids=enc[\"input_ids\"],\n",
        "                attention_masks=enc[\"attention_mask\"]\n",
        "            )\n",
        "            \n",
        "            logits = out[\"target_layer\"].numpy()[0]  # float32 [17]\n",
        "            \n",
        "            # Build SDG array with id, display_name, score\n",
        "            sdg_results = []\n",
        "            for idx, score in enumerate(logits):\n",
        "                sdg_number = idx + 1\n",
        "                sdg_label = f\"Goal {sdg_number}\"\n",
        "                \n",
        "                sdg_results.append({\n",
        "                    \"id\": f\"https://metadata.un.org/sdg/{sdg_number}\",\n",
        "                    \"display_name\": goal_names[sdg_label],\n",
        "                    \"score\": float(score)\n",
        "                })\n",
        "            \n",
        "            # Sort by score descending\n",
        "            sdg_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            \n",
        "            return sdg_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting SDG for text: {e}\")\n",
        "            return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create tables (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load input data and cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = (spark.table(\"openalex.works.works_sdg_frontfill_input\")\n",
        "      .select(\"work_id\", \"title\", \"abstract\")\n",
        "      .limit(100000)\n",
        "      .repartition(2048)\n",
        ")\n",
        "\n",
        "print(f\"Input Row Count: {df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "def process_partition(rows):\n",
        "    \"\"\"\n",
        "    Process a partition using mapPartitions\n",
        "    Returns tuples to avoid driver memory pressure\n",
        "    \"\"\"\n",
        "    ModelCache.load()\n",
        "    \n",
        "    batch_rows = []\n",
        "    batch_texts = []\n",
        "    \n",
        "    def yield_batch(rows_batch, texts_batch):\n",
        "        try:\n",
        "            # Process each text in batch using ModelCache.predict\n",
        "            for row, text in zip(rows_batch, texts_batch):\n",
        "                sdg_results = ModelCache.predict(text)\n",
        "                yield (row.work_id, sdg_results)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch: {e}\")\n",
        "            # Yield empty results for failed batch\n",
        "            for row in rows_batch:\n",
        "                yield (row.work_id, [])\n",
        "    \n",
        "    for row in rows:\n",
        "        if row is None:\n",
        "            continue\n",
        "        \n",
        "        # Combine title and abstract for prediction\n",
        "        title = (row.title or \"\").strip()\n",
        "        abstract = (row.abstract or \"\").strip()\n",
        "        combined_text = f\"{title}\\n{abstract}\"\n",
        "        \n",
        "        batch_rows.append(row)\n",
        "        batch_texts.append(combined_text)\n",
        "        \n",
        "        if len(batch_texts) >= BATCH_SIZE:\n",
        "            yield from yield_batch(batch_rows, batch_texts)\n",
        "            batch_rows = []\n",
        "            batch_texts = []\n",
        "    \n",
        "    # Process remaining rows\n",
        "    if batch_texts:\n",
        "        yield from yield_batch(batch_rows, batch_texts)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Define schema upfront\n",
        "sdg_struct = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"display_name\", StringType(), True),\n",
        "    StructField(\"score\", FloatType(), True)\n",
        "])\n",
        "\n",
        "output_schema = StructType([\n",
        "    StructField(\"work_id\", StringType(), nullable=False),\n",
        "    StructField(\"sdg_array\", ArrayType(sdg_struct), nullable=True)\n",
        "])\n",
        "\n",
        "# Use mapPartitions - creates RDD, converts to DataFrame, writes ONCE\n",
        "res_rdd = df.select(\"work_id\", \"title\", \"abstract\").rdd.mapPartitions(process_partition)\n",
        "inferred_sdg_df = spark.createDataFrame(res_rdd, output_schema).cache()\n",
        "\n",
        "output_count = inferred_sdg_df.count()\n",
        "print(f\"Output Row count: {output_count}\")\n",
        "\n",
        "runtime = time.time() - start_time\n",
        "print(f\"Total runtime: {runtime:.4f} seconds\")\n",
        "print(f\"Total throughput: {output_count / runtime:.4f} inferences/sec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write to works_sdg_frontfill table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "final_df = (inferred_sdg_df\n",
        "    .withColumn(\"created_timestamp\", current_timestamp())\n",
        "    .select(\"work_id\", \"sdg_array\", \"created_timestamp\"))\n",
        "\n",
        "# Append to table\n",
        "final_df.write.format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .saveAsTable(\"openalex.works.works_sdg_frontfill\")\n",
        "\n",
        "# Register as temp view for cleanup\n",
        "final_df.createOrReplaceTempView(\"res_df_temp\")\n",
        "\n",
        "# Delete from input table using the work_ids in res_df\n",
        "spark.sql(\"\"\"\n",
        "    DELETE FROM openalex.works.works_sdg_frontfill_input \n",
        "    WHERE work_id IN (SELECT work_id FROM res_df_temp)\n",
        "\"\"\")\n",
        "print(f\"Removed processed work_ids from works_sdg_frontfill_input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify table structure and sample results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Show sample results\n",
        "SELECT \n",
        "    work_id,\n",
        "    size(sdg_array) AS num_sdgs,\n",
        "    slice(sdg_array, 1, 3) AS top_3_sdgs,\n",
        "    created_timestamp\n",
        "FROM openalex.works.works_sdg_frontfill\n",
        "LIMIT 10\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
