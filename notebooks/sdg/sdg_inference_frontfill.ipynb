{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load SDG Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "SRC_SAVED_MODEL = \"/Volumes/openalex/works/models/sdg/saved_model\"\n",
        "\n",
        "model = tf.saved_model.load(SRC_SAVED_MODEL)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"/Volumes/openalex/works/models/sdg/tokenizer\")\n",
        "predict = model.signatures['serving_default']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load input data from openalex_works\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        w.id AS work_id,\n",
        "        w.title,\n",
        "        w.abstract\n",
        "    FROM openalex.works.openalex_works w\n",
        "    LEFT ANTI JOIN openalex.works.works_sdg_frontfill wsf\n",
        "        ON w.id = wsf.work_id\n",
        "    WHERE (w.sustainable_development_goals IS NULL OR size(w.sustainable_development_goals) = 0)\n",
        "        AND w.title IS NOT NULL\n",
        "        AND w.abstract IS NOT NULL\n",
        "        AND length(w.title) > 20\n",
        "        AND length(w.abstract) > 50\n",
        "\"\"\").repartition(4096).cache()\n",
        "\n",
        "print(f\"Total number of rows to process: {df.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run SDG inference using mapPartitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SRC_SAVED_MODEL = \"/Volumes/openalex/works/models/sdg/saved_model\"\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"work_id\", StringType(), nullable=False),\n",
        "    StructField(\"sdg_array\", ArrayType(\n",
        "        StructType([\n",
        "            StructField(\"id\", StringType(), nullable=False),\n",
        "            StructField(\"display_name\", StringType(), nullable=False),\n",
        "            StructField(\"score\", FloatType(), nullable=False),\n",
        "        ])\n",
        "    ), nullable=True)\n",
        "])\n",
        "\n",
        "goal_names = {\n",
        "    \"Goal 1\": \"No poverty\",\n",
        "    \"Goal 2\": \"Zero hunger\",\n",
        "    \"Goal 3\": \"Good health and well-being\",\n",
        "    \"Goal 4\": \"Quality Education\",\n",
        "    \"Goal 5\": \"Gender equality\",\n",
        "    \"Goal 6\": \"Clean water and sanitation\",\n",
        "    \"Goal 7\": \"Affordable and clean energy\",\n",
        "    \"Goal 8\": \"Decent work and economic growth\",\n",
        "    \"Goal 9\": \"Industry, innovation and infrastructure\",\n",
        "    \"Goal 10\": \"Reduced inequalities\",\n",
        "    \"Goal 11\": \"Sustainable cities and communities\",\n",
        "    \"Goal 12\": \"Responsible consumption and production\",\n",
        "    \"Goal 13\": \"Climate action\",\n",
        "    \"Goal 14\": \"Life below water\",\n",
        "    \"Goal 15\": \"Life in Land\",\n",
        "    \"Goal 16\": \"Peace, Justice and strong institutions\",\n",
        "    \"Goal 17\": \"Partnerships for the goals\"\n",
        "}\n",
        "\n",
        "def process_batch(batch, predict_fn, tokenizer_local):\n",
        "    \"\"\"Process a batch of rows\"\"\"\n",
        "    work_ids = []\n",
        "    texts = []\n",
        "    \n",
        "    for row in batch:\n",
        "        work_ids.append(row.work_id)\n",
        "        # Combine title and abstract\n",
        "        title = row.title or \"\"\n",
        "        abstract = row.abstract or \"\"\n",
        "        combined_text = f\"{title.strip()}\\n{abstract.strip()}\"\n",
        "        texts.append(combined_text)\n",
        "    \n",
        "    # Process each text\n",
        "    for work_id, text in zip(work_ids, texts):\n",
        "        if not text or not text.strip():\n",
        "            yield (work_id, [])\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # Tokenize\n",
        "            enc = tokenizer_local(\n",
        "                text.lower(),\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"tf\"\n",
        "            )\n",
        "            \n",
        "            # Call SavedModel\n",
        "            out = predict_fn(\n",
        "                input_ids=enc[\"input_ids\"],\n",
        "                attention_masks=enc[\"attention_mask\"]\n",
        "            )\n",
        "            \n",
        "            logits = out[\"target_layer\"].numpy()[0]  # float32 [17]\n",
        "            \n",
        "            # Build SDG array with id, display_name, score\n",
        "            sdg_results = []\n",
        "            for idx, score in enumerate(logits):\n",
        "                sdg_number = idx + 1\n",
        "                sdg_label = f\"Goal {sdg_number}\"\n",
        "                \n",
        "                sdg_results.append({\n",
        "                    \"id\": f\"https://metadata.un.org/sdg/{sdg_number}\",\n",
        "                    \"display_name\": goal_names[sdg_label],\n",
        "                    \"score\": float(score)\n",
        "                })\n",
        "            \n",
        "            # Sort by score descending\n",
        "            sdg_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            \n",
        "            yield (work_id, sdg_results)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing work_id {work_id}: {e}\")\n",
        "            yield (work_id, [])\n",
        "\n",
        "def process_partition(rows_iter, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Process partition - load model once per partition\"\"\"\n",
        "    # Load model ONCE per partition\n",
        "    model_local = tf.saved_model.load(SRC_SAVED_MODEL)\n",
        "    predict_local = model_local.signatures['serving_default']\n",
        "    tokenizer_local = BertTokenizerFast.from_pretrained(\"/Volumes/openalex/works/models/sdg/tokenizer\")\n",
        "    \n",
        "    batch = []\n",
        "    for row in rows_iter:\n",
        "        batch.append(row)\n",
        "        if len(batch) >= batch_size:\n",
        "            yield from process_batch(batch, predict_local, tokenizer_local)\n",
        "            batch = []\n",
        "    \n",
        "    if batch:\n",
        "        yield from process_batch(batch, predict_local, tokenizer_local)\n",
        "\n",
        "# Apply with mapPartitions\n",
        "result_rdd = df.rdd.mapPartitions(process_partition)\n",
        "\n",
        "# Convert to DataFrame\n",
        "inferred_sdg_df = spark.createDataFrame(result_rdd, schema).cache()\n",
        "\n",
        "print(f\"Total number of rows processed: {inferred_sdg_df.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write to works_sdg_frontfill table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "final_df = (inferred_sdg_df\n",
        "    .withColumn(\"created_timestamp\", current_timestamp())\n",
        "    .select(\"work_id\", \"sdg\", \"created_timestamp\")\n",
        "    .write.format(\"delta\")\n",
        "    .mode(\"append\")\n",
        "    .option(\"mergeSchema\", \"true\")\n",
        "    .saveAsTable(\"openalex.works.works_sdg_frontfill\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify table structure and sample results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Show sample results\n",
        "SELECT \n",
        "    work_id,\n",
        "    size(sdg_array) AS num_sdgs,\n",
        "    slice(sdg_array, 1, 3) AS top_3_sdgs,\n",
        "    created_timestamp\n",
        "FROM openalex.works.works_sdg_frontfill\n",
        "LIMIT 10\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
