{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6daaf7d-5015-4602-b248-2b52d4b60e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b641edba-ab1d-48b6-bb00-64b0483103ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SRC_SAVED_MODEL = \"/Volumes/openalex/works/models/sdg/saved_model\"\n",
    "TOKENIZER_PATH = \"/Volumes/openalex/works/models/sdg/tokenizer\"\n",
    "\n",
    "goal_names = {\n",
    "    \"Goal 1\": \"No poverty\",\n",
    "    \"Goal 2\": \"Zero hunger\",\n",
    "    \"Goal 3\": \"Good health and well-being\",\n",
    "    \"Goal 4\": \"Quality Education\",\n",
    "    \"Goal 5\": \"Gender equality\",\n",
    "    \"Goal 6\": \"Clean water and sanitation\",\n",
    "    \"Goal 7\": \"Affordable and clean energy\",\n",
    "    \"Goal 8\": \"Decent work and economic growth\",\n",
    "    \"Goal 9\": \"Industry, innovation and infrastructure\",\n",
    "    \"Goal 10\": \"Reduced inequalities\",\n",
    "    \"Goal 11\": \"Sustainable cities and communities\",\n",
    "    \"Goal 12\": \"Responsible consumption and production\",\n",
    "    \"Goal 13\": \"Climate action\",\n",
    "    \"Goal 14\": \"Life below water\",\n",
    "    \"Goal 15\": \"Life in Land\",\n",
    "    \"Goal 16\": \"Peace, Justice and strong institutions\",\n",
    "    \"Goal 17\": \"Partnerships for the goals\"\n",
    "}\n",
    "\n",
    "class ModelCache:\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    predict_fn = None\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        if cls.model is not None:\n",
    "            return\n",
    "\n",
    "        cls.model = tf.saved_model.load(SRC_SAVED_MODEL)\n",
    "        cls.tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "        cls.predict_fn = cls.model.signatures['serving_default']\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, text):\n",
    "        \"\"\"Predict SDG scores for arbitrary text string\"\"\"\n",
    "        if cls.model is None:\n",
    "            cls.load()\n",
    "        \n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Tokenize\n",
    "            enc = cls.tokenizer(\n",
    "                text.lower(),\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"tf\"\n",
    "            )\n",
    "            \n",
    "            # Call SavedModel\n",
    "            out = cls.predict_fn(\n",
    "                input_ids=enc[\"input_ids\"],\n",
    "                attention_masks=enc[\"attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            logits = out[\"target_layer\"].numpy()[0]  # float32 [17]\n",
    "            \n",
    "            # Build SDG array with id, display_name, score\n",
    "            sdg_results = []\n",
    "            for idx, score in enumerate(logits):\n",
    "                sdg_number = idx + 1\n",
    "                sdg_label = f\"Goal {sdg_number}\"\n",
    "                \n",
    "                sdg_results.append({\n",
    "                    \"id\": f\"https://metadata.un.org/sdg/{sdg_number}\",\n",
    "                    \"display_name\": goal_names[sdg_label],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "            \n",
    "            # Sort by score descending\n",
    "            sdg_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            \n",
    "            return sdg_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting SDG for text: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd7e93bb-4938-4b95-bc8e-11e1acbda2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create tables (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d73308-401c-4304-ba43-8f72d6cd50ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load input data and cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54687be-c43f-4823-9e77-ad4a1c8e9d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.table(\"openalex.works.works_sdg_frontfill_input\")\n",
    "      .select(\"work_id\", \"title\", \"abstract\")\n",
    "      .limit(2048000)\n",
    "      .repartition(512)\n",
    ")\n",
    "\n",
    "print(f\"Input Row Count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b2004f-af65-43f9-acec-db5149e394ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42808a29-0abd-47fd-9e3f-307bc06cfaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "def process_partition(rows):\n",
    "    \"\"\"\n",
    "    Process a partition using mapPartitions\n",
    "    Returns tuples to avoid driver memory pressure\n",
    "    \"\"\"\n",
    "    ModelCache.load()\n",
    "    \n",
    "    batch_rows = []\n",
    "    batch_texts = []\n",
    "    \n",
    "    def yield_batch(rows_batch, texts_batch):\n",
    "        try:\n",
    "            # Process each text in batch using ModelCache.predict\n",
    "            for row, text in zip(rows_batch, texts_batch):\n",
    "                sdg_results = ModelCache.predict(text)\n",
    "                yield (row.work_id, sdg_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Yield empty results for failed batch\n",
    "            for row in rows_batch:\n",
    "                yield (row.work_id, [])\n",
    "    \n",
    "    for row in rows:\n",
    "        if row is None:\n",
    "            continue\n",
    "        \n",
    "        # Combine title and abstract for prediction\n",
    "        title = (row.title or \"\").strip()\n",
    "        abstract = (row.abstract or \"\").strip()\n",
    "        combined_text = f\"{title}\\n{abstract}\"\n",
    "        \n",
    "        batch_rows.append(row)\n",
    "        batch_texts.append(combined_text)\n",
    "        \n",
    "        if len(batch_texts) >= BATCH_SIZE:\n",
    "            yield from yield_batch(batch_rows, batch_texts)\n",
    "            batch_rows = []\n",
    "            batch_texts = []\n",
    "    \n",
    "    # Process remaining rows\n",
    "    if batch_texts:\n",
    "        yield from yield_batch(batch_rows, batch_texts)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define schema upfront\n",
    "sdg_struct = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"display_name\", StringType(), True),\n",
    "    StructField(\"score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"work_id\", StringType(), nullable=False),\n",
    "    StructField(\"sdg_array\", ArrayType(sdg_struct), nullable=True)\n",
    "])\n",
    "\n",
    "# Use mapPartitions - creates RDD, converts to DataFrame, writes ONCE\n",
    "res_rdd = df.select(\"work_id\", \"title\", \"abstract\").rdd.mapPartitions(process_partition)\n",
    "inferred_sdg_df = spark.createDataFrame(res_rdd, output_schema).cache()\n",
    "\n",
    "output_count = inferred_sdg_df.count()\n",
    "print(f\"Output Row count: {output_count}\")\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "print(f\"Total runtime: {runtime:.4f} seconds\")\n",
    "print(f\"Total throughput: {output_count / runtime:.4f} inferences/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c66d355-deb8-4a34-98db-e2f25be26c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to works_sdg_frontfill table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9526e736-cc43-49dd-9de9-97e9cbf62753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "final_df = (inferred_sdg_df\n",
    "    .withColumn(\"created_timestamp\", current_timestamp())\n",
    "    .select(\"work_id\", \"sdg\", \"created_timestamp\"))\n",
    "\n",
    "# Append to table\n",
    "final_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"openalex.works.works_sdg_frontfill\")\n",
    "\n",
    "# Register as temp view for cleanup\n",
    "final_df.createOrReplaceTempView(\"res_df_temp\")\n",
    "\n",
    "# Delete from input table using the work_ids in res_df\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM openalex.works.works_sdg_frontfill_input \n",
    "    WHERE work_id IN (SELECT work_id FROM res_df_temp)\n",
    "\"\"\")\n",
    "print(f\"Removed processed work_ids from works_sdg_frontfill_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318d267e-1e39-4db8-8eb2-f244583987e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify table structure and sample results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5413e154-d827-4058-bc03-527855e677e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    work_id,\n",
    "    size(sdg) AS num_sdgs,\n",
    "    slice(sdg, 1, 3) AS top_3_sdgs,\n",
    "    created_timestamp\n",
    "FROM openalex.works.works_sdg_frontfill;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sdg_inference_frontfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
