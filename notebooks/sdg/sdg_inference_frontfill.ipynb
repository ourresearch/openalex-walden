{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6daaf7d-5015-4602-b248-2b52d4b60e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b641edba-ab1d-48b6-bb00-64b0483103ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "\n",
    "# Batch size for TensorFlow inference - tune based on GPU memory\n",
    "# L40S (48GB): Recommended 128-256 for optimal throughput\n",
    "# Start with 64 and increase if memory allows and throughput improves\n",
    "BATCH_SIZE = 64\n",
    "SRC_SAVED_MODEL = \"/Volumes/openalex/works/models/sdg/saved_model\"\n",
    "TOKENIZER_PATH = \"/Volumes/openalex/works/models/sdg/tokenizer\"\n",
    "\n",
    "class ModelCache:\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    predict_fn = None\n",
    "    assigned_device = None\n",
    "    gpu_device = '/cpu:0'  # Default to CPU, will be set during load()\n",
    "\n",
    "    goal_names = {\n",
    "        \"Goal 1\": \"No poverty\",\n",
    "        \"Goal 2\": \"Zero hunger\",\n",
    "        \"Goal 3\": \"Good health and well-being\",\n",
    "        \"Goal 4\": \"Quality Education\",\n",
    "        \"Goal 5\": \"Gender equality\",\n",
    "        \"Goal 6\": \"Clean water and sanitation\",\n",
    "        \"Goal 7\": \"Affordable and clean energy\",\n",
    "        \"Goal 8\": \"Decent work and economic growth\",\n",
    "        \"Goal 9\": \"Industry, innovation and infrastructure\",\n",
    "        \"Goal 10\": \"Reduced inequalities\",\n",
    "        \"Goal 11\": \"Sustainable cities and communities\",\n",
    "        \"Goal 12\": \"Responsible consumption and production\",\n",
    "        \"Goal 13\": \"Climate action\",\n",
    "        \"Goal 14\": \"Life below water\",\n",
    "        \"Goal 15\": \"Life in Land\",\n",
    "        \"Goal 16\": \"Peace, Justice and strong institutions\",\n",
    "        \"Goal 17\": \"Partnerships for the goals\"\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        if cls.model is not None:\n",
    "            return\n",
    "\n",
    "        # Determine available GPU count (TensorFlow native detection)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        num_devices = len(gpus)\n",
    "        \n",
    "        if num_devices == 0:\n",
    "            cls.assigned_device = -1\n",
    "            cls.gpu_device = '/cpu:0'\n",
    "        else:\n",
    "            # Assign device using pid hash (per Databricks docs: GPUs are zero-indexed)\n",
    "            cls.assigned_device = os.getpid() % num_devices\n",
    "            cls.gpu_device = f'/gpu:{cls.assigned_device}'\n",
    "            \n",
    "            # Configure TensorFlow GPU (per Databricks documentation)\n",
    "            tf.config.set_visible_devices(gpus[cls.assigned_device], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[cls.assigned_device], True)\n",
    "\n",
    "        # Load model with explicit device placement (per Databricks recommendation)\n",
    "        with tf.device(cls.gpu_device):\n",
    "            cls.model = tf.saved_model.load(SRC_SAVED_MODEL)\n",
    "        cls.tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "        cls.predict_fn = cls.model.signatures['serving_default']\n",
    "\n",
    "    @classmethod\n",
    "    def predict_batch(cls, texts):\n",
    "        \"\"\"Predict SDG scores for a batch of text strings - much faster than individual calls\"\"\"\n",
    "        if cls.model is None:\n",
    "            cls.load()\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Filter out empty texts\n",
    "        valid_texts = [t.strip().lower() if t and t.strip() else \"\" for t in texts]\n",
    "        if not any(valid_texts):\n",
    "            return [[] for _ in texts]\n",
    "        \n",
    "        try:\n",
    "            # Batch tokenize all texts at once\n",
    "            enc = cls.tokenizer(\n",
    "                valid_texts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"tf\"\n",
    "            )\n",
    "            \n",
    "            # Call SavedModel on batch with explicit device placement\n",
    "            with tf.device(cls.gpu_device):\n",
    "                out = cls.predict_fn(\n",
    "                    input_ids=enc[\"input_ids\"],\n",
    "                    attention_masks=enc[\"attention_mask\"]\n",
    "                )\n",
    "            \n",
    "            logits_batch = out[\"target_layer\"].numpy()  # shape: [batch_size, 17]\n",
    "            \n",
    "            # Process each result in the batch\n",
    "            results = []\n",
    "            score_threshold = 0.1\n",
    "            top_k = 3\n",
    "            \n",
    "            for logits in logits_batch:\n",
    "                # Build SDG array with id, display_name, score\n",
    "                sdg_results = []\n",
    "                for idx, score in enumerate(logits):\n",
    "                    sdg_number = idx + 1\n",
    "                    sdg_label = f\"Goal {sdg_number}\"\n",
    "                    \n",
    "                    sdg_results.append({\n",
    "                        \"id\": f\"https://metadata.un.org/sdg/{sdg_number}\",\n",
    "                        \"display_name\": cls.goal_names[sdg_label],\n",
    "                        \"score\": float(score)\n",
    "                    })\n",
    "                \n",
    "                # Sort by score descending, filter by threshold, take top 3\n",
    "                sdg_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "                filtered = [sdg for sdg in sdg_results if sdg[\"score\"] > score_threshold]\n",
    "                top_results = filtered[:top_k]\n",
    "                results.append(top_results)\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting SDG batch: {e}\")\n",
    "            return [[] for _ in texts]\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, text):\n",
    "        \"\"\"Predict SDG scores for arbitrary text string (single)\"\"\"\n",
    "        results = cls.predict_batch([text])\n",
    "        return results[0] if results else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd7e93bb-4938-4b95-bc8e-11e1acbda2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create tables (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d73308-401c-4304-ba43-8f72d6cd50ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load input data and cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54687be-c43f-4823-9e77-ad4a1c8e9d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.table(\"openalex.works.works_sdg_frontfill_input\")\n",
    "      .select(\"work_id\", \"title\", \"abstract\")\n",
    "      .limit(4096000)\n",
    "      .repartition(1024)\n",
    ")\n",
    "df.cache()\n",
    "\n",
    "print(f\"Input Row Count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b2004f-af65-43f9-acec-db5149e394ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42808a29-0abd-47fd-9e3f-307bc06cfaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "def process_partition(rows):\n",
    "    \"\"\"\n",
    "    Process a partition using mapPartitions\n",
    "    Returns tuples to avoid driver memory pressure\n",
    "    \"\"\"\n",
    "    ModelCache.load()\n",
    "    \n",
    "    batch_rows = []\n",
    "    batch_texts = []\n",
    "    \n",
    "    def yield_batch(rows_batch, texts_batch):\n",
    "        try:\n",
    "            # Process entire batch at once - much faster!\n",
    "            sdg_results_batch = ModelCache.predict_batch(texts_batch)\n",
    "            \n",
    "            # Yield results for each row\n",
    "            for row, sdg_results in zip(rows_batch, sdg_results_batch):\n",
    "                yield (row.work_id, sdg_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Yield empty results for failed batch\n",
    "            for row in rows_batch:\n",
    "                yield (row.work_id, [])\n",
    "    \n",
    "    for row in rows:\n",
    "        if row is None:\n",
    "            continue\n",
    "        \n",
    "        # Combine title and abstract for prediction\n",
    "        title = (row.title or \"\").strip()\n",
    "        abstract = (row.abstract or \"\").strip()\n",
    "        combined_text = f\"{title}\\n{abstract}\"\n",
    "        \n",
    "        batch_rows.append(row)\n",
    "        batch_texts.append(combined_text)\n",
    "        \n",
    "        if len(batch_texts) >= BATCH_SIZE:\n",
    "            yield from yield_batch(batch_rows, batch_texts)\n",
    "            batch_rows = []\n",
    "            batch_texts = []\n",
    "    \n",
    "    # Process remaining rows\n",
    "    if batch_texts:\n",
    "        yield from yield_batch(batch_rows, batch_texts)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define schema upfront\n",
    "sdg_struct = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"display_name\", StringType(), True),\n",
    "    StructField(\"score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"work_id\", StringType(), nullable=False),\n",
    "    StructField(\"sdg\", ArrayType(sdg_struct), nullable=True)\n",
    "])\n",
    "\n",
    "# Use mapPartitions - creates RDD, converts to DataFrame\n",
    "res_rdd = df.select(\"work_id\", \"title\", \"abstract\").rdd.mapPartitions(process_partition)\n",
    "inferred_sdg_df = spark.createDataFrame(res_rdd, output_schema)\n",
    "inferred_sdg_df.cache()\n",
    "\n",
    "output_count = inferred_sdg_df.count()\n",
    "print(f\"Output Row count: {output_count}\")\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "print(f\"Total runtime: {runtime:.4f} seconds\")\n",
    "if runtime > 0:\n",
    "    print(f\"Total throughput: {output_count / runtime:.4f} inferences/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c66d355-deb8-4a34-98db-e2f25be26c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to works_sdg_frontfill table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9526e736-cc43-49dd-9de9-97e9cbf62753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Write to table\n",
    "(inferred_sdg_df\n",
    "    .withColumn(\"created_timestamp\", current_timestamp())\n",
    "    .select(\"work_id\", \"sdg\", \"created_timestamp\")\n",
    "    .write.mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"openalex.works.works_sdg_frontfill\")\n",
    ")\n",
    "\n",
    "# Cleanup: delete processed work_ids from input table\n",
    "inferred_sdg_df.createOrReplaceTempView(\"processed_ids\")\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM openalex.works.works_sdg_frontfill_input \n",
    "    WHERE work_id IN (SELECT work_id FROM processed_ids)\n",
    "\"\"\")\n",
    "print(\"Removed processed work_ids from works_sdg_frontfill_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318d267e-1e39-4db8-8eb2-f244583987e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify table structure and sample results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5413e154-d827-4058-bc03-527855e677e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    work_id,\n",
    "    size(sdg) AS num_sdgs,\n",
    "    slice(sdg, 1, 3) AS top_3_sdgs,\n",
    "    created_timestamp\n",
    "FROM openalex.works.works_sdg_frontfill;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sdg_inference_frontfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
