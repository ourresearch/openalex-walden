{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4a0ae1-49d8-41c2-bfef-244aa3c90630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install /Volumes/openalex/default/libraries/openalex_dlt_utils-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7156f7-61cb-48c4-af6e-d3decbf9d532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from openalex.utils.environment import *\n",
    "\n",
    "from openalex.dlt.normalize import normalize_title_udf, normalize_license, normalize_license_udf, walden_works_schema\n",
    "from openalex.dlt.transform import apply_initial_processing, apply_final_merge_key_and_filter, enrich_with_features_and_author_keys\n",
    "\n",
    "@dlt.table(name=\"mag_enriched\", comment=\"MAG data after full parsing and enrichment.\")\n",
    "def mag_enriched():\n",
    "    mag_df = (\n",
    "        spark.readStream\n",
    "            .table(\"openalex.mag.mag_works\")  # Let DLT handle CDF\n",
    "            .withColumn(\"provenance\", F.lit(\"mag\"))\n",
    "            # ðŸš« Exclude deleted journals here, make sure null records stay in the stream\n",
    "            .filter(F.col(\"source_name\").isNull() | (F.col(\"source_name\") != \"Deleted Journal\"))\n",
    "    )\n",
    "\n",
    "    processed_df = apply_initial_processing(mag_df, \"mag\", walden_works_schema)\n",
    "    enriched_df = enrich_with_features_and_author_keys(processed_df)\n",
    "    final_df = apply_final_merge_key_and_filter(enriched_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Create target table\n",
    "dlt.create_streaming_table(\n",
    "    name=\"mag_dlt_works\",  # Final target table name\n",
    "    comment=f\"Final MAG works table with unique identifiers and CDF applied in {ENV.upper()} environment.\",\n",
    "    table_properties={\"quality\": \"gold\", \"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "# Apply changes\n",
    "dlt.apply_changes(\n",
    "    target=\"mag_dlt_works\",\n",
    "    source=\"mag_enriched\",\n",
    "    keys=[\"native_id\"],\n",
    "    sequence_by=\"updated_date\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
