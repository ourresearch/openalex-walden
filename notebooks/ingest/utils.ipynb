{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5185c3d7-a377-49a0-8018-c050cfab2b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nameparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396d2759-0857-478e-afb9-6c6573ef53e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from nameparser import HumanName # Will be installed via pipeline libraries\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configuration & Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "# Final output table name, equivalent to current 'openalex.works.sources_combined'\n",
    "# or your chosen name for the parallel run (e.g., 'locations_streamed')\n",
    "FINAL_OUTPUT_TABLE_NAME = \"locations_parsed\"\n",
    "\n",
    "UPSTREAM_SOURCES = {\n",
    "    \"crossref\": \"openalex.crossref.crossref_works\",\n",
    "    \"datacite\": \"openalex.datacite.datacite_works\",\n",
    "    \"pdf\": \"openalex.pdf.pdf_works\",\n",
    "    \"pubmed\": \"openalex.pubmed.pubmed_works\",\n",
    "    \"repo\": \"openalex.repo.repo_works\",\n",
    "    \"repo_backfill\": \"openalex.repo.repo_works_backfill\",\n",
    "    \"mag\": \"openalex.mag.mag_works\",\n",
    "    \"landing_page\": \"openalex.landing_page.landing_page_works\"\n",
    "}\n",
    "\n",
    "MERGE_COLUMN_NAME = \"merge_key\"\n",
    "\n",
    "# Walden Schema definition (as per your Locations.py)\n",
    "# This is the target schema for the first major normalization step.\n",
    "# It does NOT initially contain authors_exist or authors.author_key as per your notebook.\n",
    "# These will be added in subsequent transformations.\n",
    "walden_schema = StructType([\n",
    "    StructField(\"provenance\", StringType(), True), StructField(\"native_id\", StringType(), True),\n",
    "    StructField(\"native_id_namespace\", StringType(), True), StructField(\"title\", StringType(), True),\n",
    "    StructField(\"normalized_title\", StringType(), True),\n",
    "    StructField(\"authors\", ArrayType(StructType([\n",
    "        StructField(\"given\", StringType(), True), StructField(\"family\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True), StructField(\"orcid\", StringType(), True),\n",
    "        StructField(\"affiliations\", ArrayType(StructType([\n",
    "            StructField(\"name\", StringType(), True), StructField(\"department\", StringType(), True),\n",
    "            StructField(\"ror_id\", StringType(), True)])), True),\n",
    "        StructField(\"is_corresponding\", BooleanType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"ids\", ArrayType(StructType([\n",
    "        StructField(\"id\", StringType(), True), StructField(\"namespace\", StringType(), True),\n",
    "        StructField(\"relationship\", StringType(), True)])), True),\n",
    "    StructField(\"type\", StringType(), True), StructField(\"version\", StringType(), True),\n",
    "    StructField(\"license\", StringType(), True), StructField(\"language\", StringType(), True),\n",
    "    StructField(\"published_date\", DateType(), True), StructField(\"created_date\", DateType(), True),\n",
    "    StructField(\"updated_date\", DateType(), True), StructField(\"issue\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True), StructField(\"first_page\", StringType(), True),\n",
    "    StructField(\"last_page\", StringType(), True), StructField(\"is_retracted\", BooleanType(), True),\n",
    "    StructField(\"abstract\", StringType(), True), StructField(\"source_name\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"funders\", ArrayType(StructType([\n",
    "        StructField(\"doi\", StringType(), True), StructField(\"ror\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True), StructField(\"awards\", ArrayType(StringType(), True), True)\n",
    "    ])), True),\n",
    "    StructField(\"references\", ArrayType(StructType([\n",
    "        StructField(\"doi\", StringType(), True), StructField(\"pmid\", StringType(), True),\n",
    "        StructField(\"arxiv\", StringType(), True), StructField(\"title\", StringType(), True),\n",
    "        StructField(\"authors\", StringType(), True), StructField(\"year\", StringType(), True),\n",
    "        StructField(\"raw\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"urls\", ArrayType(StructType([\n",
    "        StructField(\"url\", StringType(), True), StructField(\"content_type\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"mesh\", StringType(), True), StructField(\"is_oa\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# --- last_name_only UDF and its helpers (VERBATIM) ---\n",
    "def remove_latin_characters(author):\n",
    "    if author and any(\"\\u0080\" <= c <= \"\\u02AF\" for c in author):\n",
    "        author = (\n",
    "            unicodedata.normalize(\"NFKD\", author)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"ascii\")\n",
    "        )\n",
    "    return author\n",
    "\n",
    "def remove_author_prefixes(author):\n",
    "    if not author: return \"\"\n",
    "    prefixes = [\"None \", \"Array \"]\n",
    "    for prefix in prefixes:\n",
    "        if author.startswith(prefix):\n",
    "            author = author.replace(prefix, \"\")\n",
    "    return author\n",
    "\n",
    "def clean_author_name(author_name):\n",
    "    if not author_name: return \"\"\n",
    "    return re.sub(r\"[ \\-‐.'' ́>]\", \"\", author_name).strip()\n",
    "\n",
    "def last_name_only(author): \n",
    "    if not author:\n",
    "        return [\"\", \"\", \"\"] \n",
    "    author = remove_latin_characters(author)\n",
    "    author = remove_author_prefixes(author)\n",
    "    author_name_obj = HumanName(author) \n",
    "    first_name = clean_author_name(author_name_obj.first)\n",
    "    last_name = clean_author_name(author_name_obj.last)\n",
    "    first_initial = first_name[0] if first_name else \"\"\n",
    "    return [ f\"{last_name};{first_initial}\", f\"{first_name}\", f\"{last_name}\" ]\n",
    "\n",
    "# Schema for the enriched author struct (output of the author processing Pandas UDF)\n",
    "# This MUST match the structure of the dictionaries returned by the Pandas UDF's internal logic\n",
    "# AND the walden_schema's authors element type plus author_key.\n",
    "enriched_author_struct_type = StructType([\n",
    "    StructField(\"given\", StringType(), True),\n",
    "    StructField(\"family\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"orcid\", StringType(), True),\n",
    "    StructField(\"affiliations\", walden_schema[\"authors\"].dataType.elementType[\"affiliations\"].dataType, True), \n",
    "    StructField(\"is_corresponding\", BooleanType(), True), \n",
    "    StructField(\"author_key\", StringType(), True)\n",
    "])\n",
    "\n",
    "@F.pandas_udf(ArrayType(enriched_author_struct_type))\n",
    "def udf_last_name_only(authors_arrays_series: pd.Series) -> pd.Series: # Name matches your original UDF variable\n",
    "    results = []\n",
    "    for author_list_for_single_record in authors_arrays_series:\n",
    "        if author_list_for_single_record is None:\n",
    "            results.append(None)\n",
    "            continue\n",
    "        \n",
    "        processed_author_list = []\n",
    "        for author_dict in author_list_for_single_record:\n",
    "            if author_dict is None:\n",
    "                processed_author_list.append(None) \n",
    "                continue\n",
    "\n",
    "            name_str_for_parser = author_dict.get(\"name\")\n",
    "            if not name_str_for_parser:\n",
    "                given_original = author_dict.get(\"given\", \"\") or \"\" \n",
    "                family_original = author_dict.get(\"family\", \"\") or \"\"\n",
    "                name_str_for_parser = f\"{given_original} {family_original}\".strip()\n",
    "            \n",
    "            parsed_name_components = [\"\", \"\", \"\"] \n",
    "            if name_str_for_parser:\n",
    "                try:\n",
    "                    # Calling YOUR original last_name_only Python function\n",
    "                    parsed_name_components = last_name_only(name_str_for_parser) \n",
    "                except Exception: \n",
    "                    pass \n",
    "\n",
    "            new_given = parsed_name_components[1]\n",
    "            new_family = parsed_name_components[2]\n",
    "            reconstructed_name = author_dict.get(\"name\")\n",
    "            if not reconstructed_name and (new_given or new_family):\n",
    "                reconstructed_name = f\"{new_given or ''} {new_family or ''}\".strip()\n",
    "            \n",
    "            is_corresponding_val = author_dict.get(\"is_corresponding\")\n",
    "            is_corresponding_bool = None\n",
    "            if isinstance(is_corresponding_val, str):\n",
    "                is_corresponding_bool = is_corresponding_val.lower() == 'true'\n",
    "            elif isinstance(is_corresponding_val, bool):\n",
    "                is_corresponding_bool = is_corresponding_val\n",
    "            \n",
    "            processed_author_struct = {\n",
    "                \"given\": new_given, \"family\": new_family, \"name\": reconstructed_name,\n",
    "                \"orcid\": author_dict.get(\"orcid\"), \n",
    "                \"affiliations\": author_dict.get(\"affiliations\"), \n",
    "                \"is_corresponding\": is_corresponding_bool,\n",
    "                \"author_key\": parsed_name_components[0].lower() if parsed_name_components and parsed_name_components[0] else None\n",
    "            }\n",
    "            processed_author_list.append(processed_author_struct)\n",
    "        results.append(processed_author_list)\n",
    "    return pd.Series(results)\n",
    "\n",
    "\n",
    "def f_generate_inverted_index(abstract_string_input): \n",
    "    import re \n",
    "    import json \n",
    "    from collections import OrderedDict \n",
    "    \n",
    "    if not abstract_string_input or not isinstance(abstract_string_input, str): \n",
    "        return None\n",
    "    \n",
    "    abstract_s = abstract_string_input\n",
    "    \n",
    "    # MODIFIED: Combined regex pattern directly in the re.sub call (inline)\n",
    "    # Replaces newlines, tabs, JATS opening/closing tags, <p>, </p> tags with a single space.\n",
    "    # The \\b replacement was removed as its intent was unclear and potentially problematic.\n",
    "    abstract_s = re.sub(\n",
    "        r\"\\n|\\t|<jats:[^>]*?>|</jats:[^>]*?>|<p>|</p>\", # Inline regex string\n",
    "        \" \", \n",
    "        abstract_s\n",
    "    )\n",
    "    \n",
    "    # Consolidate multiple spaces and strip\n",
    "    abstract_s = \" \".join(abstract_s.split()).strip()\n",
    "\n",
    "    if not abstract_s: \n",
    "        return None\n",
    "\n",
    "    invertedIndex = OrderedDict()\n",
    "    words = abstract_s.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in invertedIndex: \n",
    "            invertedIndex[word] = []\n",
    "        invertedIndex[word].append(i)\n",
    "    \n",
    "    return json.dumps(invertedIndex, ensure_ascii=False) if invertedIndex else None\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def udf_f_generate_inverted_index(abstract_series: pd.Series) -> pd.Series: # Name matches your original UDF variable\n",
    "    # This Pandas UDF calls your 'f_generate_inverted_index' Python function\n",
    "    return abstract_series.apply(f_generate_inverted_index)\n",
    "\n",
    "def transform_struct(col_name, source_struct, target_struct):\n",
    "    target_fields = {f.name: f for f in target_struct.fields}\n",
    "    source_fields = {f.name: f for f in source_struct.fields}\n",
    "    expressions = []\n",
    "    for field_name, field in target_fields.items():\n",
    "        if field_name in source_fields:\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                expressions.append(\n",
    "                    transform_struct(f\"{col_name}.{field_name}\", source_fields[field_name].dataType, field.dataType)\n",
    "                )\n",
    "            else:\n",
    "                expressions.append(F.col(f\"{col_name}.{field_name}\").cast(field.dataType).alias(field_name))\n",
    "        else:\n",
    "            expressions.append(F.lit(None).cast(field.dataType).alias(field_name))\n",
    "    return F.struct(*expressions).alias(col_name)\n",
    "\n",
    "def transform_array_of_structs(col_name, source_array, target_array):\n",
    "    target_fields = target_array.elementType.fields\n",
    "    source_fields = {f.name: f for f in source_array.elementType.fields}\n",
    "    struct_fields_expr = []\n",
    "    for field in target_fields:\n",
    "        if field.name in source_fields:\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                nested_expr = transform_struct(\"x.\" + field.name, source_fields[field.name].dataType, field.dataType)\n",
    "                struct_fields_expr.append(f\"{nested_expr} AS {field.name}\")\n",
    "            else:\n",
    "                struct_fields_expr.append(f\"x.{field.name} AS {field.name}\")\n",
    "        else:\n",
    "            struct_fields_expr.append(f\"CAST(NULL AS STRING) AS {field.name}\")\n",
    "    struct_expr = f\"STRUCT({', '.join(struct_fields_expr)})\"\n",
    "    return F.expr(f\"TRANSFORM({col_name}, x -> {struct_expr})\").alias(col_name)\n",
    "\n",
    "\n",
    "def align_column(col_name, source_type, target_type):\n",
    "    if isinstance(target_type, StructType) and isinstance(source_type, StructType):\n",
    "        return transform_struct(col_name, source_type, target_type)\n",
    "    elif isinstance(target_type, ArrayType) and isinstance(source_type, ArrayType):\n",
    "        if isinstance(target_type.elementType, StructType) and isinstance(source_type.elementType, StructType):\n",
    "            return transform_array_of_structs(col_name, source_type, target_type)\n",
    "        else:\n",
    "            return F.col(col_name).cast(target_type)\n",
    "    else:\n",
    "        return F.col(col_name).cast(target_type)\n",
    "\n",
    "def apply_walden_schema(df, schema):\n",
    "    schema_fields = {field.name: field for field in schema.fields}\n",
    "    source_schema_fields = {field.name: field for field in df.schema.fields}\n",
    "    aligned_columns = []\n",
    "    for col_name, target_field in schema_fields.items():\n",
    "        if col_name in source_schema_fields:\n",
    "            aligned_columns.append(align_column(col_name, source_schema_fields[col_name].dataType, target_field.dataType))\n",
    "        else:\n",
    "            aligned_columns.append(F.lit(None).cast(target_field.dataType).alias(col_name))\n",
    "    return df.select(*aligned_columns)\n",
    "\n",
    "# --- DOI Normalization (Spark native version for DLT) ---\n",
    "def normalize_doi_spark_col(doi_string_col_expr):\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_extract(\n",
    "            F.lower(F.trim(F.regexp_replace(doi_string_col_expr, \" \", \"\"))),\n",
    "            r\"(10\\.\\d+/[^\\s]+)\", 1),\n",
    "        r\"\\u0000\", \"\")\n",
    "\n",
    "# --- create_merge_column and clean_native_id (VERBATIM from your \"Locations Parsed\" DLT snippet) ---\n",
    "def clean_native_id(df, column_name=\"native_id\"):\n",
    "    return (\n",
    "            df.withColumn(column_name, F.regexp_replace(F.col(column_name), r\"https?://\", \"\"))\n",
    "            .withColumn(column_name, F.regexp_replace(F.col(column_name), r\"/+$\", \"\"))\n",
    "            .withColumn(column_name, F.regexp_replace(F.col(column_name), r\"[^a-zA-Z0-9./:]\", \"\"))\n",
    "            .withColumn(column_name, F.lower(F.col(column_name))) \n",
    "    )\n",
    "\n",
    "def create_merge_column(df): \n",
    "    df_cleaned = clean_native_id(df, \"native_id\") \n",
    "    df_cleaned = df_cleaned.withColumn(\"title_cleaned_newline\", F.regexp_replace(F.col(\"title\"), \"\\n\", \" \"))\n",
    "    return df_cleaned.withColumn(MERGE_COLUMN_NAME,\n",
    "        F.struct(\n",
    "            F.element_at(F.expr(\"filter(ids, x -> x.namespace = 'doi' and x.id is not null)\"), 1).getField(\"id\").alias(\"doi\"),\n",
    "            F.element_at(F.expr(\"filter(ids, x -> x.namespace = 'pmid' and x.id is not null)\"), 1).getField(\"id\").alias(\"pmid\"),\n",
    "            F.element_at(F.expr(\"filter(ids, x -> x.namespace = 'arxiv' and x.id is not null)\"), 1).getField(\"id\").alias(\"arxiv\"),\n",
    "            F.when(\n",
    "                (F.expr(f\"title_cleaned_newline in (select title from openalex.system.bad_titles)\")) |\n",
    "                (F.length(F.col(\"title_cleaned_newline\")) < 19) |\n",
    "                (F.col(\"title_cleaned_newline\").isNull()),\n",
    "                F.concat(F.col(\"native_id\"), F.col(\"provenance\")) \n",
    "            ).when(F.col(\"authors_exist\") == False, F.col(\"normalized_title\")\n",
    "            ).otherwise(F.concat_ws(\"_\", F.col(\"normalized_title\"), F.col(\"authors\").getItem(0).getField(\"author_key\"))\n",
    "            ).alias(\"title_author\")\n",
    "        )).drop(\"title_cleaned_newline\")\n",
    "    \n",
    "# normalize title and types UDFs\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<\\w+.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def remove_everything_but_alphas(input_string):\n",
    "    if input_string:\n",
    "        return \"\".join(e for e in input_string if e.isalpha())\n",
    "    return \"\"\n",
    "\n",
    "def remove_accents(text):\n",
    "    normalized = unicodedata.normalize('NFD', text)\n",
    "    return ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def normalize_title(title):\n",
    "    if not title:\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(title, bytes):\n",
    "        title = str(title, 'ascii')\n",
    "\n",
    "    text = title[0:500]\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # handle unicode characters\n",
    "    text = remove_accents(text)\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = clean_html(text)\n",
    "\n",
    "    # remove articles and common prepositions\n",
    "    text = re.sub(r\"\\b(the|a|an|of|to|in|for|on|by|with|at|from|\\n)\\b\", \"\", text)\n",
    "\n",
    "    # remove everything except alphabetic characters\n",
    "    text = remove_everything_but_alphas(text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_license(text):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    normalized_text = text.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "\n",
    "    license_lookups = [\n",
    "        # open Access patterns\n",
    "        (\"infoeureposematicsaccess\", \"other-oa\"),\n",
    "        (\"openaccess\", \"other-oa\"),\n",
    "        \n",
    "        # publisher-specific\n",
    "        (\"elsevier.com/openaccess/userlicense\", \"publisher-specific-oa\"),\n",
    "        (\"pubs.acs.org/page/policy/authorchoice_termsofuse.html\", \"publisher-specific-oa\"),\n",
    "        (\"arxiv.orgperpetual\", \"publisher-specific-oa\"),\n",
    "        (\"arxiv.orgnonexclusive\", \"publisher-specific-oa\"),\n",
    "        \n",
    "        # creative Commons licenses\n",
    "        (\"ccbyncnd\", \"cc-by-nc-nd\"),\n",
    "        (\"ccbyncsa\", \"cc-by-nc-sa\"),\n",
    "        (\"ccbynd\", \"cc-by-nd\"),\n",
    "        (\"ccbysa\", \"cc-by-sa\"),\n",
    "        (\"ccbync\", \"cc-by-nc\"),\n",
    "        (\"ccby\", \"cc-by\"),\n",
    "        (\"creativecommons.org/licenses/by/\", \"cc-by\"),\n",
    "        \n",
    "        # public domain\n",
    "        (\"publicdomain\", \"public-domain\"),\n",
    "        \n",
    "        # software/Dataset licenses\n",
    "        (\"mit \", \"mit\"),\n",
    "        (\"gpl3\", \"gpl-3\"),\n",
    "        (\"gpl2\", \"gpl-2\"),\n",
    "        (\"gpl\", \"gpl\"),\n",
    "        (\"apache2\", \"apache-2.0\")\n",
    "    ]\n",
    "\n",
    "    for lookup, license in license_lookups:\n",
    "        if lookup in normalized_text:\n",
    "            if license == \"public-domain\" and \"worksnotinthepublicdomain\" in normalized_text:\n",
    "                continue\n",
    "            return license\n",
    "    return None\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def normalize_license_udf(license_series: pd.Series) -> pd.Series:\n",
    "    # This Pandas UDF calls your original 'normalize_license' Python function\n",
    "    return license_series.apply(normalize_license)\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def normalize_title_udf(title_series: pd.Series) -> pd.Series:\n",
    "    # This Pandas UDF calls your original 'normalize_title' Python function\n",
    "    return title_series.apply(normalize_title)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
