{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81647cd1-fb1a-4700-81d0-4c1a7d7588f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install /Volumes/openalex/default/libraries/openalex_dlt_utils-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3880b645-74c4-49cb-b468-737c3cbde60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from openalex.utils.environment import *\n",
    "\n",
    "UPSTREAM_SOURCES = {\n",
    "    \"crossref\": f\"openalex{ENV_SUFFIX}.crossref.crossref_works\",\n",
    "    \"datacite\": f\"openalex{ENV_SUFFIX}.datacite.datacite_works\",\n",
    "    \"pdf\": f\"openalex{ENV_SUFFIX}.pdf.pdf_works\",\n",
    "    \"pubmed\": f\"openalex{ENV_SUFFIX}.pubmed.pubmed_works\",\n",
    "    \"repo\": f\"openalex{ENV_SUFFIX}.repo.repo_works\",\n",
    "    \"landing_page\": f\"openalex{ENV_SUFFIX}.landing_page.landing_page_works\",\n",
    "    \"mag\": f\"openalex{ENV_SUFFIX}.mag.mag_dlt_works\"\n",
    "}\n",
    "\n",
    "# Step 1: Union upstreams into a streaming view\n",
    "@dlt.view(name=\"locations_parsed_union\")\n",
    "def locations_parsed_union():\n",
    "    # Get canonical column order from repo (which has raw_native_type in correct position)\n",
    "    canonical_columns = None\n",
    "    \n",
    "    dfs = []\n",
    "    for key, table_name in UPSTREAM_SOURCES.items():\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .table(table_name)\n",
    "            .filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "        )\n",
    "        # Get canonical columns from repo (first source with raw_native_type)\n",
    "        if (key == \"repo\"):\n",
    "            canonical_columns = df.columns\n",
    "        else:\n",
    "            # Add raw_native_type as NULL for non-repo sources\n",
    "            if \"raw_native_type\" not in df.columns:\n",
    "                df = df.withColumn(\"raw_native_type\", F.lit(None).cast(\"string\"))                \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Union all DataFrames, then select columns in canonical order (from repo)\n",
    "    df_all = reduce(lambda d1, d2: d1.unionByName(d2, allowMissingColumns=True), dfs)\n",
    "    \n",
    "    # Filter out records with empty titles AFTER union to avoid missing updates\n",
    "    # do not bother saving anything that has no title - it's a waste of space per Jason\n",
    "    # commented out because it excludes a lot of pdf_works records that we need for pdf_urls\n",
    "    # df_all = df_all.filter(F.col(\"title\").isNotNull() & (F.length(F.col(\"title\")) > 0))\n",
    "    df_all = df_all.select(*canonical_columns)\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "# Step 2: Define the final SCD1 table and apply changes\n",
    "dlt.create_streaming_table(\n",
    "    name=\"locations_parsed\",\n",
    "    comment=f\"Unified parsed works data in {ENV.upper()} environment from Crossref, DataCite, PDF, PubMed, Repo and Landing Page.\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"delta.enableChangeDataFeed\": \"true\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target=\"locations_parsed\",\n",
    "    source=\"locations_parsed_union\",\n",
    "    keys=[\"native_id\"],\n",
    "    sequence_by=\"updated_date\",\n",
    "    stored_as_scd_type=1,\n",
    "    except_column_list=[\"_change_type\", \"_commit_version\", \"_commit_timestamp\"],\n",
    "    apply_as_deletes=\"lower(_change_type) = 'delete'\",\n",
    "    # ignore_null_updates:\n",
    "    # - True: Skip updates that only set fields to NULL (preserves existing values)\n",
    "    #         Use this if upstream sends sparse updates and you want to preserve existing data\n",
    "    # - False: Process all updates, including those that set fields to NULL (can overwrite with NULL)\n",
    "    #          Use this if you want all upstream updates to propagate, even if they set fields to NULL\n",
    "    ignore_null_updates=False  # Process all updates including sparse partial ones - changed from True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8586065140519705,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UnionAllWorksIntoLocationsParsed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
