{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81647cd1-fb1a-4700-81d0-4c1a7d7588f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install /Volumes/openalex/default/libraries/openalex_dlt_utils-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3880b645-74c4-49cb-b468-737c3cbde60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from openalex.utils.environment import *\n",
    "\n",
    "UPSTREAM_SOURCES = {\n",
    "    \"crossref\": f\"openalex{ENV_SUFFIX}.crossref.crossref_works\",\n",
    "    \"datacite\": f\"openalex{ENV_SUFFIX}.datacite.datacite_works\",\n",
    "    \"pdf\": f\"openalex{ENV_SUFFIX}.pdf.pdf_works\",\n",
    "    \"pubmed\": f\"openalex{ENV_SUFFIX}.pubmed.pubmed_works\",\n",
    "    \"repo\": f\"openalex{ENV_SUFFIX}.repo.repo_works\",\n",
    "    \"landing_page\": f\"openalex{ENV_SUFFIX}.landing_page.landing_page_works\",\n",
    "    \"mag\": f\"openalex{ENV_SUFFIX}.mag.mag_dlt_works\"\n",
    "}\n",
    "\n",
    "# Step 1: Union upstreams into a streaming view\n",
    "@dlt.view(name=\"locations_parsed_union\")\n",
    "def locations_parsed_union():\n",
    "    # Get canonical column order from repo (which has raw_native_type in correct position)\n",
    "    repo_df = (\n",
    "        spark.readStream\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .table(UPSTREAM_SOURCES[\"repo\"])\n",
    "        .limit(0)  # Just need schema, not data\n",
    "    )\n",
    "    canonical_columns = repo_df.columns\n",
    "    \n",
    "    dfs = []\n",
    "    for key, table_name in UPSTREAM_SOURCES.items():\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .table(table_name)\n",
    "            .filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "            # do not bother saving anything that has no title - it's a waste of space per Jason\n",
    "            .filter(F.col(\"title\").isNotNull() & (F.length(F.col(\"title\")) > 0))\n",
    "        )\n",
    "        # remove CiteSeerX (better suited upstream but more painful to re-run)\n",
    "        if (key == \"repo\"):\n",
    "            # should this move to the locations_validated logic perhaps? That's where Sources become handy (after locations_w_sources)\n",
    "            df = df.where(\"\"\"NOT(native_id ILIKE 'oai:CiteSeerX%'\n",
    "                OR (size(urls) = 1 AND urls[0].url ILIKE 'http://citeseerx.ist.psu.edu%'))\"\"\")\n",
    "        else:\n",
    "            # Add raw_native_type as NULL for non-repo sources\n",
    "            if \"raw_native_type\" not in df.columns:\n",
    "                df = df.withColumn(\"raw_native_type\", F.lit(None).cast(\"string\"))\n",
    "        \n",
    "        # Select columns in canonical order (only columns that exist in this DataFrame)\n",
    "        # This ensures raw_native_type appears before type for all sources\n",
    "        existing_columns = df.columns\n",
    "        ordered_columns = [col for col in canonical_columns if col in existing_columns]\n",
    "        df = df.select(*ordered_columns)\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Now all DataFrames have same columns in same order\n",
    "    return reduce(lambda d1, d2: d1.unionByName(d2, allowMissingColumns=True), dfs)\n",
    "\n",
    "# Step 2: Define the final SCD1 table and apply changes\n",
    "dlt.create_streaming_table(\n",
    "    name=\"locations_parsed\",\n",
    "    comment=f\"Unified parsed works data in {ENV.upper()} environment from Crossref, DataCite, PDF, PubMed, Repo and Landing Page.\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"delta.enableChangeDataFeed\": \"true\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target=\"locations_parsed\",\n",
    "    source=\"locations_parsed_union\",\n",
    "    keys=[\"native_id\"],\n",
    "    sequence_by=\"updated_date\",\n",
    "    stored_as_scd_type=1,\n",
    "    except_column_list=[\"_change_type\", \"_commit_version\", \"_commit_timestamp\"],\n",
    "    apply_as_deletes=\"lower(_change_type) = 'delete'\",  # ðŸ‘ˆ Enable DELETE detection\n",
    "    ignore_null_updates=True                            # ðŸ‘ˆ Optional safety for sparse updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced4d6f1-0c3c-4cc6-932b-8d49672421ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8586065140519705,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UnionAllWorksIntoLocationsParsed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
