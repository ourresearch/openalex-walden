{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d005b0b-19e8-44dc-91b6-9f6564ac974e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Daily unpaywall snapshot - all records into single compressed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c699ae-fdc0-4cd2-9b42-23249cfda76d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "s3_bucket = \"unpaywall-data-feed-walden\"\n",
    "source_table = \"openalex.unpaywall.unpaywall\"\n",
    "s3_prefix = \"full_snapshots\"\n",
    "\n",
    "df = spark.table(source_table)\n",
    "current_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H%M%S')\n",
    "filename = f\"unpaywall_snapshot_{current_date}.jsonl.gz\"\n",
    "\n",
    "record_count = df.count()\n",
    "print(f\"Found {record_count} records to export\")\n",
    "\n",
    "if record_count == 0:\n",
    "    print(\"No records found to export. Job completed.\")\n",
    "    dbutils.notebook.exit(\"No records found to export\")\n",
    "\n",
    "final_path = f\"s3://{s3_bucket}/{s3_prefix}/{filename}\"\n",
    "\n",
    "print(f\"Exporting {record_count} records to {final_path}\")\n",
    "\n",
    "records_per_partition = 5000000\n",
    "partition_count = max(1, int(record_count / records_per_partition))\n",
    "print(f\"Using {partition_count} partitions for initial distribution\")\n",
    "\n",
    "# Create a temp directory with unique name for uncompressed data\n",
    "temp_dir = f\"s3://{s3_bucket}/temp_export_{current_date}/\"\n",
    "\n",
    "# Write uncompressed data in distributed fashion\n",
    "(df.select(\"json_response\")\n",
    "    .repartition(partition_count)\n",
    "    .write\n",
    "    .format(\"text\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(temp_dir)\n",
    ")\n",
    "\n",
    "print(\"Data written to temp location in uncompressed format\")\n",
    "print(\"Compressing data into a single file...\")\n",
    "\n",
    "# use coalesce instead of repartition for the final step to avoid shuffle\n",
    "spark.read.text(temp_dir).coalesce(1).write.format(\"text\").mode(\"overwrite\").option(\"compression\", \"gzip\").save(\"/tmp/unpaywall_single/\")\n",
    "\n",
    "# get the resulting single part file\n",
    "single_part_file = [f.path for f in dbutils.fs.ls(\"/tmp/unpaywall_single/\") if f.name.startswith(\"part-\") and f.name.endswith(\".gz\")][0]\n",
    "\n",
    "dbutils.fs.cp(single_part_file, final_path)\n",
    "dbutils.fs.rm(\"/tmp/unpaywall_single/\", recurse=True)\n",
    "dbutils.fs.rm(temp_dir, recurse=True)\n",
    "\n",
    "print(f\"Successfully exported {record_count} records to {final_path}\")\n",
    "\n",
    "# cleanup: Keep only the 5 most recent snapshots\n",
    "print(\"Starting cleanup of old snapshots...\")\n",
    "\n",
    "try:\n",
    "    # list all files in the snapshots directory\n",
    "    snapshot_files = dbutils.fs.ls(f\"s3://{s3_bucket}/{s3_prefix}/\")\n",
    "    \n",
    "    # filter for snapshot files and extract timestamps\n",
    "    snapshot_pattern = re.compile(r'unpaywall_snapshot_(\\d{4}-\\d{2}-\\d{2}T\\d{6})\\.jsonl\\.gz$')\n",
    "    snapshots_with_timestamps = []\n",
    "    \n",
    "    for file_info in snapshot_files:\n",
    "        match = snapshot_pattern.search(file_info.name)\n",
    "        if match:\n",
    "            timestamp_str = match.group(1)\n",
    "            # Convert timestamp to datetime for sorting\n",
    "            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%dT%H%M%S')\n",
    "            snapshots_with_timestamps.append((file_info.path, timestamp, file_info.name))\n",
    "    \n",
    "    print(f\"Found {len(snapshots_with_timestamps)} snapshot files\")\n",
    "    \n",
    "    # sort by timestamp (newest first)\n",
    "    snapshots_with_timestamps.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # keep only the 5 most recent, delete the rest\n",
    "    snapshots_to_keep = 5\n",
    "    if len(snapshots_with_timestamps) > snapshots_to_keep:\n",
    "        snapshots_to_delete = snapshots_with_timestamps[snapshots_to_keep:]\n",
    "        \n",
    "        print(f\"Deleting {len(snapshots_to_delete)} old snapshot(s):\")\n",
    "        for file_path, timestamp, filename in snapshots_to_delete:\n",
    "            print(f\"  - Deleting: {filename} (created: {timestamp})\")\n",
    "            dbutils.fs.rm(file_path)\n",
    "        \n",
    "        print(f\"Cleanup completed. Kept {snapshots_to_keep} most recent snapshots.\")\n",
    "    else:\n",
    "        print(f\"Only {len(snapshots_with_timestamps)} snapshots found. No cleanup needed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {str(e)}\")\n",
    "    print(\"Snapshot creation was successful, but cleanup failed.\")\n",
    "\n",
    "print(\"Script completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "export_wunpaywall",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
