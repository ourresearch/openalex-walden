{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8fc31f-fc33-4f55-b0b8-394b15c4cca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.functions import col\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "s3_bucket = \"unpaywall-data-feed-walden\"\n",
    "source_table = \"openalex.unpaywall.unpaywall\"\n",
    "s3_prefix = \"full_snapshots\"\n",
    "\n",
    "AWS_ACCESS_KEY_ID = dbutils.secrets.get(\"webscraper\", \"aws_access_key_id\")\n",
    "AWS_SECRET_ACCESS_KEY = dbutils.secrets.get(\"webscraper\", \"aws_secret_access_key\")\n",
    "\n",
    "df = spark.table(source_table)\n",
    "current_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H%M%S')\n",
    "final_filename = f\"unpaywall_snapshot_{current_date}.jsonl.gz\"\n",
    "final_key = f\"{s3_prefix}/{final_filename}\"\n",
    "temp_prefix = f\"temp_parts_{current_date}\"\n",
    "temp_s3_path = f\"s3://{s3_bucket}/{temp_prefix}\"\n",
    "\n",
    "record_count = df.count()\n",
    "print(f\"Found {record_count} records to export\")\n",
    "\n",
    "if record_count == 0:\n",
    "    dbutils.notebook.exit(\"No records found to export\")\n",
    "\n",
    "records_per_partition = 1000000\n",
    "num_partitions = max(1, int(record_count / records_per_partition))\n",
    "\n",
    "print(f\"Writing data in parallel to {temp_s3_path} using {num_partitions} partitions...\")\n",
    "\n",
    "(df.select(\"json_response\")\n",
    "    .repartition(num_partitions)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"gzip\")\n",
    "    .text(temp_s3_path)\n",
    ")\n",
    "\n",
    "# combine the parts without downloading them to the driver\n",
    "\n",
    "print(\"Parallel write complete. Starting S3 Multipart Merge...\")\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3', \n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "\n",
    "def copy_part(bucket, source_key, dest_key, part_num, upload_id):\n",
    "    \"\"\"Helper function to copy a single part.\"\"\"\n",
    "    try:\n",
    "        copy_source = {'Bucket': bucket, 'Key': source_key}\n",
    "        response = s3_client.upload_part_copy(\n",
    "            Bucket=bucket,\n",
    "            CopySource=copy_source,\n",
    "            Key=dest_key,\n",
    "            PartNumber=part_num,\n",
    "            UploadId=upload_id\n",
    "        )\n",
    "        return {\n",
    "            'ETag': response['CopyPartResult']['ETag'],\n",
    "            'PartNumber': part_num\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying part {part_num}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def s3_multipart_merge(bucket, source_prefix, dest_key):\n",
    "    \"\"\"Merges all GZIP part files in a prefix into a single S3 object using threads.\"\"\"\n",
    "    \n",
    "    # List all part files\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=source_prefix)\n",
    "    \n",
    "    parts = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Key'].endswith('.gz') and 'part-' in obj['Key']:\n",
    "                    parts.append(obj['Key'])\n",
    "    \n",
    "    print(f\"Found {len(parts)} parts to merge.\")\n",
    "    if not parts:\n",
    "        raise Exception(\"No parts found to merge.\")\n",
    "\n",
    "    # sort parts to ensure deterministic order\n",
    "    parts.sort()\n",
    "\n",
    "    mp_upload = s3_client.create_multipart_upload(Bucket=bucket, Key=dest_key)\n",
    "    upload_id = mp_upload['UploadId']\n",
    "    \n",
    "    completed_parts = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting parallel merge...\")\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            futures = []\n",
    "            for i, part_key in enumerate(parts):\n",
    "                part_num = i + 1\n",
    "                futures.append(\n",
    "                    executor.submit(copy_part, bucket, part_key, dest_key, part_num, upload_id)\n",
    "                )\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                completed_parts.append(future.result())\n",
    "                \n",
    "        completed_parts.sort(key=lambda x: x['PartNumber'])\n",
    "            \n",
    "        print(\"Finalizing multipart upload...\")\n",
    "        s3_client.complete_multipart_upload(\n",
    "            Bucket=bucket,\n",
    "            Key=dest_key,\n",
    "            UploadId=upload_id,\n",
    "            MultipartUpload={'Parts': completed_parts}\n",
    "        )\n",
    "        print(\"Merge completed successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Merge failed: {e}\")\n",
    "        s3_client.abort_multipart_upload(Bucket=bucket, Key=dest_key, UploadId=upload_id)\n",
    "        raise e\n",
    "\n",
    "try:\n",
    "    s3_multipart_merge(s3_bucket, temp_prefix, final_key)\n",
    "    print(f\"Successfully created single file: s3://{s3_bucket}/{final_key}\")\n",
    "    \n",
    "    dbutils.fs.rm(temp_s3_path, recurse=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during S3 merge: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f05c275-514c-4fba-9221-fe5a4a1bfb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting cleanup of old snapshots...\")\n",
    "\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        snapshot_files = []\n",
    "        snapshot_pattern = re.compile(r'unpaywall_snapshot_(\\d{4}-\\d{2}-\\d{2}T\\d{6})\\.jsonl\\.gz$')\n",
    "        \n",
    "        for obj in response['Contents']:\n",
    "            key = obj['Key']\n",
    "            filename = key.split('/')[-1]\n",
    "            match = snapshot_pattern.search(filename)\n",
    "            if match:\n",
    "                timestamp = datetime.strptime(match.group(1), '%Y-%m-%dT%H%M%S')\n",
    "                snapshot_files.append({'Key': key, 'LastModified': timestamp})\n",
    "        \n",
    "        snapshot_files.sort(key=lambda x: x['LastModified'], reverse=True)\n",
    "        \n",
    "        snapshots_to_keep = 5\n",
    "        if len(snapshot_files) > snapshots_to_keep:\n",
    "            to_delete = snapshot_files[snapshots_to_keep:]\n",
    "            print(f\"Deleting {len(to_delete)} old snapshot(s)...\")\n",
    "            \n",
    "            objects_to_delete = [{'Key': item['Key']} for item in to_delete]\n",
    "            \n",
    "            for i in range(0, len(objects_to_delete), 1000):\n",
    "                batch = objects_to_delete[i:i+1000]\n",
    "                s3_client.delete_objects(Bucket=s3_bucket, Delete={'Objects': batch})\n",
    "                print(f\"Deleted batch of {len(batch)} files.\")\n",
    "        else:\n",
    "            print(\"No cleanup needed.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Cleanup failed, but export was successful. Error: {e}\")\n",
    "\n",
    "print(\"Job completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "export_wunpaywall",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
