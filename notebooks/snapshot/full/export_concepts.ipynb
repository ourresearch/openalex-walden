{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create `openalex.common.openalex_concepts_snapshot` in same format as API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transformed = (\n",
        "    spark.read.table(\"openalex.common.concepts_api\")\n",
        "    # Convert BIGINT id to full URL\n",
        "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/C\"), F.col(\"id\")))\n",
        ")\n",
        "\n",
        "df_transformed.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"openalex.common.openalex_concepts_snapshot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Export in json lines format to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_type = \"concepts\"\n",
        "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "s3_base_path = f\"s3://openalex-sandbox/snapshots/{date_str}\"\n",
        "output_path = f\"{s3_base_path}/{entity_type}\"\n",
        "\n",
        "def export():\n",
        "    print(f\"Starting export to: {output_path}\")\n",
        "    \n",
        "    df = spark.read.table(\"openalex.common.openalex_concepts_snapshot\")\n",
        "    record_count = df.count()\n",
        "    print(f\"Total records: {record_count:,}\")\n",
        "    \n",
        "    # Concepts is small enough to export as a single partition\n",
        "    # Partition by updated_date for consistency with other entities\n",
        "    # Use created_date as fallback, then current_date if both are null\n",
        "    df = df.withColumn(\"_partition_date\", F.coalesce(F.to_date(\"updated_date\"), F.to_date(\"created_date\"), F.current_date()))\n",
        "    \n",
        "    # Coalesce to single partition per date since dataset is small\n",
        "    (df.coalesce(1)\n",
        "       .write\n",
        "       .mode(\"overwrite\")\n",
        "       .option(\"compression\", \"gzip\")\n",
        "       .partitionBy(\"_partition_date\")\n",
        "       .json(output_path))\n",
        "    \n",
        "    print(\"Export completed!\")\n",
        "\n",
        "export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Rename files and cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_files_and_cleanup(output_path):\n",
        "    \"\"\"Rename partition directories and files for consistency.\"\"\"\n",
        "    \n",
        "    partitions = dbutils.fs.ls(output_path)\n",
        "    partitions_to_process = [p for p in partitions if p.name.startswith(\"_partition_date=\")]\n",
        "    \n",
        "    print(f\"Found {len(partitions_to_process)} partitions to process\")\n",
        "    \n",
        "    for partition in partitions_to_process:\n",
        "        date_value = partition.name.replace(\"_partition_date=\", \"\").rstrip(\"/\")\n",
        "        new_partition_path = f\"{output_path}/updated_date={date_value}/\"\n",
        "        \n",
        "        files = dbutils.fs.ls(partition.path)\n",
        "        json_files = [f for f in files if f.name.endswith('.gz')]\n",
        "        json_files.sort(key=lambda x: x.name)\n",
        "        \n",
        "        # Move and rename files\n",
        "        for idx, file_info in enumerate(json_files):\n",
        "            new_name = f\"part_{str(idx).zfill(4)}.gz\"\n",
        "            new_path = f\"{new_partition_path}{new_name}\"\n",
        "            dbutils.fs.mv(file_info.path, new_path)\n",
        "            print(f\"  Moved {file_info.name} -> updated_date={date_value}/{new_name}\")\n",
        "        \n",
        "        # Clean up metadata files\n",
        "        for f in files:\n",
        "            if not f.name.endswith('.gz'):\n",
        "                try:\n",
        "                    dbutils.fs.rm(f.path)\n",
        "                except:\n",
        "                    pass\n",
        "        \n",
        "        # Remove old partition directory\n",
        "        try:\n",
        "            dbutils.fs.rm(partition.path, recurse=True)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Clean up root-level Spark metadata\n",
        "    print(\"\\nCleaning up root metadata files...\")\n",
        "    try:\n",
        "        root_files = dbutils.fs.ls(output_path)\n",
        "        for f in root_files:\n",
        "            if f.name.startswith(\"_\"):\n",
        "                dbutils.fs.rm(f.path, recurse=True)\n",
        "                print(f\"  Removed {f.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not clean up root files: {e}\")\n",
        "    \n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "rename_files_and_cleanup(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create manifest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_manifest():\n",
        "    \"\"\"Create a manifest file with all file metadata.\"\"\"\n",
        "    output_path = f\"{s3_base_path}/{entity_type}\"\n",
        "    \n",
        "    print(f\"\\nCreating manifest...\")\n",
        "    \n",
        "    partitions = dbutils.fs.ls(output_path)\n",
        "    partitions_to_process = sorted(\n",
        "        [p for p in partitions if p.name.startswith(\"updated_date=\")],\n",
        "        key=lambda x: x.name, reverse=True\n",
        "    )\n",
        "    \n",
        "    entries = []\n",
        "    total_content_length = 0\n",
        "    total_record_count = 0\n",
        "    \n",
        "    for partition in partitions_to_process:\n",
        "        files = dbutils.fs.ls(partition.path)\n",
        "        for file_info in files:\n",
        "            if not file_info.name.endswith('.gz'):\n",
        "                continue\n",
        "            \n",
        "            # Count records\n",
        "            record_count = spark.read.text(file_info.path).count()\n",
        "            \n",
        "            # Build S3 URL for prod\n",
        "            raw = file_info.path.replace(\"dbfs:/\", \"s3://\")\n",
        "            marker = f\"/{entity_type}/\"\n",
        "            idx = raw.find(marker)\n",
        "            relative = raw[idx:]\n",
        "            s3_url = f\"s3://openalex/data{relative}\"\n",
        "            \n",
        "            entry = {\n",
        "                \"url\": s3_url,\n",
        "                \"meta\": {\n",
        "                    \"content_length\": file_info.size,\n",
        "                    \"record_count\": record_count\n",
        "                }\n",
        "            }\n",
        "            entries.append(entry)\n",
        "            total_content_length += file_info.size\n",
        "            total_record_count += record_count\n",
        "            \n",
        "            print(f\"  {partition.name}{file_info.name}: {record_count:,} records, {file_info.size/(1024*1024):.2f} MB\")\n",
        "    \n",
        "    entries.sort(key=lambda x: x[\"url\"])\n",
        "    \n",
        "    manifest = {\n",
        "        \"entries\": entries,\n",
        "        \"meta\": {\n",
        "            \"content_length\": total_content_length,\n",
        "            \"record_count\": total_record_count\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    manifest_path = f\"{output_path}/manifest\"\n",
        "    manifest_json = json.dumps(manifest, indent=2)\n",
        "    dbutils.fs.put(manifest_path, manifest_json, overwrite=True)\n",
        "    \n",
        "    print(f\"\\nManifest created: {manifest_path}\")\n",
        "    print(f\"Total files: {len(entries)}\")\n",
        "    print(f\"Total size (compressed): {total_content_length / (1024**2):.2f} MB\")\n",
        "    print(f\"Total records: {total_record_count:,}\")\n",
        "    \n",
        "    return manifest\n",
        "\n",
        "create_manifest()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "language": "python",
      "notebookMetadata": {},
      "notebookName": "export_concepts"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}