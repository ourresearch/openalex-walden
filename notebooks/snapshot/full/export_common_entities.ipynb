{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "s3_base_path = f\"s3://openalex-snapshots/full/{date_str}\"\n",
    "\n",
    "ENTITIES = [\n",
    "    {\n",
    "        \"name\": \"continents\",\n",
    "        \"source_table\": \"openalex.common.continents_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_continents_snapshot\",\n",
    "        \"array_columns\": [\"display_name_alternatives\", \"countries\"],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"countries\",\n",
    "        \"source_table\": \"openalex.common.countries_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_countries_snapshot\",\n",
    "        \"array_columns\": [\"display_name_alternatives\"],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"institution-types\",\n",
    "        \"source_table\": \"openalex.common.institution_types_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_institution_types_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"languages\",\n",
    "        \"source_table\": \"openalex.common.languages_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_languages_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"licenses\",\n",
    "        \"source_table\": \"openalex.common.licenses_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_licenses_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sdgs\",\n",
    "        \"source_table\": \"openalex.common.sdgs_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_sdgs_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"source-types\",\n",
    "        \"source_table\": \"openalex.common.source_types_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_source_types_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"work-types\",\n",
    "        \"source_table\": \"openalex.common.work_types_api\",\n",
    "        \"snapshot_table\": \"openalex.common.openalex_work_types_snapshot\",\n",
    "        \"array_columns\": [],\n",
    "    },\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export common entities to S3 snapshot\n",
    "Exports continents, countries, institution-types, languages, licenses, sdgs, source-types, and work-types as gzip JSON lines to S3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rename_files_and_cleanup(output_path):\n",
    "    \"\"\"Rename partition directories and files for consistency.\"\"\"\n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = [p for p in partitions if p.name.startswith(\"_partition_date=\")]\n",
    "\n",
    "    for partition in partitions_to_process:\n",
    "        date_value = partition.name.replace(\"_partition_date=\", \"\").rstrip(\"/\")\n",
    "        new_partition_path = f\"{output_path}/updated_date={date_value}/\"\n",
    "\n",
    "        files = dbutils.fs.ls(partition.path)\n",
    "        json_files = sorted([f for f in files if f.name.endswith('.gz')], key=lambda x: x.name)\n",
    "\n",
    "        for idx, file_info in enumerate(json_files):\n",
    "            new_name = f\"part_{str(idx).zfill(4)}.gz\"\n",
    "            new_path = f\"{new_partition_path}{new_name}\"\n",
    "            dbutils.fs.mv(file_info.path, new_path)\n",
    "            print(f\"  Moved {file_info.name} -> updated_date={date_value}/{new_name}\")\n",
    "\n",
    "        for f in files:\n",
    "            if not f.name.endswith('.gz'):\n",
    "                try:\n",
    "                    dbutils.fs.rm(f.path)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            dbutils.fs.rm(partition.path, recurse=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Clean up root-level Spark metadata\n",
    "    try:\n",
    "        root_files = dbutils.fs.ls(output_path)\n",
    "        for f in root_files:\n",
    "            if f.name.startswith(\"_\"):\n",
    "                dbutils.fs.rm(f.path, recurse=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_manifest(output_path, entity_type):\n",
    "    \"\"\"Create a manifest file with all file metadata.\"\"\"\n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = sorted(\n",
    "        [p for p in partitions if p.name.startswith(\"updated_date=\")],\n",
    "        key=lambda x: x.name, reverse=True\n",
    "    )\n",
    "\n",
    "    entries = []\n",
    "    total_content_length = 0\n",
    "    total_record_count = 0\n",
    "\n",
    "    for partition in partitions_to_process:\n",
    "        files = dbutils.fs.ls(partition.path)\n",
    "        for file_info in files:\n",
    "            if not file_info.name.endswith('.gz'):\n",
    "                continue\n",
    "\n",
    "            record_count = spark.read.text(file_info.path).count()\n",
    "\n",
    "            raw = file_info.path.replace(\"dbfs:/\", \"s3://\")\n",
    "            marker = f\"/{entity_type}/\"\n",
    "            idx = raw.find(marker)\n",
    "            relative = raw[idx:]\n",
    "            s3_url = f\"s3://openalex/data{relative}\"\n",
    "\n",
    "            entries.append({\n",
    "                \"url\": s3_url,\n",
    "                \"meta\": {\n",
    "                    \"content_length\": file_info.size,\n",
    "                    \"record_count\": record_count\n",
    "                }\n",
    "            })\n",
    "            total_content_length += file_info.size\n",
    "            total_record_count += record_count\n",
    "\n",
    "            print(f\"  {partition.name}{file_info.name}: {record_count:,} records, {file_info.size/(1024*1024):.2f} MB\")\n",
    "\n",
    "    entries.sort(key=lambda x: x[\"url\"])\n",
    "\n",
    "    manifest = {\n",
    "        \"entries\": entries,\n",
    "        \"meta\": {\n",
    "            \"content_length\": total_content_length,\n",
    "            \"record_count\": total_record_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "    manifest_path = f\"{output_path}/manifest\"\n",
    "    dbutils.fs.put(manifest_path, json.dumps(manifest, indent=2), overwrite=True)\n",
    "\n",
    "    print(f\"  Manifest: {len(entries)} files, {total_content_length / (1024**2):.2f} MB, {total_record_count:,} records\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for entity in ENTITIES:\n",
    "    name = entity[\"name\"]\n",
    "    output_path = f\"{s3_base_path}/{name}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Read and coalesce null arrays\n",
    "    df = spark.read.table(entity[\"source_table\"])\n",
    "    for col_name in entity[\"array_columns\"]:\n",
    "        df = df.withColumn(col_name, F.coalesce(F.col(col_name), F.array()))\n",
    "\n",
    "    # Write snapshot table\n",
    "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(entity[\"snapshot_table\"])\n",
    "    print(f\"  Snapshot table: {entity['snapshot_table']}\")\n",
    "\n",
    "    # Export to S3\n",
    "    df = spark.read.table(entity[\"snapshot_table\"])\n",
    "    record_count = df.count()\n",
    "    print(f\"  Records: {record_count:,}\")\n",
    "\n",
    "    df = df.withColumn(\"_partition_date\", F.coalesce(F.to_date(\"updated_date\"), F.col(\"created_date\"), F.current_date()))\n",
    "\n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"compression\", \"gzip\")\n",
    "       .partitionBy(\"_partition_date\")\n",
    "       .json(output_path))\n",
    "\n",
    "    # Rename and cleanup\n",
    "    rename_files_and_cleanup(output_path)\n",
    "\n",
    "    # Create manifest\n",
    "    create_manifest(output_path, name)\n",
    "\n",
    "    print(f\"  Done: {output_path}\")\n",
    "\n",
    "print(f\"\\nAll exports complete!\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "export_common_entities",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
