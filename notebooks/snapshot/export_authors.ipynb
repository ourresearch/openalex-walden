{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `openalex.authors.openalex_authors_snapshot` in same format as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit field whitelist matching elastic sync\n",
    "df_transformed = (\n",
    "    spark.read.table(\"openalex.authors.openalex_authors\")\n",
    "    # Transform id to full URL format\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/A\"), F.col(\"id\").cast(\"string\")))\n",
    "    # Limit topics and topic_share to first 5 (matching elastic sync)\n",
    "    .withColumn(\"topics\", F.slice(F.col(\"topics\"), 1, 5))\n",
    "    .withColumn(\"topic_share\", F.slice(F.col(\"topic_share\"), 1, 5))\n",
    "    # Coalesce null arrays to empty arrays\n",
    "    .withColumn(\"display_name_alternatives\", F.coalesce(F.col(\"display_name_alternatives\"), F.array()))\n",
    "    .withColumn(\"affiliations\", F.coalesce(F.col(\"affiliations\"), F.array()))\n",
    "    .withColumn(\"last_known_institutions\", F.coalesce(F.col(\"last_known_institutions\"), F.array()))\n",
    "    .withColumn(\"topics\", F.coalesce(F.col(\"topics\"), F.array()))\n",
    "    .withColumn(\"topic_share\", F.coalesce(F.col(\"topic_share\"), F.array()))\n",
    "    .withColumn(\"x_concepts\", F.coalesce(F.col(\"x_concepts\"), F.array()))\n",
    "    .withColumn(\"sources\", F.coalesce(F.col(\"sources\"), F.array()))\n",
    "    .withColumn(\"counts_by_year\", F.coalesce(F.col(\"counts_by_year\"), F.array()))\n",
    "    # Explicit field selection\n",
    "    .select(\n",
    "        \"id\",\n",
    "        \"display_name\",\n",
    "        \"display_name_alternatives\",\n",
    "        \"orcid\",\n",
    "        \"works_count\",\n",
    "        \"cited_by_count\",\n",
    "        \"summary_stats\",\n",
    "        \"ids\",\n",
    "        \"affiliations\",\n",
    "        \"last_known_institutions\",\n",
    "        \"topics\",\n",
    "        \"topic_share\",\n",
    "        \"x_concepts\",\n",
    "        \"sources\",\n",
    "        \"counts_by_year\",\n",
    "        \"works_api_url\",\n",
    "        \"updated_date\",\n",
    "        \"created_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_transformed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"openalex.authors.openalex_authors_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export in json lines format to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = \"authors\"\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RECORDS_PER_FILE = 400000\n",
    "s3_base_path = f\"s3://openalex-sandbox/snapshots/{date_str}\"\n",
    "output_path = f\"{s3_base_path}/{entity_type}\"\n",
    "\n",
    "def export():\n",
    "    print(f\"Starting export to: {output_path}\")\n",
    "    print(f\"Records per file: {RECORDS_PER_FILE:,}\")\n",
    "    \n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "    \n",
    "    df = spark.read.table(\"openalex.authors.openalex_authors_snapshot\")\n",
    "    df = df.withColumn(\"updated_date\", F.to_date(\"updated_date\"))\n",
    "    \n",
    "    df_with_count = df.join(\n",
    "        df.groupBy(\"updated_date\").count().withColumnRenamed(\"count\", \"date_count\"),\n",
    "        on=\"updated_date\"\n",
    "    )\n",
    "\n",
    "    date_stats = df_with_count.select(\"updated_date\", \"date_count\").distinct().orderBy(F.desc(\"date_count\")).collect()\n",
    "    print(\"\\nDate distribution (top 10):\")\n",
    "    for row in date_stats[:10]:\n",
    "        expected_files = (row['date_count'] + RECORDS_PER_FILE - 1) // RECORDS_PER_FILE\n",
    "        print(f\"  {row['updated_date']}: {row['date_count']:,} records → {expected_files} files expected\")\n",
    "    \n",
    "    # apply hash-based salting for predictable distribution\n",
    "    df_salted = df_with_count.withColumn(\n",
    "        \"salt\",\n",
    "        F.when(F.col(\"date_count\") > 100_000_000, F.abs(F.hash(\"id\")) % 1400)\n",
    "        .when(F.col(\"date_count\") > 40_000_000, F.abs(F.hash(\"id\")) % 160)\n",
    "        .when(F.col(\"date_count\") > 10_000_000, F.abs(F.hash(\"id\")) % 50)\n",
    "        .when(F.col(\"date_count\") > 5_000_000, F.abs(F.hash(\"id\")) % 25)\n",
    "        .when(F.col(\"date_count\") > 2_000_000, F.abs(F.hash(\"id\")) % 10)\n",
    "        .when(F.col(\"date_count\") > 800_000, F.abs(F.hash(\"id\")) % 3)\n",
    "        .otherwise(0)\n",
    "    ).drop(\"date_count\")\n",
    "    \n",
    "    print(\"\\nRepartitioning and writing to S3...\")\n",
    "    df_out = df_salted.repartition(F.col(\"updated_date\"), F.col(\"salt\")).drop(\"salt\")\n",
    "    \n",
    "    (df_out.write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"compression\", \"gzip\")\n",
    "         .option(\"maxRecordsPerFile\", RECORDS_PER_FILE)\n",
    "         .partitionBy(\"updated_date\")\n",
    "         .json(output_path))\n",
    "    \n",
    "    print(\"Export completed!\")\n",
    "\n",
    "export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the files into sequential numbers, remove spark metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_and_cleanup(output_path, max_workers=30):\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = [p for p in partitions if p.name.startswith(\"updated_date=\")]\n",
    "    \n",
    "    print(f\"Found {len(partitions_to_process)} partitions to process\")\n",
    "    \n",
    "    def process_single_partition_fast(partition):\n",
    "        \"\"\"Process large partitions with parallel renaming\"\"\"\n",
    "        try:\n",
    "            files = dbutils.fs.ls(partition.path)\n",
    "            \n",
    "            # categorize files\n",
    "            already_renamed = []\n",
    "            needs_renaming = []\n",
    "            metadata_files = []\n",
    "            \n",
    "            for f in files:\n",
    "                if f.name.startswith('part_') and f.name.endswith('.gz'):\n",
    "                    already_renamed.append(f)\n",
    "                elif f.name.endswith('.json.gz'):\n",
    "                    needs_renaming.append(f)\n",
    "                else:\n",
    "                    metadata_files.append(f)\n",
    "            \n",
    "            # sort by full name to preserve .c000, .c001, .c002 order\n",
    "            needs_renaming.sort(key=lambda x: x.name)\n",
    "            \n",
    "            if len(needs_renaming) == 0:\n",
    "                return partition.name, True, f\"{len(already_renamed)} files already renamed\"\n",
    "            \n",
    "            # find highest existing number\n",
    "            max_existing = -1\n",
    "            for f in already_renamed:\n",
    "                try:\n",
    "                    num_str = f.name.replace('part_', '').replace('.gz', '')\n",
    "                    max_existing = max(max_existing, int(num_str))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            start_idx = max_existing + 1\n",
    "            \n",
    "            # for large directories, use parallel renaming\n",
    "            if len(needs_renaming) > 100:\n",
    "                print(f\"  {partition.name}: Large directory ({len(needs_renaming)} files), using parallel rename...\")\n",
    "                \n",
    "                # pre-assign unique numbers to avoid conflicts\n",
    "                file_assignments = [(f, start_idx + i) for i, f in enumerate(needs_renaming)]\n",
    "                \n",
    "                counter_lock = threading.Lock()\n",
    "                counter = {'renamed': 0, 'errors': 0}\n",
    "                \n",
    "                def rename_single_file(file_info, file_number):\n",
    "                    try:\n",
    "                        new_name = f\"part_{str(file_number).zfill(4)}.gz\"\n",
    "                        new_path = f\"{partition.path}{new_name}\"\n",
    "                        dbutils.fs.mv(file_info.path, new_path)\n",
    "                        \n",
    "                        with counter_lock:\n",
    "                            counter['renamed'] += 1\n",
    "                            if counter['renamed'] % 100 == 0:\n",
    "                                print(f\"    {partition.name}: {counter['renamed']}/{len(needs_renaming)} renamed...\")\n",
    "                        return True\n",
    "                    except Exception as e:\n",
    "                        with counter_lock:\n",
    "                            counter['errors'] += 1\n",
    "                        return False\n",
    "                \n",
    "                # use 50 workers for large directories\n",
    "                with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "                    futures = [executor.submit(rename_single_file, f, num) \n",
    "                              for f, num in file_assignments]\n",
    "                    \n",
    "                    for future in as_completed(futures):\n",
    "                        future.result()\n",
    "                \n",
    "                renamed_count = counter['renamed']\n",
    "                \n",
    "            else:\n",
    "                # small directories - sequential is fine\n",
    "                renamed_count = 0\n",
    "                for idx, file_info in enumerate(needs_renaming):\n",
    "                    new_number = start_idx + idx\n",
    "                    new_name = f\"part_{str(new_number).zfill(4)}.gz\"\n",
    "                    new_path = f\"{partition.path}{new_name}\"\n",
    "                    \n",
    "                    try:\n",
    "                        dbutils.fs.mv(file_info.path, new_path)\n",
    "                        renamed_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error: {e}\")\n",
    "            \n",
    "            # clean up metadata files\n",
    "            cleanup_count = 0\n",
    "            for f in metadata_files:\n",
    "                try:\n",
    "                    dbutils.fs.rm(f.path)\n",
    "                    cleanup_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return partition.name, True, f\"{renamed_count} renamed, {len(already_renamed)} existing, {cleanup_count} cleaned\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return partition.name, False, str(e)\n",
    "    \n",
    "    # process partitions\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_single_partition_fast, p): p for p in partitions_to_process}\n",
    "        \n",
    "        completed = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            partition_name, success, message = future.result()\n",
    "            completed += 1\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✓ {partition_name}: {message} ({elapsed:.1f}s)\")\n",
    "            else:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✗ {partition_name}: Error - {message}\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "rename_files_and_cleanup(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manifest():\n",
    "    \"\"\"\n",
    "    Create a manifest file with all file metadata using parallel processing.\n",
    "    \"\"\"\n",
    "    output_path = f\"{s3_base_path}/{entity_type}\"\n",
    "    \n",
    "    print(f\"\\nCreating manifest...\")\n",
    "    \n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = sorted([p for p in partitions if p.name.startswith(\"updated_date=\")], \n",
    "                                   key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    def process_file(partition_name, file_info):\n",
    "        \"\"\"Process a single file to get its metadata\"\"\"\n",
    "        if not file_info.name.endswith('.gz'):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # count records in the file\n",
    "            record_count = spark.read.text(file_info.path).count()\n",
    "            \n",
    "            # set the s3 url to the prod s3 folder\n",
    "            raw = file_info.path.replace(\"dbfs:/\", \"s3://\")\n",
    "            marker = f\"/{entity_type}/\"\n",
    "            idx = raw.find(marker)\n",
    "            if idx == -1:\n",
    "                raise ValueError(f\"Could not find '{marker}' in path: {raw}\")\n",
    "            relative = raw[idx:]\n",
    "            s3_url = f\"s3://openalex/data{relative}\"\n",
    "\n",
    "            entry = {\n",
    "                \"url\": s3_url,\n",
    "                \"meta\": {\n",
    "                    \"content_length\": file_info.size,\n",
    "                    \"record_count\": record_count\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"entry\": entry,\n",
    "                \"partition\": partition_name,\n",
    "                \"file\": file_info.name,\n",
    "                \"size\": file_info.size,\n",
    "                \"count\": record_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {partition_name}{file_info.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # collect all file tasks\n",
    "    file_tasks = []\n",
    "    for partition in partitions_to_process:\n",
    "        files = dbutils.fs.ls(partition.path)\n",
    "        for file_info in files:\n",
    "            if file_info.name.endswith('.gz'):\n",
    "                file_tasks.append((partition.name, file_info))\n",
    "    \n",
    "    print(f\"Processing {len(file_tasks)} files across {len(partitions_to_process)} partitions...\")\n",
    "    \n",
    "    # process files in parallel\n",
    "    entries = []\n",
    "    total_content_length = 0\n",
    "    total_record_count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = {executor.submit(process_file, task[0], task[1]): task \n",
    "                  for task in file_tasks}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if result:\n",
    "                entries.append(result[\"entry\"])\n",
    "                total_content_length += result[\"size\"]\n",
    "                total_record_count += result[\"count\"]\n",
    "                \n",
    "                if completed % 50 == 0 or completed == len(file_tasks):\n",
    "                    print(f\"  Progress: {completed}/{len(file_tasks)} files processed...\")\n",
    "                \n",
    "                # print details for large files\n",
    "                if result[\"size\"] > 100 * 1024 * 1024:  # Files > 100MB\n",
    "                    print(f\"  {result['partition']}{result['file']}: \"\n",
    "                          f\"{result['count']:,} records, {result['size']/(1024*1024):.1f} MB\")\n",
    "    \n",
    "    entries.sort(key=lambda x: x[\"url\"])\n",
    "    \n",
    "    manifest = {\n",
    "        \"entries\": entries,\n",
    "        \"meta\": {\n",
    "            \"content_length\": total_content_length,\n",
    "            \"record_count\": total_record_count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    manifest_path = f\"{output_path}/manifest\"\n",
    "    manifest_json = json.dumps(manifest, indent=2)\n",
    "    dbutils.fs.put(manifest_path, manifest_json, overwrite=True)\n",
    "    \n",
    "    print(f\"\\nManifest created: {manifest_path}\")\n",
    "    print(f\"Total files: {len(entries)}\")\n",
    "    print(f\"Total size (compressed): {total_content_length / (1024**3):.2f} GB\")\n",
    "    print(f\"Total records: {total_record_count:,}\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "create_manifest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
