{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `openalex.authors.openalex_authors_snapshot` in same format as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explicit field whitelist matching elastic sync\ndf_transformed = (\n    spark.read.table(\"openalex.authors.openalex_authors\")\n    # Transform id to full URL format\n    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/A\"), F.col(\"id\").cast(\"string\")))\n    # Limit topics and topic_share to first 5 (matching elastic sync)\n    .withColumn(\"topics\", F.slice(F.col(\"topics\"), 1, 5))\n    .withColumn(\"topic_share\", F.slice(F.col(\"topic_share\"), 1, 5))\n    # Fix x_concepts: add URL prefix to id and rename col4 to level\n    .withColumn(\"x_concepts\", F.expr(\"\"\"\n        transform(x_concepts, c -> named_struct(\n            'id', concat('https://openalex.org/C', cast(c.id as string)),\n            'wikidata', c.wikidata,\n            'display_name', c.display_name,\n            'level', c.col4,\n            'score', c.score,\n            'count', c.count\n        ))\n    \"\"\"))\n    # Coalesce null arrays to empty arrays\n    .withColumn(\"display_name_alternatives\", F.coalesce(F.col(\"display_name_alternatives\"), F.array()))\n    .withColumn(\"affiliations\", F.coalesce(F.col(\"affiliations\"), F.array()))\n    .withColumn(\"last_known_institutions\", F.coalesce(F.col(\"last_known_institutions\"), F.array()))\n    .withColumn(\"topics\", F.coalesce(F.col(\"topics\"), F.array()))\n    .withColumn(\"topic_share\", F.coalesce(F.col(\"topic_share\"), F.array()))\n    .withColumn(\"x_concepts\", F.coalesce(F.col(\"x_concepts\"), F.array()))\n    .withColumn(\"sources\", F.coalesce(F.col(\"sources\"), F.array()))\n    .withColumn(\"counts_by_year\", F.coalesce(F.col(\"counts_by_year\"), F.array()))\n    # Explicit field selection\n    .select(\n        \"id\",\n        \"display_name\",\n        \"display_name_alternatives\",\n        \"orcid\",\n        \"works_count\",\n        \"cited_by_count\",\n        \"summary_stats\",\n        \"ids\",\n        \"affiliations\",\n        \"last_known_institutions\",\n        \"topics\",\n        \"topic_share\",\n        \"x_concepts\",\n        \"sources\",\n        \"counts_by_year\",\n        \"works_api_url\",\n        \"updated_date\",\n        \"created_date\"\n    )\n)\n\ndf_transformed.write \\\n    .mode(\"overwrite\") \\\n    .option(\"overwriteSchema\", \"true\") \\\n    .saveAsTable(\"openalex.authors.openalex_authors_snapshot\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export in json lines format to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "entity_type = \"authors\"\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\nRECORDS_PER_FILE = 400000\ns3_base_path = f\"s3://openalex-sandbox/snapshots/{date_str}\"\noutput_path = f\"{s3_base_path}/{entity_type}\"\n\ndef export():\n    print(f\"Starting export to: {output_path}\")\n    print(f\"Records per file: {RECORDS_PER_FILE:,}\")\n    \n    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n    \n    df = spark.read.table(\"openalex.authors.openalex_authors_snapshot\")\n    # Create partition column (this gets removed during partitionBy)\n    # Keep updated_date in the data for the JSON output\n    df = df.withColumn(\"_partition_date\", F.to_date(\"updated_date\"))\n    \n    # Use broadcast for the counts join - the aggregated counts table is tiny (~10K rows)\n    # This avoids a second full scan of the dataset\n    date_counts = df.groupBy(\"_partition_date\").count().withColumnRenamed(\"count\", \"date_count\")\n    df_with_count = df.join(F.broadcast(date_counts), on=\"_partition_date\")\n\n    date_stats = df_with_count.select(\"_partition_date\", \"date_count\").distinct().orderBy(F.desc(\"date_count\")).collect()\n    print(\"\\nDate distribution (top 10):\")\n    for row in date_stats[:10]:\n        expected_files = (row['date_count'] + RECORDS_PER_FILE - 1) // RECORDS_PER_FILE\n        print(f\"  {row['_partition_date']}: {row['date_count']:,} records → {expected_files} files expected\")\n    \n    # apply hash-based salting for predictable distribution\n    df_salted = df_with_count.withColumn(\n        \"salt\",\n        F.when(F.col(\"date_count\") > 100_000_000, F.abs(F.hash(\"id\")) % 1400)\n        .when(F.col(\"date_count\") > 40_000_000, F.abs(F.hash(\"id\")) % 160)\n        .when(F.col(\"date_count\") > 10_000_000, F.abs(F.hash(\"id\")) % 50)\n        .when(F.col(\"date_count\") > 5_000_000, F.abs(F.hash(\"id\")) % 25)\n        .when(F.col(\"date_count\") > 2_000_000, F.abs(F.hash(\"id\")) % 10)\n        .when(F.col(\"date_count\") > 800_000, F.abs(F.hash(\"id\")) % 3)\n        .otherwise(0)\n    ).drop(\"date_count\")\n    \n    print(\"\\nRepartitioning and writing to S3...\")\n    df_out = df_salted.repartition(F.col(\"_partition_date\"), F.col(\"salt\")).drop(\"salt\")\n    \n    (df_out.write\n         .mode(\"overwrite\")\n         .option(\"compression\", \"gzip\")\n         .option(\"maxRecordsPerFile\", RECORDS_PER_FILE)\n         .partitionBy(\"_partition_date\")\n         .json(output_path))\n    \n    print(\"Export completed!\")\n\nexport()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the files into sequential numbers, remove spark metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rename_files_and_cleanup(output_path, max_workers=30):\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    import threading\n    import time\n\n    partitions = dbutils.fs.ls(output_path)\n    partitions_to_process = [p for p in partitions if p.name.startswith(\"_partition_date=\")]\n    \n    print(f\"Found {len(partitions_to_process)} partitions to process\")\n    print(\"Will rename directories from _partition_date= to updated_date= during file processing\")\n    \n    def process_single_partition_fast(partition):\n        \"\"\"Process partition: rename files and move to updated_date= directory\"\"\"\n        try:\n            # Determine new directory path\n            date_value = partition.name.replace(\"_partition_date=\", \"\").rstrip(\"/\")\n            new_partition_path = f\"{output_path}/updated_date={date_value}/\"\n            \n            files = dbutils.fs.ls(partition.path)\n            \n            # categorize files\n            json_files = []\n            metadata_files = []\n            \n            for f in files:\n                if f.name.endswith('.json.gz') or (f.name.startswith('part_') and f.name.endswith('.gz')):\n                    json_files.append(f)\n                else:\n                    metadata_files.append(f)\n            \n            # sort by full name to preserve order\n            json_files.sort(key=lambda x: x.name)\n            \n            if len(json_files) == 0:\n                # Clean up empty partition\n                for f in metadata_files:\n                    try:\n                        dbutils.fs.rm(f.path)\n                    except:\n                        pass\n                try:\n                    dbutils.fs.rm(partition.path, recurse=True)\n                except:\n                    pass\n                return partition.name, True, \"empty partition cleaned up\"\n            \n            # Move and rename files to new partition path\n            if len(json_files) > 100:\n                print(f\"  {partition.name}: Large directory ({len(json_files)} files), using parallel processing...\")\n                \n                file_assignments = [(f, i) for i, f in enumerate(json_files)]\n                \n                counter_lock = threading.Lock()\n                counter = {'moved': 0, 'errors': 0}\n                \n                def move_single_file(file_info, file_number):\n                    try:\n                        new_name = f\"part_{str(file_number).zfill(4)}.gz\"\n                        new_path = f\"{new_partition_path}{new_name}\"\n                        dbutils.fs.mv(file_info.path, new_path)\n                        \n                        with counter_lock:\n                            counter['moved'] += 1\n                            if counter['moved'] % 100 == 0:\n                                print(f\"    {partition.name}: {counter['moved']}/{len(json_files)} moved...\")\n                        return True\n                    except Exception as e:\n                        with counter_lock:\n                            counter['errors'] += 1\n                        return False\n                \n                with ThreadPoolExecutor(max_workers=50) as executor:\n                    futures = [executor.submit(move_single_file, f, num) \n                              for f, num in file_assignments]\n                    \n                    for future in as_completed(futures):\n                        future.result()\n                \n                moved_count = counter['moved']\n                \n            else:\n                # small directories - sequential\n                moved_count = 0\n                for idx, file_info in enumerate(json_files):\n                    new_name = f\"part_{str(idx).zfill(4)}.gz\"\n                    new_path = f\"{new_partition_path}{new_name}\"\n                    \n                    try:\n                        dbutils.fs.mv(file_info.path, new_path)\n                        moved_count += 1\n                    except Exception as e:\n                        print(f\"    Error: {e}\")\n            \n            # clean up metadata files and old partition directory\n            for f in metadata_files:\n                try:\n                    dbutils.fs.rm(f.path)\n                except:\n                    pass\n            \n            # Remove old empty partition directory\n            try:\n                dbutils.fs.rm(partition.path, recurse=True)\n            except:\n                pass\n            \n            return partition.name, True, f\"{moved_count} files moved to updated_date={date_value}\"\n            \n        except Exception as e:\n            return partition.name, False, str(e)\n    \n    # process partitions\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = {executor.submit(process_single_partition_fast, p): p for p in partitions_to_process}\n        \n        completed = 0\n        start_time = time.time()\n        \n        for future in as_completed(futures):\n            partition_name, success, message = future.result()\n            completed += 1\n            elapsed = time.time() - start_time\n            \n            if success:\n                print(f\"  [{completed}/{len(partitions_to_process)}] ✓ {partition_name}: {message} ({elapsed:.1f}s)\")\n            else:\n                print(f\"  [{completed}/{len(partitions_to_process)}] ✗ {partition_name}: Error - {message}\")\n    \n    print(f\"\\nTotal time: {time.time() - start_time:.1f} seconds\")\n\nrename_files_and_cleanup(output_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_manifest():\n    \"\"\"\n    Create a manifest file with all file metadata using parallel processing.\n    \"\"\"\n    output_path = f\"{s3_base_path}/{entity_type}\"\n    \n    print(f\"\\nCreating manifest...\")\n    \n    partitions = dbutils.fs.ls(output_path)\n    partitions_to_process = sorted([p for p in partitions if p.name.startswith(\"updated_date=\")], \n                                   key=lambda x: x.name, reverse=True)\n    \n    def process_file(partition_name, file_info):\n        \"\"\"Process a single file to get its metadata\"\"\"\n        if not file_info.name.endswith('.gz'):\n            return None\n            \n        try:\n            # count records in the file\n            record_count = spark.read.text(file_info.path).count()\n            \n            # set the s3 url to the prod s3 folder\n            raw = file_info.path.replace(\"dbfs:/\", \"s3://\")\n            marker = f\"/{entity_type}/\"\n            idx = raw.find(marker)\n            if idx == -1:\n                raise ValueError(f\"Could not find '{marker}' in path: {raw}\")\n            relative = raw[idx:]\n            s3_url = f\"s3://openalex/data{relative}\"\n\n            entry = {\n                \"url\": s3_url,\n                \"meta\": {\n                    \"content_length\": file_info.size,\n                    \"record_count\": record_count\n                }\n            }\n            \n            return {\n                \"entry\": entry,\n                \"partition\": partition_name,\n                \"file\": file_info.name,\n                \"size\": file_info.size,\n                \"count\": record_count\n            }\n        except Exception as e:\n            print(f\"Error processing {partition_name}{file_info.name}: {e}\")\n            return None\n    \n    # collect all file tasks\n    file_tasks = []\n    for partition in partitions_to_process:\n        files = dbutils.fs.ls(partition.path)\n        for file_info in files:\n            if file_info.name.endswith('.gz'):\n                file_tasks.append((partition.name, file_info))\n    \n    print(f\"Processing {len(file_tasks)} files across {len(partitions_to_process)} partitions...\")\n    \n    # process files in parallel\n    entries = []\n    total_content_length = 0\n    total_record_count = 0\n    \n    with ThreadPoolExecutor(max_workers=50) as executor:\n        futures = {executor.submit(process_file, task[0], task[1]): task \n                  for task in file_tasks}\n        \n        completed = 0\n        for future in as_completed(futures):\n            result = future.result()\n            completed += 1\n            \n            if result:\n                entries.append(result[\"entry\"])\n                total_content_length += result[\"size\"]\n                total_record_count += result[\"count\"]\n                \n                if completed % 50 == 0 or completed == len(file_tasks):\n                    print(f\"  Progress: {completed}/{len(file_tasks)} files processed...\")\n                \n                # print details for large files\n                if result[\"size\"] > 100 * 1024 * 1024:  # Files > 100MB\n                    print(f\"  {result['partition']}{result['file']}: \"\n                          f\"{result['count']:,} records, {result['size']/(1024*1024):.1f} MB\")\n    \n    entries.sort(key=lambda x: x[\"url\"])\n    \n    manifest = {\n        \"entries\": entries,\n        \"meta\": {\n            \"content_length\": total_content_length,\n            \"record_count\": total_record_count\n        }\n    }\n    \n    manifest_path = f\"{output_path}/manifest\"\n    manifest_json = json.dumps(manifest, indent=2)\n    dbutils.fs.put(manifest_path, manifest_json, overwrite=True)\n    \n    print(f\"\\nManifest created: {manifest_path}\")\n    print(f\"Total files: {len(entries)}\")\n    print(f\"Total size (compressed): {total_content_length / (1024**3):.2f} GB\")\n    print(f\"Total records: {total_record_count:,}\")\n    \n    return manifest\n\ncreate_manifest()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}