{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Snapshot â€” Works\n",
    "Exports works updated on the snapshot day to JSONL and Parquet.\n",
    "Ports the full 52-field transformation from the full snapshot.\n",
    "\n",
    "For JSONL, uses hash-based salting when daily volume exceeds 10M records\n",
    "to control file sizes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType, MapType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "date_str = get_snapshot_date()\n",
    "print(f\"Snapshot date: {date_str}\")\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def truncate_abstract_index_string(raw_json: str, max_bytes: int = 32760) -> str:\n",
    "    try:\n",
    "        if not raw_json:\n",
    "            return None\n",
    "        try:\n",
    "            json.loads(raw_json)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            return None\n",
    "        if len(raw_json) <= (max_bytes // 4):\n",
    "            return raw_json\n",
    "        encoded = raw_json.encode('utf-8')\n",
    "        if len(encoded) <= max_bytes:\n",
    "            return raw_json\n",
    "        truncated = encoded[:max_bytes].decode('utf-8', errors='ignore')\n",
    "        last_bracket = truncated.rfind(']')\n",
    "        if last_bracket == -1:\n",
    "            return None\n",
    "        return truncated[:last_bracket + 1] + '}'\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sanitize_name(col_name: str):\n",
    "    unwanted_chars_pattern = r\"[^\\p{L}\\p{N}\\p{P}\\p{S}\\p{Z}]\"\n",
    "    multiple_spaces_pattern = r\"\\s+\"\n",
    "    return F.trim(\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(F.col(col_name), unwanted_chars_pattern, \"\"),\n",
    "            multiple_spaces_pattern, \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def sanitize_string(col_name: str, max_len: int = 32000):\n",
    "    return F.when(F.col(col_name).isNotNull(), F.substring(F.col(col_name), 1, max_len)).otherwise(None)\n",
    "\n",
    "\n",
    "empty_sdg_array = F.array().cast(\"array<struct<id:string,display_name:string,score:double>>\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read daily slice and apply full 52-field transformation\n",
    "df_raw = get_daily_df(spark, \"openalex.works.openalex_works\", date_str)\n",
    "\n",
    "df_transformed = (\n",
    "    df_raw\n",
    "    .withColumn(\"display_name\", F.col(\"title\"))\n",
    "    .withColumn(\"created_date\", F.to_timestamp(\"created_date\"))\n",
    "    .withColumn(\"updated_date\", F.to_timestamp(\"updated_date\"))\n",
    "    .withColumn(\"publication_date\", F.to_date(\"publication_date\"))\n",
    "    .withColumn(\n",
    "        \"concepts\",\n",
    "        F.transform(\n",
    "            F.col(\"concepts\"),\n",
    "            lambda c: F.struct(\n",
    "                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n",
    "                c.wikidata.alias(\"wikidata\"),\n",
    "                c.display_name.alias(\"display_name\"),\n",
    "                c.level.alias(\"level\"),\n",
    "                c.score.alias(\"score\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"created_date\",\n",
    "        F.when(\n",
    "            F.col(\"created_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"created_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"updated_date\",\n",
    "        F.when(\n",
    "            F.col(\"updated_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"updated_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"publication_date\",\n",
    "        F.when(\n",
    "            F.col(\"publication_date\").between(F.lit(\"1000-01-01\"), F.lit(\"2050-12-31\")),\n",
    "            F.col(\"publication_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    "    .withColumn(\"publication_year\", F.year(\"publication_date\"))\n",
    "    .withColumn(\"title\", sanitize_name(\"title\"))\n",
    "    .withColumn(\"display_name\", sanitize_name(\"display_name\"))\n",
    "    .withColumn(\"ids\",\n",
    "        F.transform_values(\"ids\",\n",
    "            lambda k, v: F.when(k == \"doi\",\n",
    "                    F.concat(F.lit(\"https://doi.org/\"), v)).otherwise(v)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"doi\", sanitize_string(\"doi\"))\n",
    "    .withColumn(\"language\", sanitize_string(\"language\"))\n",
    "    .withColumn(\"type\", sanitize_string(\"type\"))\n",
    "    .withColumn(\"abstract\", sanitize_string(\"abstract\"))\n",
    "    .withColumn(\"referenced_works\",\n",
    "                F.expr(\"transform(referenced_works, x -> 'https://openalex.org/W' || x)\"))\n",
    "    .withColumn(\"referenced_works_count\",\n",
    "                F.when(F.col(\"referenced_works\").isNotNull(), F.size(\"referenced_works\")).otherwise(0))\n",
    "    .withColumn(\"abstract_inverted_index\", truncate_abstract_index_string(F.col(\"abstract_inverted_index\")))\n",
    "    .withColumn(\"open_access\", F.struct(\n",
    "        F.col(\"open_access.is_oa\"),\n",
    "        sanitize_string(\"open_access.oa_status\").alias(\"oa_status\"),\n",
    "        F.col(\"open_access.any_repository_has_fulltext\"),\n",
    "        F.col(\"open_access.oa_url\")\n",
    "    ))\n",
    "    .withColumn(\"authorships\", F.expr(\"\"\"\n",
    "        transform(authorships, x -> named_struct(\n",
    "            'author', x.author,\n",
    "            'affiliations', x.affiliations,\n",
    "            'countries', x.countries,\n",
    "            'raw_author_name', substring(x.raw_author_name, 1, 32000),\n",
    "            'is_corresponding', x.is_corresponding,\n",
    "            'raw_affiliation_strings', transform(x.raw_affiliation_strings, aff -> substring(aff, 1, 32000)),\n",
    "            'institutions', x.institutions\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"locations\", F.expr(\"\"\"\n",
    "        transform(locations, x -> named_struct(\n",
    "            'native_id', x.native_id,\n",
    "            'source', x.source,\n",
    "            'is_oa', x.is_oa,\n",
    "            'is_published', x.version = 'publishedVersion',\n",
    "            'landing_page_url', substring(x.landing_page_url, 1, 32000),\n",
    "            'pdf_url', substring(x.pdf_url, 1, 32000),\n",
    "            'raw_source_name', x.raw_source_name,\n",
    "            'raw_type', x.raw_type,\n",
    "            'provenance', x.provenance,\n",
    "            'license', x.license,\n",
    "            'license_id', x.license_id,\n",
    "            'version', x.version,\n",
    "            'is_accepted', x.is_accepted\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"concepts\", F.slice(F.col(\"concepts\"), 1, 40))\n",
    "    .withColumn(\"indexed_in\", F.expr(\"\"\"\n",
    "        array_sort(\n",
    "            array_distinct(\n",
    "                array_compact(\n",
    "                    flatten(\n",
    "                        TRANSFORM(locations, loc ->\n",
    "                            CASE\n",
    "                            WHEN loc.provenance IN ('crossref', 'pubmed', 'datacite')\n",
    "                                THEN array(loc.provenance, IF(loc.source.is_in_doaj, 'doaj', NULL))\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:arxiv.org%'\n",
    "                                THEN array('arxiv')\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:doaj.org/%'\n",
    "                                THEN array('doaj')\n",
    "                            WHEN loc.provenance = 'mag' AND lower(loc.source.display_name) = 'pubmed'\n",
    "                                THEN array('pubmed')\n",
    "                            ELSE array()\n",
    "                            END\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \"\"\"))\n",
    "    .withColumn(\"has_fulltext\", F.col(\"fulltext\").isNotNull())\n",
    "    .withColumn(\"corresponding_author_ids\", F.coalesce(F.col(\"corresponding_author_ids\"), F.lit([])))\n",
    "    .withColumn(\"corresponding_institution_ids\", F.coalesce(F.col(\"corresponding_institution_ids\"), F.lit([])))\n",
    "    .withColumn(\"sustainable_development_goals\", F.coalesce(F.col(\"sustainable_development_goals\"), empty_sdg_array))\n",
    "    .withColumn(\"related_works\", F.coalesce(F.col(\"related_works\"), F.lit([])))\n",
    "    .withColumn(\"fwci\", F.coalesce(F.col(\"fwci\"), F.lit(0)))\n",
    "    .withColumn(\"mesh\", F.coalesce(F.col(\"mesh\"), F.lit([])))\n",
    "    .withColumn(\"authorships\", F.coalesce(F.col(\"authorships\"), F.lit([])))\n",
    "    .select(\n",
    "        \"id\",\n",
    "        \"doi\",\n",
    "        \"title\",\n",
    "        \"display_name\",\n",
    "        \"ids\",\n",
    "        \"indexed_in\",\n",
    "        \"publication_date\",\n",
    "        \"publication_year\",\n",
    "        \"language\",\n",
    "        \"type\",\n",
    "        \"authorships\",\n",
    "        \"authors_count\",\n",
    "        \"corresponding_author_ids\",\n",
    "        \"corresponding_institution_ids\",\n",
    "        \"primary_topic\",\n",
    "        \"topics\",\n",
    "        \"keywords\",\n",
    "        \"concepts\",\n",
    "        \"locations\",\n",
    "        \"locations_count\",\n",
    "        \"primary_location\",\n",
    "        \"best_oa_location\",\n",
    "        \"sustainable_development_goals\",\n",
    "        \"awards\",\n",
    "        \"funders\",\n",
    "        \"institutions\",\n",
    "        \"countries_distinct_count\",\n",
    "        \"institutions_distinct_count\",\n",
    "        \"open_access\",\n",
    "        \"is_paratext\",\n",
    "        \"is_retracted\",\n",
    "        \"is_xpac\",\n",
    "        \"biblio\",\n",
    "        \"abstract\",\n",
    "        \"referenced_works\",\n",
    "        \"referenced_works_count\",\n",
    "        \"related_works\",\n",
    "        \"abstract_inverted_index\",\n",
    "        \"cited_by_count\",\n",
    "        \"counts_by_year\",\n",
    "        \"apc_list\",\n",
    "        \"apc_paid\",\n",
    "        \"fwci\",\n",
    "        \"citation_normalized_percentile\",\n",
    "        \"cited_by_percentile_year\",\n",
    "        \"mesh\",\n",
    "        \"has_abstract\",\n",
    "        \"has_content\",\n",
    "        \"has_fulltext\",\n",
    "        \"created_date\",\n",
    "        \"updated_date\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Transformation complete for {date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to all formats\n",
    "For works, we use `export_all_formats` which caches the DataFrame,\n",
    "writes JSONL (with abstract_inverted_index parsed to map) and Parquet,\n",
    "then writes per-entity metadata for each."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_all_formats(\n",
    "    spark, dbutils, df_transformed, date_str, \"works\",\n",
    "    jsonl_records_per_file=500_000,\n",
    "    columnar_records_per_file=500_000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "language": "python",
   "notebookName": "export_works",
   "notebookOrigID": 0
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
