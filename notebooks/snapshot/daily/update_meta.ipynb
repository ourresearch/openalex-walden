{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Update daily snapshot metadata\nRuns after all export tasks complete.\n1. Reads per-entity metadata from `_meta/{format}/{entity}.json` and builds a combined `manifest.json` per format at `daily/{date}/{format}/manifest.json`\n2. Updates `daily/latest.json` with list of available dates (used by API for date listing)\n3. Cleans up `_meta/` and `_temp/` directories"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datetime import datetime, timedelta, timezone\n\ndate_str = get_snapshot_date()\nprint(f\"Snapshot date: {date_str}\")\n\nENTITIES = [\n    \"works\", \"authors\", \"institutions\", \"sources\", \"publishers\", \"funders\",\n    \"topics\", \"subfields\", \"fields\", \"domains\",\n    \"concepts\", \"keywords\", \"awards\",\n    \"continents\", \"countries\", \"institution-types\", \"languages\",\n    \"licenses\", \"sdgs\", \"source-types\", \"work-types\",\n]\n\nFORMATS = [\"jsonl\", \"parquet\"]\n\n# Build combined per-format manifests from _meta/ files\nfor fmt in FORMATS:\n    combined_entries = []\n    for entity in ENTITIES:\n        meta_path = f\"{S3_BASE}/{date_str}/_meta/{fmt}/{entity}.json\"\n        try:\n            content = dbutils.fs.head(meta_path, 65536)\n            meta = json.loads(content)\n        except Exception:\n            meta = {\"entity\": entity, \"filename\": None, \"record_count\": 0, \"content_length\": 0}\n\n        # Build file list (single file or empty for zero-record entities)\n        files = []\n        if meta.get(\"filename\"):\n            files.append({\n                \"url\": f\"s3://{S3_BUCKET}/daily/{date_str}/{fmt}/{meta['filename']}\",\n                \"meta\": {\n                    \"content_length\": meta[\"content_length\"],\n                    \"record_count\": meta[\"record_count\"],\n                }\n            })\n\n        combined_entries.append({\n            \"entity\": entity,\n            \"record_count\": meta[\"record_count\"],\n            \"content_length\": meta[\"content_length\"],\n            \"files\": files,\n        })\n\n    total_records = sum(e[\"record_count\"] for e in combined_entries)\n    total_size = sum(e[\"content_length\"] for e in combined_entries)\n\n    combined_manifest = {\n        \"date\": date_str,\n        \"format\": fmt,\n        \"meta\": {\n            \"record_count\": total_records,\n            \"content_length\": total_size,\n        },\n        \"entities\": combined_entries,\n    }\n\n    combined_path = f\"{S3_BASE}/{date_str}/{fmt}/manifest.json\"\n    dbutils.fs.put(combined_path, json.dumps(combined_manifest, indent=2), overwrite=True)\n    print(f\"{fmt}: {total_records:,} records, {total_size / (1024**2):.1f} MB across {len(ENTITIES)} entities\")\n\n# Clean up _meta/ directory\ntry:\n    dbutils.fs.rm(f\"{S3_BASE}/{date_str}/_meta\", recurse=True)\n    print(f\"\\nCleaned up _meta/ directory\")\nexcept Exception:\n    print(f\"\\nWarning: could not clean up _meta/ directory\")\n\n# Clean up _temp/ directory\ntry:\n    dbutils.fs.rm(f\"{S3_BASE}/{date_str}/_temp\", recurse=True)\n    print(f\"Cleaned up _temp/ directory\")\nexcept Exception:\n    print(f\"Warning: could not clean up _temp/ directory\")\n\n# Update latest.json with available dates\nmeta_path = f\"{S3_BASE}/latest.json\"\ncutoff = (datetime.now(timezone.utc) - timedelta(days=60)).strftime(\"%Y-%m-%d\")\n\n# Read existing dates\ntry:\n    existing = json.loads(dbutils.fs.head(meta_path, 65536))\n    available_dates = existing.get(\"available_dates\", [])\nexcept Exception:\n    available_dates = []\n\n# Add today, prune old dates, deduplicate, sort descending\nif date_str not in available_dates:\n    available_dates.append(date_str)\navailable_dates = sorted([d for d in available_dates if d >= cutoff], reverse=True)\n\nlatest = {\"available_dates\": available_dates}\ndbutils.fs.put(meta_path, json.dumps(latest, indent=2), overwrite=True)\nprint(f\"\\nUpdated {meta_path} ({len(available_dates)} dates)\")\n\n# Print per-entity summary\nprint(f\"\\nPer-entity record counts:\")\nfor entry in combined_entries:\n    print(f\"  {entry['entity']:25s} {entry['record_count']:>14,}\")",
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "language": "python",
   "notebookName": "update_meta",
   "notebookOrigID": 0
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}