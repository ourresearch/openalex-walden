{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a5233b-3517-4b0a-8002-754e7c5a7f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType, MapType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5be9a94-c75d-480d-a534-08f4e18d636a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create `openalex.works.openalex_works_snapshot` in same format as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd232240-2869-4cc0-a951-92d8ae74ff14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def truncate_abstract_index_string(raw_json: str, max_bytes: int = 32760) -> str:\n",
    "    try:\n",
    "        if not raw_json:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            json.loads(raw_json)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            # Invalid JSON - return None\n",
    "            return None\n",
    "\n",
    "        if len(raw_json) <= (max_bytes // 4):\n",
    "            return raw_json\n",
    "\n",
    "        encoded = raw_json.encode('utf-8')\n",
    "        if len(encoded) <= max_bytes:\n",
    "            return raw_json\n",
    "\n",
    "        truncated = encoded[:max_bytes].decode('utf-8', errors='ignore')\n",
    "        last_bracket = truncated.rfind(']')\n",
    "        if last_bracket == -1:\n",
    "            return None\n",
    "\n",
    "        return truncated[:last_bracket + 1] + '}'\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sanitize_name(col_name: str):\n",
    "    \"\"\"\n",
    "    Cleans a string column by removing unwanted characters and normalizing whitespace.\n",
    "    Handles multilingual text by preserving letters, numbers, punctuation, and symbols from all Unicode scripts.\n",
    "    \"\"\"\n",
    "    unwanted_chars_pattern = r\"[^\\p{L}\\p{N}\\p{P}\\p{S}\\p{Z}]\"\n",
    "    multiple_spaces_pattern = r\"\\s+\"\n",
    "\n",
    "    return F.trim(\n",
    "        F.regexp_replace( \n",
    "            F.regexp_replace(F.col(col_name), unwanted_chars_pattern, \"\"),\n",
    "            multiple_spaces_pattern, \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def sanitize_string(col_name: str, max_len: int = 32000):\n",
    "    return F.when(F.col(col_name).isNotNull(), F.substring(F.col(col_name), 1, max_len)).otherwise(None)\n",
    "\n",
    "\n",
    "empty_sdg_array = F.array().cast(\"array<struct<id:string,display_name:string,score:double>>\")\n",
    "\n",
    "# Explicit field whitelist matching elastic sync's _source struct exactly\n",
    "df_transformed = (\n",
    "    spark.read.table(\"openalex.works.openalex_works\")\n",
    "    .withColumn(\"display_name\", F.col(\"title\"))\n",
    "    .withColumn(\"created_date\", F.to_timestamp(\"created_date\"))\n",
    "    .withColumn(\"updated_date\", F.to_timestamp(\"updated_date\"))\n",
    "    .withColumn(\"publication_date\", F.to_date(\"publication_date\"))\n",
    "    .withColumn(\n",
    "        \"concepts\",\n",
    "        F.transform(\n",
    "            F.col(\"concepts\"),\n",
    "            lambda c: F.struct(\n",
    "                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n",
    "                c.wikidata.alias(\"wikidata\"),\n",
    "                c.display_name.alias(\"display_name\"),\n",
    "                c.level.alias(\"level\"),\n",
    "                c.score.alias(\"score\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"created_date\",\n",
    "        F.when(\n",
    "            F.col(\"created_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"created_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"updated_date\",\n",
    "        F.when(\n",
    "            F.col(\"updated_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"updated_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"publication_date\",\n",
    "        F.when(\n",
    "            F.col(\"publication_date\").between(F.lit(\"1000-01-01\"), F.lit(\"2050-12-31\")),\n",
    "            F.col(\"publication_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    "    .withColumn(\"publication_year\", F.year(\"publication_date\"))\n",
    "    .withColumn(\"title\", sanitize_name(\"title\"))\n",
    "    .withColumn(\"display_name\", sanitize_name(\"display_name\"))\n",
    "    .withColumn(\"ids\", \n",
    "        F.transform_values(\"ids\",\n",
    "            lambda k, v: F.when(k == \"doi\", \n",
    "                    F.concat(F.lit(\"https://doi.org/\"),v)).otherwise(v)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"doi\", sanitize_string(\"doi\"))\n",
    "    .withColumn(\"language\", sanitize_string(\"language\"))\n",
    "    .withColumn(\"type\", sanitize_string(\"type\"))\n",
    "    .withColumn(\"abstract\", sanitize_string(\"abstract\"))\n",
    "    .withColumn(\"referenced_works\", \n",
    "                F.expr(\"transform(referenced_works, x -> 'https://openalex.org/W' || x)\"))\n",
    "    .withColumn(\"referenced_works_count\", \n",
    "                F.when(F.col(\"referenced_works\").isNotNull(), F.size(\"referenced_works\")).otherwise(0))\n",
    "    .withColumn(\"abstract_inverted_index\", truncate_abstract_index_string(F.col(\"abstract_inverted_index\")))\n",
    "    .withColumn(\"open_access\", F.struct(\n",
    "        F.col(\"open_access.is_oa\"),\n",
    "        sanitize_string(\"open_access.oa_status\").alias(\"oa_status\"),\n",
    "        F.lit(False).cast(\"boolean\").alias(\"any_repository_has_fulltext\"),\n",
    "        F.col(\"open_access.oa_url\")\n",
    "    ))\n",
    "    # Keep all authorships (no limit, unlike elastic sync which limits to 100)\n",
    "    .withColumn(\"authorships\", F.expr(\"\"\"\n",
    "        transform(authorships, x -> named_struct(\n",
    "            'author', x.author,\n",
    "            'affiliations', x.affiliations,\n",
    "            'countries', x.countries,\n",
    "            'raw_author_name', substring(x.raw_author_name, 1, 32000),\n",
    "            'is_corresponding', x.is_corresponding,\n",
    "            'raw_affiliation_strings', transform(x.raw_affiliation_strings, aff -> substring(aff, 1, 32000)),\n",
    "            'institutions', x.institutions\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    # Updated locations struct to match elastic sync (includes raw_type, version, is_accepted)\n",
    "    .withColumn(\"locations\", F.expr(\"\"\"\n",
    "        transform(locations, x -> named_struct(\n",
    "            'native_id', x.native_id,\n",
    "            'source', x.source,\n",
    "            'is_oa', x.is_oa,\n",
    "            'is_published', x.version = 'publishedVersion',\n",
    "            'landing_page_url', substring(x.landing_page_url, 1, 32000),\n",
    "            'pdf_url', substring(x.pdf_url, 1, 32000),\n",
    "            'raw_source_name', x.raw_source_name,\n",
    "            'raw_type', x.raw_type,\n",
    "            'provenance', x.provenance,\n",
    "            'license', x.license,\n",
    "            'license_id', x.license_id,\n",
    "            'version', x.version,\n",
    "            'is_accepted', x.is_accepted\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"concepts\", F.slice(F.col(\"concepts\"), 1, 40))\n",
    "    # indexed_in computation (matching elastic sync)\n",
    "    .withColumn(\"indexed_in\", F.expr(\"\"\"\n",
    "        array_sort(\n",
    "            array_distinct(\n",
    "                array_compact(\n",
    "                    flatten(\n",
    "                        TRANSFORM(locations, loc ->\n",
    "                            CASE\n",
    "                            WHEN loc.provenance IN ('crossref', 'pubmed', 'datacite')\n",
    "                                THEN array(loc.provenance, IF(loc.source.is_in_doaj, 'doaj', NULL))\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:arxiv.org%'\n",
    "                                THEN array('arxiv')\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:doaj.org/%'\n",
    "                                THEN array('doaj')\n",
    "                            WHEN loc.provenance = 'mag' AND lower(loc.source.display_name) = 'pubmed'\n",
    "                                THEN array('pubmed')\n",
    "                            ELSE array()\n",
    "                            END\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \"\"\"))\n",
    "    # has_fulltext derived column (matching elastic sync)\n",
    "    .withColumn(\"has_fulltext\", F.col(\"fulltext\").isNotNull())\n",
    "    # Coalesce null arrays to empty arrays (matching elastic sync)\n",
    "    .withColumn(\"corresponding_author_ids\", F.coalesce(F.col(\"corresponding_author_ids\"), F.lit([])))\n",
    "    .withColumn(\"corresponding_institution_ids\", F.coalesce(F.col(\"corresponding_institution_ids\"), F.lit([])))\n",
    "    .withColumn(\"sustainable_development_goals\", F.coalesce(F.col(\"sustainable_development_goals\"), empty_sdg_array))\n",
    "    .withColumn(\"related_works\", F.coalesce(F.col(\"related_works\"), F.lit([])))\n",
    "    .withColumn(\"fwci\", F.coalesce(F.col(\"fwci\"), F.lit(0)))\n",
    "    .withColumn(\"mesh\", F.coalesce(F.col(\"mesh\"), F.lit([])))\n",
    "    .withColumn(\"authorships\", F.coalesce(F.col(\"authorships\"), F.lit([])))\n",
    "    # Explicit field selection (52 fields matching elastic sync, excluding indexed_timestamp)\n",
    "    .select(\n",
    "        \"id\",\n",
    "        \"doi\",\n",
    "        \"title\",\n",
    "        \"display_name\",\n",
    "        \"ids\",\n",
    "        \"indexed_in\",\n",
    "        \"publication_date\",\n",
    "        \"publication_year\",\n",
    "        \"language\",\n",
    "        \"type\",\n",
    "        \"authorships\",\n",
    "        \"authors_count\",\n",
    "        \"corresponding_author_ids\",\n",
    "        \"corresponding_institution_ids\",\n",
    "        \"primary_topic\",\n",
    "        \"topics\",\n",
    "        \"keywords\",\n",
    "        \"concepts\",\n",
    "        \"locations\",\n",
    "        \"locations_count\",\n",
    "        \"primary_location\",\n",
    "        \"best_oa_location\",\n",
    "        \"sustainable_development_goals\",\n",
    "        \"awards\",\n",
    "        \"funders\",\n",
    "        \"institutions\",\n",
    "        \"countries_distinct_count\",\n",
    "        \"institutions_distinct_count\",\n",
    "        \"open_access\",\n",
    "        \"is_paratext\",\n",
    "        \"is_retracted\",\n",
    "        \"is_xpac\",\n",
    "        \"biblio\",\n",
    "        \"abstract\",\n",
    "        \"referenced_works\",\n",
    "        \"referenced_works_count\",\n",
    "        \"related_works\",\n",
    "        \"abstract_inverted_index\",\n",
    "        \"cited_by_count\",\n",
    "        \"counts_by_year\",\n",
    "        \"apc_list\",\n",
    "        \"apc_paid\",\n",
    "        \"fwci\",\n",
    "        \"citation_normalized_percentile\",\n",
    "        \"cited_by_percentile_year\",\n",
    "        \"mesh\",\n",
    "        \"has_abstract\",\n",
    "        \"has_content\",\n",
    "        \"fulltext\",\n",
    "        \"has_fulltext\",\n",
    "        \"created_date\",\n",
    "        \"updated_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_transformed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"openalex.works.openalex_works_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebdfef42-2c2c-4f50-96ba-df8a190c457a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Export in json lines format to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ba0c81-d769-40af-81e0-f56940103621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "entity_type = \"works\"\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\nRECORDS_PER_FILE = 400000\ns3_base_path = f\"s3://openalex-sandbox/snapshots/{date_str}\"\noutput_path = f\"{s3_base_path}/{entity_type}\"\n\ndef export():\n    print(f\"Starting export to: {output_path}\")\n    print(f\"Records per file: {RECORDS_PER_FILE:,}\")\n    \n    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n    \n    df = spark.read.table(\"openalex.works.openalex_works_snapshot\")\n    df = df.withColumn(\"updated_date\", F.to_date(\"updated_date\"))\n\n    # parse abstract_inverted_index so it's not escaped\n    df = df.withColumn(\n        \"abstract_inverted_index\",\n        F.from_json(\n            F.col(\"abstract_inverted_index\"),\n            MapType(StringType(), ArrayType(IntegerType()))\n        )\n    )\n\n    # No longer need to drop fields - explicit select in cell-2 handles field selection\n    \n    df_with_count = df.join(\n        df.groupBy(\"updated_date\").count().withColumnRenamed(\"count\", \"date_count\"),\n        on=\"updated_date\"\n    )\n\n    date_stats = df_with_count.select(\"updated_date\", \"date_count\").distinct().orderBy(F.desc(\"date_count\")).collect()\n    print(\"\\nDate distribution (top 10):\")\n    for row in date_stats[:10]:\n        expected_files = (row['date_count'] + RECORDS_PER_FILE - 1) // RECORDS_PER_FILE\n        print(f\"  {row['updated_date']}: {row['date_count']:,} records → {expected_files} files expected\")\n    \n    # apply hash-based salting for predictable distribution\n    df_salted = df_with_count.withColumn(\n        \"salt\",\n        F.when(F.col(\"date_count\") > 100_000_000, F.abs(F.hash(\"id\")) % 1400)\n        .when(F.col(\"date_count\") > 40_000_000, F.abs(F.hash(\"id\")) % 160)\n        .when(F.col(\"date_count\") > 10_000_000, F.abs(F.hash(\"id\")) % 50)\n        .when(F.col(\"date_count\") > 5_000_000, F.abs(F.hash(\"id\")) % 25)\n        .when(F.col(\"date_count\") > 2_000_000, F.abs(F.hash(\"id\")) % 10)\n        .when(F.col(\"date_count\") > 800_000, F.abs(F.hash(\"id\")) % 3)\n        .otherwise(0)\n    ).drop(\"date_count\")\n    \n    print(\"\\nRepartitioning and writing to S3...\")\n    df_out = df_salted.repartition(F.col(\"updated_date\"), F.col(\"salt\")).drop(\"salt\")\n    \n    (df_out.write\n         .mode(\"overwrite\")\n         .option(\"compression\", \"gzip\")\n         .option(\"maxRecordsPerFile\", RECORDS_PER_FILE)\n         .partitionBy(\"updated_date\")\n         .json(output_path))\n    \n    print(\"Export completed!\")\n\nexport()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b123b1de-ed0b-43fe-bbae-3b24427fd2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Rename the files into sequential numbers, remove spark metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3138c66-09b3-4eab-af3a-27eb1d6d90ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_files_and_cleanup(output_path, max_workers=30):\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = [p for p in partitions if p.name.startswith(\"updated_date=\")]\n",
    "    \n",
    "    print(f\"Found {len(partitions_to_process)} partitions to process\")\n",
    "    \n",
    "    def process_single_partition_fast(partition):\n",
    "        \"\"\"Process large partitions with parallel renaming\"\"\"\n",
    "        try:\n",
    "            files = dbutils.fs.ls(partition.path)\n",
    "            \n",
    "            # categorize files\n",
    "            already_renamed = []\n",
    "            needs_renaming = []\n",
    "            metadata_files = []\n",
    "            \n",
    "            for f in files:\n",
    "                if f.name.startswith('part_') and f.name.endswith('.gz'):\n",
    "                    already_renamed.append(f)\n",
    "                elif f.name.endswith('.json.gz'):\n",
    "                    needs_renaming.append(f)\n",
    "                else:\n",
    "                    metadata_files.append(f)\n",
    "            \n",
    "            # sort by full name to preserve .c000, .c001, .c002 order\n",
    "            needs_renaming.sort(key=lambda x: x.name)\n",
    "            \n",
    "            if len(needs_renaming) == 0:\n",
    "                return partition.name, True, f\"{len(already_renamed)} files already renamed\"\n",
    "            \n",
    "            # find highest existing number\n",
    "            max_existing = -1\n",
    "            for f in already_renamed:\n",
    "                try:\n",
    "                    num_str = f.name.replace('part_', '').replace('.gz', '')\n",
    "                    max_existing = max(max_existing, int(num_str))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            start_idx = max_existing + 1\n",
    "            \n",
    "            # for large directories, use parallel renaming\n",
    "            if len(needs_renaming) > 100:\n",
    "                print(f\"  {partition.name}: Large directory ({len(needs_renaming)} files), using parallel rename...\")\n",
    "                \n",
    "                # pre-assign unique numbers to avoid conflicts\n",
    "                file_assignments = [(f, start_idx + i) for i, f in enumerate(needs_renaming)]\n",
    "                \n",
    "                counter_lock = threading.Lock()\n",
    "                counter = {'renamed': 0, 'errors': 0}\n",
    "                \n",
    "                def rename_single_file(file_info, file_number):\n",
    "                    try:\n",
    "                        new_name = f\"part_{str(file_number).zfill(4)}.gz\"\n",
    "                        new_path = f\"{partition.path}{new_name}\"\n",
    "                        dbutils.fs.mv(file_info.path, new_path)\n",
    "                        \n",
    "                        with counter_lock:\n",
    "                            counter['renamed'] += 1\n",
    "                            if counter['renamed'] % 100 == 0:\n",
    "                                print(f\"    {partition.name}: {counter['renamed']}/{len(needs_renaming)} renamed...\")\n",
    "                        return True\n",
    "                    except Exception as e:\n",
    "                        with counter_lock:\n",
    "                            counter['errors'] += 1\n",
    "                        return False\n",
    "                \n",
    "                # use 50 workers for large directories\n",
    "                with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "                    futures = [executor.submit(rename_single_file, f, num) \n",
    "                              for f, num in file_assignments]\n",
    "                    \n",
    "                    for future in as_completed(futures):\n",
    "                        future.result()\n",
    "                \n",
    "                renamed_count = counter['renamed']\n",
    "                \n",
    "            else:\n",
    "                # small directories - sequential is fine\n",
    "                renamed_count = 0\n",
    "                for idx, file_info in enumerate(needs_renaming):\n",
    "                    new_number = start_idx + idx\n",
    "                    new_name = f\"part_{str(new_number).zfill(4)}.gz\"\n",
    "                    new_path = f\"{partition.path}{new_name}\"\n",
    "                    \n",
    "                    try:\n",
    "                        dbutils.fs.mv(file_info.path, new_path)\n",
    "                        renamed_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error: {e}\")\n",
    "            \n",
    "            # clean up metadata files\n",
    "            cleanup_count = 0\n",
    "            for f in metadata_files:\n",
    "                try:\n",
    "                    dbutils.fs.rm(f.path)\n",
    "                    cleanup_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return partition.name, True, f\"{renamed_count} renamed, {len(already_renamed)} existing, {cleanup_count} cleaned\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return partition.name, False, str(e)\n",
    "    \n",
    "    # process partitions\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_single_partition_fast, p): p for p in partitions_to_process}\n",
    "        \n",
    "        completed = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            partition_name, success, message = future.result()\n",
    "            completed += 1\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✓ {partition_name}: {message} ({elapsed:.1f}s)\")\n",
    "            else:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✗ {partition_name}: Error - {message}\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "rename_files_and_cleanup(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98cfadc3-512c-44c4-b0bd-344804722952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc02610-62d4-4d43-a7cd-243a21cf98f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_manifest():\n",
    "    \"\"\"\n",
    "    Create a manifest file with all file metadata using parallel processing.\n",
    "    \"\"\"\n",
    "    output_path = f\"{s3_base_path}/{entity_type}\"\n",
    "    \n",
    "    print(f\"\\nCreating manifest...\")\n",
    "    \n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = sorted([p for p in partitions if p.name.startswith(\"updated_date=\")], \n",
    "                                   key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    def process_file(partition_name, file_info):\n",
    "        \"\"\"Process a single file to get its metadata\"\"\"\n",
    "        if not file_info.name.endswith('.gz'):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # count records in the file\n",
    "            record_count = spark.read.text(file_info.path).count()\n",
    "            \n",
    "            # set the s3 url to the prod s3 folder\n",
    "            raw = file_info.path.replace(\"dbfs:/\", \"s3://\")\n",
    "            marker = f\"/{entity_type}/\"\n",
    "            idx = raw.find(marker)\n",
    "            if idx == -1:\n",
    "                raise ValueError(f\"Could not find '{marker}' in path: {raw}\")\n",
    "            relative = raw[idx:]\n",
    "            s3_url = f\"s3://openalex/data{relative}\"\n",
    "\n",
    "            entry = {\n",
    "                \"url\": s3_url,\n",
    "                \"meta\": {\n",
    "                    \"content_length\": file_info.size,\n",
    "                    \"record_count\": record_count\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"entry\": entry,\n",
    "                \"partition\": partition_name,\n",
    "                \"file\": file_info.name,\n",
    "                \"size\": file_info.size,\n",
    "                \"count\": record_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {partition_name}{file_info.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # collect all file tasks\n",
    "    file_tasks = []\n",
    "    for partition in partitions_to_process:\n",
    "        files = dbutils.fs.ls(partition.path)\n",
    "        for file_info in files:\n",
    "            if file_info.name.endswith('.gz'):\n",
    "                file_tasks.append((partition.name, file_info))\n",
    "    \n",
    "    print(f\"Processing {len(file_tasks)} files across {len(partitions_to_process)} partitions...\")\n",
    "    \n",
    "    # process files in parallel\n",
    "    entries = []\n",
    "    total_content_length = 0\n",
    "    total_record_count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = {executor.submit(process_file, task[0], task[1]): task \n",
    "                  for task in file_tasks}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if result:\n",
    "                entries.append(result[\"entry\"])\n",
    "                total_content_length += result[\"size\"]\n",
    "                total_record_count += result[\"count\"]\n",
    "                \n",
    "                if completed % 50 == 0 or completed == len(file_tasks):\n",
    "                    print(f\"  Progress: {completed}/{len(file_tasks)} files processed...\")\n",
    "                \n",
    "                # print details for large files\n",
    "                if result[\"size\"] > 100 * 1024 * 1024:  # Files > 100MB\n",
    "                    print(f\"  {result['partition']}{result['file']}: \"\n",
    "                          f\"{result['count']:,} records, {result['size']/(1024*1024):.1f} MB\")\n",
    "    \n",
    "    entries.sort(key=lambda x: x[\"url\"])\n",
    "    \n",
    "    manifest = {\n",
    "        \"entries\": entries,\n",
    "        \"meta\": {\n",
    "            \"content_length\": total_content_length,\n",
    "            \"record_count\": total_record_count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    manifest_path = f\"{output_path}/manifest\"\n",
    "    manifest_json = json.dumps(manifest, indent=2)\n",
    "    dbutils.fs.put(manifest_path, manifest_json, overwrite=True)\n",
    "    \n",
    "    print(f\"\\nManifest created: {manifest_path}\")\n",
    "    print(f\"Total files: {len(entries)}\")\n",
    "    print(f\"Total size (compressed): {total_content_length / (1024**3):.2f} GB\")\n",
    "    print(f\"Total records: {total_record_count:,}\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "create_manifest()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "export_works",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
