{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e2df64-8056-4fe0-a097-031f621303f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Format works same as API and save to `openalex.works.openalex_works_snapshot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a5233b-3517-4b0a-8002-754e7c5a7f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5be9a94-c75d-480d-a534-08f4e18d636a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create `openalex.works.openalex_works_snapshot` in same format as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd232240-2869-4cc0-a951-92d8ae74ff14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def truncate_abstract_index_string(raw_json: str, max_bytes: int = 32760) -> str:\n",
    "    try:\n",
    "        if not raw_json:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            json.loads(raw_json)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            # Invalid JSON - return None\n",
    "            return None\n",
    "\n",
    "        if len(raw_json) <= (max_bytes // 4):\n",
    "            return raw_json\n",
    "\n",
    "        encoded = raw_json.encode('utf-8')\n",
    "        if len(encoded) <= max_bytes:\n",
    "            return raw_json\n",
    "\n",
    "        truncated = encoded[:max_bytes].decode('utf-8', errors='ignore')\n",
    "        last_bracket = truncated.rfind(']')\n",
    "        if last_bracket == -1:\n",
    "            return None\n",
    "\n",
    "        return truncated[:last_bracket + 1] + '}'\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sanitize_name(col_name: str):\n",
    "    \"\"\"\n",
    "    Cleans a string column by removing unwanted characters and normalizing whitespace.\n",
    "    Handles multilingual text by preserving letters, numbers, punctuation, and symbols from all Unicode scripts.\n",
    "    \"\"\"\n",
    "    unwanted_chars_pattern = r\"[^\\p{L}\\p{N}\\p{P}\\p{S}\\p{Z}]\"\n",
    "    multiple_spaces_pattern = r\"\\s+\"\n",
    "\n",
    "    return F.trim(\n",
    "        F.regexp_replace( \n",
    "            F.regexp_replace(F.col(col_name), unwanted_chars_pattern, \"\"),\n",
    "            multiple_spaces_pattern, \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def sanitize_string(col_name: str, max_len: int = 32000):\n",
    "    return F.when(F.col(col_name).isNotNull(), F.substring(F.col(col_name), 1, max_len)).otherwise(None)\n",
    "\n",
    "\n",
    "empty_sdg_array = F.array().cast(\"array<struct<id:string,display_name:string,score:double>>\")\n",
    "\n",
    "df_transformed = (\n",
    "    spark.read.table(\"openalex.works.openalex_works\")\n",
    "    .withColumn(\"display_name\", F.col(\"title\"))\n",
    "    .withColumn(\"created_date\", F.to_timestamp(\"created_date\"))\n",
    "    .withColumn(\"updated_date\", F.to_timestamp(\"updated_date\"))\n",
    "    .withColumn(\"publication_date\", F.to_date(\"publication_date\"))\n",
    "    .withColumn(\n",
    "        \"concepts\",\n",
    "        F.transform(\n",
    "            F.col(\"concepts\"),\n",
    "            lambda c: F.struct(\n",
    "                F.concat(F.lit(\"https://openalex.org/C\"), c.id).alias(\"id\"),\n",
    "                c.wikidata.alias(\"wikidata\"),\n",
    "                c.display_name.alias(\"display_name\"),\n",
    "                c.level.alias(\"level\"),\n",
    "                c.score.alias(\"score\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"created_date\",\n",
    "        F.when(\n",
    "            F.col(\"created_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"created_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"updated_date\",\n",
    "        F.when(\n",
    "            F.col(\"updated_date\").between(F.lit(\"1000-01-01\"), F.lit(\"9999-12-31\")),\n",
    "            F.col(\"updated_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"publication_date\",\n",
    "        F.when(\n",
    "            F.col(\"publication_date\").between(F.lit(\"1000-01-01\"), F.lit(\"2050-12-31\")),\n",
    "            F.col(\"publication_date\")\n",
    "        ).otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    "    .withColumn(\"id\", F.concat(F.lit(\"https://openalex.org/W\"), F.col(\"id\")))\n",
    "    .withColumn(\"publication_year\", F.year(\"publication_date\"))\n",
    "    .withColumn(\"title\", sanitize_name(\"title\"))\n",
    "    .withColumn(\"display_name\", sanitize_name(\"display_name\"))\n",
    "    .withColumn(\"ids\", \n",
    "        F.transform_values(\"ids\",\n",
    "            lambda k, v: F.when(k == \"doi\", \n",
    "                    F.concat(F.lit(\"https://doi.org/\"),v)).otherwise(v)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"doi\", sanitize_string(\"doi\"))\n",
    "    .withColumn(\"language\", sanitize_string(\"language\"))\n",
    "    .withColumn(\"type\", sanitize_string(\"type\"))\n",
    "    .withColumn(\"abstract\", sanitize_string(\"abstract\"))\n",
    "    .withColumn(\"referenced_works\", \n",
    "                F.expr(\"transform(referenced_works, x -> 'https://openalex.org/W' || x)\"))\n",
    "    .withColumn(\"referenced_works_count\", \n",
    "                F.when(F.col(\"referenced_works\").isNotNull(), F.size(\"referenced_works\")).otherwise(0))\n",
    "    .withColumn(\"abstract_inverted_index\", truncate_abstract_index_string(F.col(\"abstract_inverted_index\")))\n",
    "    .withColumn(\"open_access\", F.struct(\n",
    "        F.col(\"open_access.is_oa\"),\n",
    "        sanitize_string(\"open_access.oa_status\").alias(\"oa_status\"),\n",
    "        F.lit(False).cast(\"boolean\").alias(\"any_repository_has_fulltext\"),\n",
    "        F.col(\"open_access.oa_url\")\n",
    "    ))\n",
    "    .withColumn(\"authorships\", F.expr(\"\"\"\n",
    "        transform(authorships, x -> named_struct(\n",
    "            'affiliations', x.affiliations,\n",
    "            'author', x.author,\n",
    "            'author_position', substring(x.author_position, 1, 32000),\n",
    "            'countries', x.countries,\n",
    "            'raw_author_name', substring(x.raw_author_name, 1, 32000),\n",
    "            'is_corresponding', x.is_corresponding,\n",
    "            'raw_affiliation_strings', transform(x.raw_affiliation_strings, aff -> substring(aff, 1, 32000)),\n",
    "            'institutions', x.institutions\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"locations\", F.expr(\"\"\"\n",
    "        transform(locations, x -> named_struct(\n",
    "            'is_oa', x.is_oa,\n",
    "            'is_published', x.version = 'publishedVersion',\n",
    "            'landing_page_url', substring(x.landing_page_url, 1, 32000),\n",
    "            'pdf_url', substring(x.pdf_url, 1, 32000),\n",
    "            'source', x.source,\n",
    "            'raw_source_name', x.raw_source_name,\n",
    "            'native_id', x.native_id,\n",
    "            'provenance', x.provenance,\n",
    "            'license', x.license,\n",
    "            'license_id', x.license_id\n",
    "        ))\n",
    "    \"\"\"))\n",
    "    .withColumn(\"concepts\", F.slice(F.col(\"concepts\"), 1, 40))\n",
    "    .withColumn(\"indexed_in\", F.expr(\"\"\"\n",
    "        array_sort(\n",
    "            array_distinct(\n",
    "                array_compact(\n",
    "                    flatten(\n",
    "                        TRANSFORM(locations, loc ->\n",
    "                            CASE\n",
    "                            WHEN loc.provenance IN ('crossref', 'pubmed', 'datacite')\n",
    "                                THEN array(loc.provenance, IF(loc.source.is_in_doaj, 'doaj', NULL))\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:arxiv.org%'\n",
    "                                THEN array('arxiv')\n",
    "                            WHEN loc.provenance = 'repo' AND lower(loc.native_id) like 'oai:doaj.org/%'\n",
    "                                THEN array('doaj')\n",
    "                            WHEN loc.provenance = 'mag' AND lower(loc.source.display_name) = 'pubmed'\n",
    "                                THEN array('pubmed')\n",
    "                            ELSE array()\n",
    "                            END\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \"\"\"))\n",
    "    .withColumn(\"corresponding_author_ids\", F.coalesce(F.col(\"corresponding_author_ids\"), F.lit([])))\n",
    "    .withColumn(\"corresponding_institution_ids\", F.coalesce(F.col(\"corresponding_institution_ids\"), F.lit([])))\n",
    "    .withColumn(\"sustainable_development_goals\", F.coalesce(F.col(\"sustainable_development_goals\"), empty_sdg_array))\n",
    "    .withColumn(\"related_works\", F.coalesce(F.col(\"related_works\"), F.lit([])))\n",
    "    .withColumn(\"fwci\", F.coalesce(F.col(\"fwci\"), F.lit(0)))\n",
    "    .withColumn(\"mesh\", F.coalesce(F.col(\"mesh\"), F.lit([])))\n",
    ")\n",
    "\n",
    "df_transformed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"openalex.works.openalex_works_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebdfef42-2c2c-4f50-96ba-df8a190c457a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Export in json lines format to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ba0c81-d769-40af-81e0-f56940103621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "entity_type = \"works\"\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "RECORDS_PER_FILE = 400000\n",
    "s3_base_path = f\"s3://openalex-sandbox/snapshots/{date_str}\"\n",
    "output_path = f\"{s3_base_path}/{entity_type}\"\n",
    "\n",
    "def export():\n",
    "    print(f\"Starting export to: {output_path}\")\n",
    "    print(f\"Records per file: {RECORDS_PER_FILE:,}\")\n",
    "    \n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "    \n",
    "    df = spark.read.table(\"openalex.works.openalex_works_snapshot\")\n",
    "    df = df.withColumn(\"updated_date\", F.to_date(\"updated_date\"))\n",
    "    \n",
    "    df_with_count = df.join(\n",
    "        df.groupBy(\"updated_date\").count().withColumnRenamed(\"count\", \"date_count\"),\n",
    "        on=\"updated_date\"\n",
    "    )\n",
    "\n",
    "    date_stats = df_with_count.select(\"updated_date\", \"date_count\").distinct().orderBy(F.desc(\"date_count\")).collect()\n",
    "    print(\"\\nDate distribution (top 10):\")\n",
    "    for row in date_stats[:10]:\n",
    "        expected_files = (row['date_count'] + RECORDS_PER_FILE - 1) // RECORDS_PER_FILE\n",
    "        print(f\"  {row['updated_date']}: {row['date_count']:,} records → {expected_files} files expected\")\n",
    "    \n",
    "    # apply hash-based salting for predictable distribution\n",
    "    df_salted = df_with_count.withColumn(\n",
    "        \"salt\",\n",
    "        F.when(F.col(\"date_count\") > 100_000_000, F.abs(F.hash(\"id\")) % 1400)\n",
    "        .when(F.col(\"date_count\") > 40_000_000, F.abs(F.hash(\"id\")) % 160)\n",
    "        .when(F.col(\"date_count\") > 10_000_000, F.abs(F.hash(\"id\")) % 50)\n",
    "        .when(F.col(\"date_count\") > 5_000_000, F.abs(F.hash(\"id\")) % 25)\n",
    "        .when(F.col(\"date_count\") > 2_000_000, F.abs(F.hash(\"id\")) % 10)\n",
    "        .when(F.col(\"date_count\") > 800_000, F.abs(F.hash(\"id\")) % 3)\n",
    "        .otherwise(0)\n",
    "    ).drop(\"date_count\")\n",
    "    \n",
    "    print(\"\\nRepartitioning and writing to S3...\")\n",
    "    df_out = df_salted.repartition(F.col(\"updated_date\"), F.col(\"salt\")).drop(\"salt\")\n",
    "    \n",
    "    (df_out.write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"compression\", \"gzip\")\n",
    "         .option(\"maxRecordsPerFile\", RECORDS_PER_FILE)\n",
    "         .partitionBy(\"updated_date\")\n",
    "         .json(output_path))\n",
    "    \n",
    "    print(\"Export completed!\")\n",
    "\n",
    "export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b123b1de-ed0b-43fe-bbae-3b24427fd2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Rename the files into sequential numbers, remove spark metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3138c66-09b3-4eab-af3a-27eb1d6d90ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_files_and_cleanup(output_path, max_workers=30):\n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = [p for p in partitions if p.name.startswith(\"updated_date=\")]\n",
    "    \n",
    "    print(f\"Found {len(partitions_to_process)} partitions to process\")\n",
    "    \n",
    "    def process_single_partition(partition):\n",
    "        try:\n",
    "            # first, get all files in the directory\n",
    "            files = dbutils.fs.ls(partition.path)\n",
    "            \n",
    "            # separate already-renamed files from those needing renaming\n",
    "            already_renamed = []\n",
    "            needs_renaming = []\n",
    "            metadata_files = []\n",
    "            \n",
    "            for f in files:\n",
    "                if f.name.startswith('part_') and f.name.endswith('.gz'):\n",
    "                    already_renamed.append(f)\n",
    "                elif f.name.endswith('.json.gz'):\n",
    "                    needs_renaming.append(f)\n",
    "                else:\n",
    "                    metadata_files.append(f)\n",
    "            \n",
    "            # sort both lists to ensure consistent ordering\n",
    "            already_renamed.sort(key=lambda x: x.name)\n",
    "            needs_renaming.sort(key=lambda x: x.name)\n",
    "            \n",
    "            total_data_files = len(already_renamed) + len(needs_renaming)\n",
    "            \n",
    "            if len(needs_renaming) == 0:\n",
    "                # already processed\n",
    "                return partition.name, True, f\"{total_data_files} files already renamed\"\n",
    "            \n",
    "            # find the highest existing part number\n",
    "            max_existing = -1\n",
    "            for f in already_renamed:\n",
    "                try:\n",
    "                    num_str = f.name.replace('part_', '').replace('.gz', '')\n",
    "                    num = int(num_str)\n",
    "                    max_existing = max(max_existing, num)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # start renaming from the next available number\n",
    "            start_idx = max_existing + 1\n",
    "            \n",
    "            renamed_count = 0\n",
    "            for idx, file_info in enumerate(needs_renaming):\n",
    "                new_number = start_idx + idx\n",
    "                new_name = f\"part_{str(new_number).zfill(4)}.gz\"\n",
    "                new_path = f\"{partition.path}{new_name}\"\n",
    "                \n",
    "                try:\n",
    "                    # check if target already exists (safety check)\n",
    "                    existing = [f for f in files if f.name == new_name]\n",
    "                    if existing:\n",
    "                        print(f\"  WARNING: {new_name} already exists in {partition.name}, skipping\")\n",
    "                        continue\n",
    "                    \n",
    "                    dbutils.fs.mv(file_info.path, new_path)\n",
    "                    renamed_count += 1\n",
    "                    \n",
    "                    # Progress indicator for large directories\n",
    "                    if renamed_count % 100 == 0:\n",
    "                        print(f\"  {partition.name}: Renamed {renamed_count}/{len(needs_renaming)} files...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error renaming in {partition.name}: {e}\")\n",
    "            \n",
    "            # clean up metadata files\n",
    "            cleanup_count = 0\n",
    "            for f in metadata_files:\n",
    "                try:\n",
    "                    dbutils.fs.rm(f.path)\n",
    "                    cleanup_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return partition.name, True, f\"{renamed_count} renamed, {len(already_renamed)} existing, {cleanup_count} cleaned\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return partition.name, False, str(e)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_single_partition, p): p for p in partitions_to_process}\n",
    "        \n",
    "        completed = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            partition_name, success, message = future.result()\n",
    "            completed += 1\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✓ {partition_name}: {message} ({elapsed:.1f}s)\")\n",
    "            else:\n",
    "                print(f\"  [{completed}/{len(partitions_to_process)}] ✗ {partition_name}: Error - {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98cfadc3-512c-44c4-b0bd-344804722952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc02610-62d4-4d43-a7cd-243a21cf98f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_manifest():\n",
    "    \"\"\"\n",
    "    Create a manifest file with all file metadata using parallel processing.\n",
    "    \"\"\"\n",
    "    output_path = f\"{s3_base_path}/{entity_type}\"\n",
    "    \n",
    "    print(f\"\\nCreating manifest...\")\n",
    "    \n",
    "    partitions = dbutils.fs.ls(output_path)\n",
    "    partitions_to_process = sorted([p for p in partitions if p.name.startswith(\"updated_date=\")], \n",
    "                                   key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    def process_file(partition_name, file_info):\n",
    "        \"\"\"Process a single file to get its metadata\"\"\"\n",
    "        if not file_info.name.endswith('.gz'):\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # count records in the file\n",
    "            record_count = spark.read.json(file_info.path).count()\n",
    "            \n",
    "            s3_url = file_info.path.replace(\"dbfs:/\", \"s3://\")\n",
    "            entry = {\n",
    "                \"url\": s3_url,\n",
    "                \"meta\": {\n",
    "                    \"content_length\": file_info.size,\n",
    "                    \"record_count\": record_count\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"entry\": entry,\n",
    "                \"partition\": partition_name,\n",
    "                \"file\": file_info.name,\n",
    "                \"size\": file_info.size,\n",
    "                \"count\": record_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {partition_name}{file_info.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # collect all file tasks\n",
    "    file_tasks = []\n",
    "    for partition in partitions_to_process:\n",
    "        files = dbutils.fs.ls(partition.path)\n",
    "        for file_info in files:\n",
    "            if file_info.name.endswith('.gz'):\n",
    "                file_tasks.append((partition.name, file_info))\n",
    "    \n",
    "    print(f\"Processing {len(file_tasks)} files across {len(partitions_to_process)} partitions...\")\n",
    "    \n",
    "    # process files in parallel\n",
    "    entries = []\n",
    "    total_content_length = 0\n",
    "    total_record_count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:  # Increase workers for file reading\n",
    "        futures = {executor.submit(process_file, task[0], task[1]): task \n",
    "                  for task in file_tasks}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if result:\n",
    "                entries.append(result[\"entry\"])\n",
    "                total_content_length += result[\"size\"]\n",
    "                total_record_count += result[\"count\"]\n",
    "                \n",
    "                if completed % 50 == 0 or completed == len(file_tasks):\n",
    "                    print(f\"  Progress: {completed}/{len(file_tasks)} files processed...\")\n",
    "                \n",
    "                # print details for large files\n",
    "                if result[\"size\"] > 100 * 1024 * 1024:  # Files > 100MB\n",
    "                    print(f\"  {result['partition']}{result['file']}: \"\n",
    "                          f\"{result['count']:,} records, {result['size']/(1024*1024):.1f} MB\")\n",
    "    \n",
    "    entries.sort(key=lambda x: x[\"url\"])\n",
    "    \n",
    "    manifest = {\n",
    "        \"entries\": entries,\n",
    "        \"meta\": {\n",
    "            \"content_length\": total_content_length,\n",
    "            \"record_count\": total_record_count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    manifest_path = f\"{output_path}/manifest\"\n",
    "    manifest_json = json.dumps(manifest, indent=2)\n",
    "    dbutils.fs.put(manifest_path, manifest_json, overwrite=True)\n",
    "    \n",
    "    print(f\"\\nManifest created: {manifest_path}\")\n",
    "    print(f\"Total files: {len(entries)}\")\n",
    "    print(f\"Total size (compressed): {total_content_length / (1024**3):.2f} GB\")\n",
    "    print(f\"Total records: {total_record_count:,}\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "manifest = create_manifest()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "export_works",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
