{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de02de46-49a1-4e0b-a2d8-dde083dcc7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and Prepare Concept `model` and `vocabularies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73559f49-4b17-407d-af89-82a4e657da8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "    \n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/paper_title_vocab.pkl\", \"rb\") as f:\n",
    "    title_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/doc_type_vocab.pkl\", \"rb\") as f:\n",
    "    doc_type_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/journal_name_vocab.pkl\", \"rb\") as f:\n",
    "    journal_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/tag_id_vocab.pkl\", \"rb\") as f:\n",
    "    tag_id_vocab = pickle.load(f)    \n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/topics_vocab.pkl\", \"rb\") as f:\n",
    "    topics_vocab = pickle.load(f)\n",
    "    inverted_topics_vocab = {v: k for k, v in topics_vocab.items()}\n",
    "\n",
    "# broadcast model and vocabularies\n",
    "# this results in pickle error - need to find other ways\n",
    "# bc_model = spark.sparkContext.broadcast(model)\n",
    "bc_title_vocab = spark.sparkContext.broadcast(title_vocab)\n",
    "bc_journal_vocab = spark.sparkContext.broadcast(journal_vocab)\n",
    "bc_doc_type_vocab = spark.sparkContext.broadcast(doc_type_vocab)\n",
    "bc_tag_id_vocab = spark.sparkContext.broadcast(tag_id_vocab)\n",
    "bc_topics_vocab = spark.sparkContext.broadcast(topics_vocab)\n",
    "bc_inverted_topics_vocab = spark.sparkContext.broadcast(inverted_topics_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32763bb-3e86-4500-a0b5-4745666df8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "#### Load Input from `works_concepts_frontfill_input` Table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49da2c75-73e2-48c8-bf0b-3e5dbfab789a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Read from pre-computed input table (created by concepts_create_frontfill_input notebook)\ndf = spark.sql(\"\"\"\n    SELECT concept_key, title, abstract, journal, doc_type\n    FROM openalex.works.works_concepts_frontfill_input\n    LIMIT 20480000\n\"\"\").repartition(4096).cache()\n\nprint(f\"Total number of rows to process: {df.count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3a94e-8ad6-4421-88b7-bdf09f98ab6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load model per partition, execute via `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d20466-1042-464a-9c6e-21ef77864d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"concept_key\", LongType(), nullable=False),\n",
    "    StructField(\"concepts\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), nullable=False),\n",
    "            StructField(\"score\", DoubleType(), nullable=False),\n",
    "        ])\n",
    "    ), nullable=True)\n",
    "])\n",
    "\n",
    "def tokenize(text, vocab, max_len):\n",
    "    if text is None or text.strip() == \"\":\n",
    "        # Fully padded zeros for missing or empty text\n",
    "        return np.zeros(max_len, dtype=np.int64)\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(token, 0) for token in tokens][:max_len]\n",
    "    token_ids += [0] * (max_len - len(token_ids))\n",
    "    return np.array(token_ids, dtype=np.int64)\n",
    "\n",
    "def process_batch(batch, model, title_vocab, journal_vocab, doc_type_vocab, tag_id_vocab):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    # Prepare arrays\n",
    "    concept_keys = []\n",
    "    title_ids_batch = []\n",
    "    abstract_ids_batch = []\n",
    "    doc_type_ids = []\n",
    "    journal_ids = []\n",
    "\n",
    "    for row in batch:\n",
    "        concept_keys.append(row.concept_key)\n",
    "        title_ids_batch.append(tokenize(row.title or \"\", title_vocab, 32))\n",
    "        abstract_ids_batch.append(tokenize(row.abstract or \"\", title_vocab, 256))\n",
    "        doc_type_ids.append([doc_type_vocab.get((row.doc_type or '').lower(), 0)])\n",
    "        journal_ids.append([journal_vocab.get((row.journal or '').lower(), 0)])\n",
    "\n",
    "    # Convert to tensors\n",
    "    title_ids_batch = tf.constant(np.stack(title_ids_batch))\n",
    "    abstract_ids_batch = tf.constant(np.stack(abstract_ids_batch))\n",
    "    doc_type_ids = tf.constant(np.array(doc_type_ids, dtype=np.int64))\n",
    "    journal_ids = tf.constant(np.array(journal_ids, dtype=np.int64))\n",
    "\n",
    "    outputs = model.signatures['serving_default'](\n",
    "        paper_title_ids=title_ids_batch,\n",
    "        abstract_ids=abstract_ids_batch,\n",
    "        doc_type_id=doc_type_ids,\n",
    "        journal_id=journal_ids\n",
    "    )\n",
    "    logits_batch = outputs['cls'].numpy()  # shape: [batch_size, num_concepts]\n",
    "\n",
    "    score_threshold = 0.25\n",
    "    for i in range(len(batch)):\n",
    "        logits = logits_batch[i]\n",
    "        top_k_idx = np.argpartition(-logits, kth=9)[:10]\n",
    "        combined = np.unique(np.concatenate((np.where(logits >= score_threshold)[0], top_k_idx)))\n",
    "        sorted_idx = combined[np.argsort(-logits[combined])][:65]\n",
    "\n",
    "        concepts = [\n",
    "            {\"id\": int(tag_id_vocab.get(idx, 0)), \"score\": float(logits[idx])}\n",
    "            for idx in sorted_idx\n",
    "        ]\n",
    "        yield (concept_keys[i], concepts)\n",
    "\n",
    "# Now apply on partitions:\n",
    "def process_partition(rows_iter, batch_size=64):\n",
    "    # Load model ONCE per partition\n",
    "    model = tf.saved_model.load(\"/Volumes/openalex/works/models/concept_tagger_v3/model\")\n",
    "\n",
    "    batch = []\n",
    "    for row in rows_iter:\n",
    "        batch.append(row)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield from process_batch(batch, model, bc_title_vocab.value,\n",
    "                                     bc_journal_vocab.value, bc_doc_type_vocab.value, bc_tag_id_vocab.value)\n",
    "            batch = []\n",
    "\n",
    "    if batch:\n",
    "        yield from process_batch(batch, model, bc_title_vocab.value,\n",
    "                                    bc_journal_vocab.value, bc_doc_type_vocab.value, bc_tag_id_vocab.value)\n",
    "\n",
    "# Apply with mapPartitions\n",
    "result_rdd = df.rdd.mapPartitions(process_partition)\n",
    "\n",
    "# Convert to DataFrame\n",
    "inferred_concepts_df = spark.createDataFrame(result_rdd, schema).cache()\n",
    "\n",
    "print(f\"Total number of rows processed: {inferred_concepts_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546cbf05-a2aa-4223-877b-1f998b887d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# array<struct<id:bigint,wikidata:string,display_name:string,level:int,score:float>>\n",
    "concepts_enriched_schema = ArrayType(StructType([\n",
    "    StructField(\"id\", LongType(), nullable=True),\n",
    "    StructField(\"wikidata\", StringType(), nullable=True),\n",
    "    StructField(\"display_name\", StringType(), nullable=True),\n",
    "    StructField(\"level\", IntegerType(), nullable=True),\n",
    "    StructField(\"score\", FloatType(), nullable=True)\n",
    "]))\n",
    "keywords_schema = ArrayType(StructType([\n",
    "    StructField(\"id\", StringType(), nullable=True),\n",
    "    StructField(\"display_name\", StringType(), nullable=True),\n",
    "    StructField(\"score\", FloatType(), nullable=True)\n",
    "]))\n",
    "\n",
    "final_df = (df.join(inferred_concepts_df, on=\"concept_key\", how=\"left\")\n",
    "    .withColumn(\"concepts_enriched\", lit(None).cast(concepts_enriched_schema))\n",
    "    .withColumn(\"keywords\", lit(None).cast(keywords_schema))\n",
    "    .withColumn(\"created_timestamp\", current_timestamp()))\n",
    "\n",
    "final_df.write.mode(\"append\").saveAsTable(\"openalex.works.openalex_works_concepts_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd438abe-6585-4f45-88b9-13f3ab8fd3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Show stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d96af70-9e59-4d48-887b-691267dd5af8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752204370890}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*), \n",
    "  count(DISTINCT concept_key),\n",
    "  count_if(size(concepts) > 0) AS concepts_not_null,\n",
    "  count_if(size(concepts_enriched) > 0) AS concepts_enriched_not_null\n",
    "FROM openalex.works.openalex_works_concepts_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac7f032-a05a-4361-aa8a-0c20642ca02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*), count_if(keywords is null) \n",
    "from works.openalex_works_concepts_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfa8de4-2b50-4db1-b9c4-31fde3448aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM openalex.works.openalex_works_concepts_predicted"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2638448112843526,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "concepts_model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
