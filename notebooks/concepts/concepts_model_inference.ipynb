{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de02de46-49a1-4e0b-a2d8-dde083dcc7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and Prepare Concept `model` and `vocabularies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73559f49-4b17-407d-af89-82a4e657da8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "    \n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/paper_title_vocab.pkl\", \"rb\") as f:\n",
    "    title_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/doc_type_vocab.pkl\", \"rb\") as f:\n",
    "    doc_type_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/journal_name_vocab.pkl\", \"rb\") as f:\n",
    "    journal_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/tag_id_vocab.pkl\", \"rb\") as f:\n",
    "    tag_id_vocab = pickle.load(f)    \n",
    "\n",
    "with open(\"/Volumes/openalex/works/models/concept_tagger_v3/topics_vocab.pkl\", \"rb\") as f:\n",
    "    topics_vocab = pickle.load(f)\n",
    "    inverted_topics_vocab = {v: k for k, v in topics_vocab.items()}\n",
    "\n",
    "# broadcast model and vocabularies\n",
    "# this results in pickle error - need to find other ways\n",
    "# bc_model = spark.sparkContext.broadcast(model)\n",
    "bc_title_vocab = spark.sparkContext.broadcast(title_vocab)\n",
    "bc_journal_vocab = spark.sparkContext.broadcast(journal_vocab)\n",
    "bc_doc_type_vocab = spark.sparkContext.broadcast(doc_type_vocab)\n",
    "bc_tag_id_vocab = spark.sparkContext.broadcast(tag_id_vocab)\n",
    "bc_topics_vocab = spark.sparkContext.broadcast(topics_vocab)\n",
    "bc_inverted_topics_vocab = spark.sparkContext.broadcast(inverted_topics_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32763bb-3e86-4500-a0b5-4745666df8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Prepare `openalex_works` Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49da2c75-73e2-48c8-bf0b-3e5dbfab789a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    WITH works_with_concept_keys AS (\n",
    "        SELECT\n",
    "            xxhash64(\n",
    "                concat_ws('|',\n",
    "                    title,\n",
    "                    abstract,\n",
    "                    primary_location.source.display_name,\n",
    "                    primary_location.source.type\n",
    "                )\n",
    "            ) AS concept_key,\n",
    "            FIRST(title) as title,\n",
    "            FIRST(abstract) as abstract,\n",
    "            FIRST(primary_location.source.display_name) AS journal,\n",
    "            FIRST(primary_location.source.id) AS source_id,\n",
    "            FIRST(primary_location.source.type) AS doc_type\n",
    "        FROM openalex.works.openalex_works\n",
    "        WHERE (concepts IS NULL OR size(concepts) = 0)\n",
    "            AND title IS NOT NULL\n",
    "            AND (\n",
    "                (length(title) > 20 AND length(abstract) > 50)\n",
    "                OR length(title) > 50\n",
    "                OR length(abstract) > 150\n",
    "            )\n",
    "        GROUP BY concept_key\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM works_with_concept_keys w\n",
    "    LEFT ANTI JOIN openalex.works.openalex_works_concepts_predicted p\n",
    "    ON w.concept_key = p.concept_key\n",
    "    LIMIT 20480000\n",
    "\"\"\").repartition(4096).cache()\n",
    "\n",
    "print(f\"Total number of rows to process: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3a94e-8ad6-4421-88b7-bdf09f98ab6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load model per partition, execute via `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d20466-1042-464a-9c6e-21ef77864d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"concept_key\", LongType(), nullable=False),\n",
    "    StructField(\"concepts\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), nullable=False),\n",
    "            StructField(\"score\", DoubleType(), nullable=False),\n",
    "        ])\n",
    "    ), nullable=True)\n",
    "])\n",
    "\n",
    "def tokenize(text, vocab, max_len):\n",
    "    if text is None or text.strip() == \"\":\n",
    "        # Fully padded zeros for missing or empty text\n",
    "        return np.zeros(max_len, dtype=np.int64)\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(token, 0) for token in tokens][:max_len]\n",
    "    token_ids += [0] * (max_len - len(token_ids))\n",
    "    return np.array(token_ids, dtype=np.int64)\n",
    "\n",
    "def process_batch(batch, model, title_vocab, journal_vocab, doc_type_vocab, tag_id_vocab):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    # Prepare arrays\n",
    "    concept_keys = []\n",
    "    title_ids_batch = []\n",
    "    abstract_ids_batch = []\n",
    "    doc_type_ids = []\n",
    "    journal_ids = []\n",
    "\n",
    "    for row in batch:\n",
    "        concept_keys.append(row.concept_key)\n",
    "        title_ids_batch.append(tokenize(row.title or \"\", title_vocab, 32))\n",
    "        abstract_ids_batch.append(tokenize(row.abstract or \"\", title_vocab, 256))\n",
    "        doc_type_ids.append([doc_type_vocab.get((row.doc_type or '').lower(), 0)])\n",
    "        journal_ids.append([journal_vocab.get((row.journal or '').lower(), 0)])\n",
    "\n",
    "    # Convert to tensors\n",
    "    title_ids_batch = tf.constant(np.stack(title_ids_batch))\n",
    "    abstract_ids_batch = tf.constant(np.stack(abstract_ids_batch))\n",
    "    doc_type_ids = tf.constant(np.array(doc_type_ids, dtype=np.int64))\n",
    "    journal_ids = tf.constant(np.array(journal_ids, dtype=np.int64))\n",
    "\n",
    "    outputs = model.signatures['serving_default'](\n",
    "        paper_title_ids=title_ids_batch,\n",
    "        abstract_ids=abstract_ids_batch,\n",
    "        doc_type_id=doc_type_ids,\n",
    "        journal_id=journal_ids\n",
    "    )\n",
    "    logits_batch = outputs['cls'].numpy()  # shape: [batch_size, num_concepts]\n",
    "\n",
    "    score_threshold = 0.25\n",
    "    for i in range(len(batch)):\n",
    "        logits = logits_batch[i]\n",
    "        top_k_idx = np.argpartition(-logits, kth=9)[:10]\n",
    "        combined = np.unique(np.concatenate((np.where(logits >= score_threshold)[0], top_k_idx)))\n",
    "        sorted_idx = combined[np.argsort(-logits[combined])][:65]\n",
    "\n",
    "        concepts = [\n",
    "            {\"id\": int(tag_id_vocab.get(idx, 0)), \"score\": float(logits[idx])}\n",
    "            for idx in sorted_idx\n",
    "        ]\n",
    "        yield (concept_keys[i], concepts)\n",
    "\n",
    "# Now apply on partitions:\n",
    "def process_partition(rows_iter, batch_size=64):\n",
    "    # Load model ONCE per partition\n",
    "    model = tf.saved_model.load(\"/Volumes/openalex/works/models/concept_tagger_v3/model\")\n",
    "\n",
    "    batch = []\n",
    "    for row in rows_iter:\n",
    "        batch.append(row)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield from process_batch(batch, model, bc_title_vocab.value,\n",
    "                                     bc_journal_vocab.value, bc_doc_type_vocab.value, bc_tag_id_vocab.value)\n",
    "            batch = []\n",
    "\n",
    "    if batch:\n",
    "        yield from process_batch(batch, model, bc_title_vocab.value,\n",
    "                                    bc_journal_vocab.value, bc_doc_type_vocab.value, bc_tag_id_vocab.value)\n",
    "\n",
    "# Apply with mapPartitions\n",
    "result_rdd = df.rdd.mapPartitions(process_partition)\n",
    "\n",
    "# Convert to DataFrame\n",
    "inferred_concepts_df = spark.createDataFrame(result_rdd, schema).cache()\n",
    "\n",
    "print(f\"Total number of rows processed: {inferred_concepts_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546cbf05-a2aa-4223-877b-1f998b887d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# array<struct<id:bigint,wikidata:string,display_name:string,level:int,score:float>>\n",
    "concepts_enriched_schema = ArrayType(StructType([\n",
    "    StructField(\"id\", LongType(), nullable=True),\n",
    "    StructField(\"wikidata\", StringType(), nullable=True),\n",
    "    StructField(\"display_name\", StringType(), nullable=True),\n",
    "    StructField(\"level\", IntegerType(), nullable=True),\n",
    "    StructField(\"score\", FloatType(), nullable=True)\n",
    "]))\n",
    "keywords_schema = ArrayType(StructType([\n",
    "    StructField(\"id\", StringType(), nullable=True),\n",
    "    StructField(\"display_name\", StringType(), nullable=True),\n",
    "    StructField(\"score\", FloatType(), nullable=True)\n",
    "]))\n",
    "\n",
    "final_df = (df.join(inferred_concepts_df, on=\"concept_key\", how=\"left\")\n",
    "    .withColumn(\"concepts_enriched\", lit(None).cast(concepts_enriched_schema))\n",
    "    .withColumn(\"keywords\", lit(None).cast(keywords_schema))\n",
    "    .withColumn(\"created_timestamp\", current_timestamp()))\n",
    "\n",
    "final_df.write.mode(\"append\").saveAsTable(\"openalex.works.openalex_works_concepts_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd438abe-6585-4f45-88b9-13f3ab8fd3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Show stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d96af70-9e59-4d48-887b-691267dd5af8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752204370890}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*), \n",
    "  count(DISTINCT concept_key),\n",
    "  count_if(size(concepts) > 0) AS concepts_not_null,\n",
    "  count_if(size(concepts_enriched) > 0) AS concepts_enriched_not_null\n",
    "FROM openalex.works.openalex_works_concepts_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac7f032-a05a-4361-aa8a-0c20642ca02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*), count_if(keywords is null) \n",
    "from works.openalex_works_concepts_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfa8de4-2b50-4db1-b9c4-31fde3448aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM openalex.works.openalex_works_concepts_predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b1f86b-ea8e-465e-86f3-dda3696049ae",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755589013369}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# WITH new_keyword_id AS (\n",
    "#   SELECT display_name, \n",
    "#   regexp_replace(\n",
    "#     regexp_replace(\n",
    "#       regexp_replace(replace(lower(display_name), '\\'', ''), '\\\\s*\\\\([^)]*\\\\)', ''),  -- remove \" ( ... )\"\n",
    "#       '[^^\\\\p{L}\\\\p{N}\\./–\\*#]+', '-'                               -- non-alnum -> \"-\"\n",
    "#     ),\n",
    "#     '(^-+|-+$)', ''                                               -- trim leading/trailing \"-\"\n",
    "#   ) as new_keyword_id, keyword_id \n",
    "#   from openalex.common.concepts\n",
    "# )\n",
    "# SELECT * FROM new_keyword_id \n",
    "# WHERE keyword_id is not null \n",
    "#   and new_keyword_id <> keyword_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebb7e07-dc64-480f-80bd-5a8483de6e22",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755121507032}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# WITH concept_metadata AS (\n",
    "#   SELECT\n",
    "#     concept_id,\n",
    "#     wikidata_id as wikidata,\n",
    "#     display_name,\n",
    "#     keyword_id,\n",
    "#     use_as_keyword,\n",
    "#     level\n",
    "#   FROM openalex.common.concepts --WHERE wikidata_id IS NOT NULL\n",
    "# ),\n",
    "# concepts_exploded AS (\n",
    "#   SELECT\n",
    "#     concept_key,\n",
    "#     explode(concepts_enriched) as concept\n",
    "#   FROM openalex.works.openalex_works_concepts_predicted\n",
    "#   WHERE keywords IS NULL AND concepts_enriched IS NOT NULL\n",
    "# ),\n",
    "# enriched_concepts_exploded AS (\n",
    "#   SELECT\n",
    "#     wc.concept_key,\n",
    "#     concept,\n",
    "#     STRUCT(\n",
    "#         concat('https://openalex.org/keywords/',   \n",
    "#           regexp_replace(\n",
    "#             regexp_replace(\n",
    "#               regexp_replace(replace(lower(display_name), '\\'', ''), '\\\\s*\\\\([^)]*\\\\)', ''),  -- remove \" ( ... )\"\n",
    "#               '[^^\\\\p{L}\\\\p{N}\\./–\\*#]+', '-' -- non-alnum -> \"-\"\n",
    "#             ),\n",
    "#             '(^-+|-+$)', '' -- trim leading/trailing \"-\"\n",
    "#           )\n",
    "#         ) as id,\n",
    "#         c.display_name,\n",
    "#         wc.concept.score\n",
    "#       ) AS keyword\n",
    "#   FROM concepts_exploded wc\n",
    "#   JOIN concept_metadata c ON wc.concept.id = c.concept_id\n",
    "#   WHERE c.use_as_keyword = true\n",
    "# ),\n",
    "# updates AS (\n",
    "# select\n",
    "#   first(concept_key) as concept_key,\n",
    "#   array_sort(\n",
    "#     array_agg(keyword),\n",
    "#     (left, right) -> CASE\n",
    "#       WHEN left.score > right.score THEN -1\n",
    "#       WHEN left.score < right.score THEN 1\n",
    "#       ELSE 0\n",
    "#     END\n",
    "#   ) AS keywords\n",
    "# from enriched_concepts_exploded\n",
    "# group by work_id\n",
    "# )\n",
    "# MERGE INTO openalex.works.openalex_works_concepts_predicted AS target\n",
    "# USING updates AS src ON target.concept_key = src.concept_key\n",
    "# WHEN MATCHED THEN UPDATE SET target.keywords = src.keywords;\n",
    "# -- SELECT * FROM work_concepts_keywords; --7092554937, first update 219,825,895"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2638448112843526,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "concepts_model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
