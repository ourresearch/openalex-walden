{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e263ffd-8ed2-445e-86da-15cfc76d0d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b25c7c5f-bbb0-4780-9e31-2a2fc3c90247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8899026-8cfc-4d2f-ae96-59c8b7d056c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name = \"postgres-works\"):\n",
    "\n",
    "    if secret_name == \"postgres-works\":\n",
    "        secret = {'username': dbutils.secrets.get(scope = \"postgres-works\", key = \"user\"),\n",
    "                'password': dbutils.secrets.get(scope = \"postgres-works\", key = \"password\"),\n",
    "                'host': dbutils.secrets.get(scope = \"postgres-works\", key = \"host\"),\n",
    "                'dbname': dbutils.secrets.get(scope = \"postgres-works\", key = \"dbname\"),\n",
    "                'port': dbutils.secrets.get(scope = \"postgres-works\", key = \"port\"),\n",
    "                'engine': dbutils.secrets.get(scope = \"postgres-works\", key = \"engine\")}\n",
    "    elif secret_name == \"author-disambiguation-buckets\":\n",
    "        secret = {'and_save_path': dbutils.secrets.get(scope = \"author-disambiguation-buckets\", key = \"and_save_path\"),\n",
    "                  'database_copy_save_path': dbutils.secrets.get(scope = \"author-disambiguation-buckets\", key = \"database_copy_save_path\"),\n",
    "                  'temp_save_path': dbutils.secrets.get(scope = \"author-disambiguation-buckets\", key = \"temp_save_path\"),\n",
    "                  'orcid_save_path': dbutils.secrets.get(scope = \"author-disambiguation-buckets\", key = \"orcid_save_path\")}\n",
    "    elif secret_name == \"heroku-creds\":\n",
    "        secret = {'heroku_id': dbutils.secrets.get(scope = \"heroku-creds\", key = \"heroku_id\"),\n",
    "                  'heroku_token': dbutils.secrets.get(scope = \"heroku-creds\", key = \"heroku_token\")}\n",
    "\n",
    "    return secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1445bfb4-4d3d-4680-a789-b5552c49baa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret = get_secret()\n",
    "buckets = get_secret(\"author-disambiguation-buckets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644d768f-341e-4d8a-88e0-6addb5a82da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod_save_path = f\"{buckets['and_save_path']}/V3/PROD\"\n",
    "orcid_save_path = f\"{buckets['orcid_save_path']}\"\n",
    "database_copy_save_path = f\"{buckets['database_copy_save_path']}\"\n",
    "temp_save_path = f\"{buckets['temp_save_path']}/latest_tables_for_AND/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24bb3159-cfbb-4eff-ad67-c3d935c8db60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tables to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5377d834-3850-45db-8a19-7581803871f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get histogram_bounds from PostgreSQL pg_stats for the 'paper_id' column\n",
    "df = (spark.read\n",
    "    .format(\"postgresql\")\n",
    "    .option(\"dbtable\", \"\"\"\n",
    "        (SELECT histogram_bounds::text::bigint[] \n",
    "         FROM pg_stats \n",
    "         WHERE tablename = 'affiliation' \n",
    "           AND attname = 'paper_id') new_table\n",
    "    \"\"\")\n",
    "    .option(\"host\", secret['host'])\n",
    "    .option(\"port\", secret['port'])\n",
    "    .option(\"database\", secret['dbname'])\n",
    "    .option(\"user\", secret['username'])\n",
    "    .option(\"password\", secret['password'])\n",
    "    .load())\n",
    "\n",
    "# Extract the histogram bounds array from the collected DataFrame\n",
    "work_id_predicates = df.collect()[0][0]\n",
    "\n",
    "# Validate that bounds were returned\n",
    "if not work_id_predicates or len(work_id_predicates) < 2:\n",
    "    raise ValueError(\"Insufficient histogram bounds to generate predicates.\")\n",
    "\n",
    "# Build full-range predicates using all consecutive bounds\n",
    "final_predicates = []\n",
    "for i in range(len(work_id_predicates) - 1):\n",
    "    final_predicates.append(\n",
    "        f\"paper_id >= {work_id_predicates[i]} AND paper_id < {work_id_predicates[i+1]}\"\n",
    "    )\n",
    "\n",
    "# Add catch-all final predicate for upper bound (optional, but safer)\n",
    "final_predicates.append(f\"paper_id >= {work_id_predicates[-1]}\")\n",
    "\n",
    "# Optionally, you could assign this to testing_new_predicates directly\n",
    "testing_new_predicates = final_predicates\n",
    "\n",
    "# Print how many predicate ranges were generated\n",
    "print(f\"Total JDBC predicates generated: {len(testing_new_predicates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "925b22e8-366d-40ee-bda0-fc93eeef26a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testing_new_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f54a5ac-c159-41f8-933b-33a1b13acbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.work\n",
    "df = (spark.read\n",
    "      .jdbc(\n",
    "              url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "              table=\"(select distinct paper_id, original_title, doi_lower, oa_status, journal_id, merge_into_id, publication_date, type, type_crossref, arxiv_id, is_paratext, best_url, best_free_url, unpaywall_normalize_title, created_date from mid.work) new_table\", \n",
    "              properties={\"user\": secret['username'],\n",
    "                          \"password\": secret['password']}, \n",
    "              predicates=testing_new_predicates))\n",
    "\n",
    "df \\\n",
    "        .repartition(384)\\\n",
    "        .dropDuplicates()\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/mid/work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9ebb56-dc3d-4350-b333-4aa533a350e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.affiliation\n",
    "\n",
    "df = (spark.read\n",
    "      .jdbc(\n",
    "              url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "              table=\"(select distinct paper_id,author_id,affiliation_id,author_sequence_number,original_author,original_orcid,original_affiliation,is_corresponding_author from mid.affiliation) new_table\", \n",
    "              properties={\"user\": secret['username'],\n",
    "                          \"password\": secret['password']}, \n",
    "              predicates=testing_new_predicates))\n",
    "\n",
    "df \\\n",
    "        .repartition(384)\\\n",
    "        .dropDuplicates()\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/mid/affiliation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f87d940a-c469-4b8e-af53-242a265f0003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.work_topic (not needed yet for AND)\n",
    "\n",
    "# df = (spark.read\n",
    "#       .jdbc(\n",
    "#               url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "#               table=\"(select distinct paper_id, topic_id from mid.work_topic) as new_table\", \n",
    "#               properties={\"user\": secret['username'],\n",
    "#                           \"password\": secret['password']}, \n",
    "#               predicates=final_predicates))\n",
    "\n",
    "# df \\\n",
    "#         .write.mode('overwrite') \\\n",
    "#         .parquet(f\"{temp_save_path}work_topic_temp\")\n",
    "\n",
    "# spark.read.parquet(f\"{temp_save_path}work_topic_temp\")\\\n",
    "#     .repartition(256).write.mode('overwrite') \\\n",
    "#     .parquet(f\"{temp_save_path}work_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2964d1f7-34fa-4842-ad7a-8bdedcc3609c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.topic (not needed yet for AND)\n",
    "\n",
    "# df = (spark.read\n",
    "# .format(\"postgresql\")\n",
    "# .option(\"dbtable\", \n",
    "#         f\"(select distinct topic_id, display_name from mid.topic) as new_table\")\n",
    "# .option(\"host\", secret['host'])\n",
    "# .option(\"port\", secret['port'])\n",
    "# .option(\"database\", secret['dbname'])\n",
    "# .option(\"user\", secret['username'])\n",
    "# .option(\"password\", secret['password'])\n",
    "# .load())\n",
    "\n",
    "# df \\\n",
    "# .select('topic_id','display_name') \\\n",
    "# .repartition(64) \\\n",
    "# .write.mode('overwrite') \\\n",
    "# .parquet(f\"{temp_save_path}topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d4d0d0-1acc-4386-9563-830cad94916c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.work_concept\n",
    "\n",
    "df = (spark.read\n",
    "      .jdbc(\n",
    "              url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "              table=\"(select distinct paper_id, field_of_study, score from mid.work_concept where field_of_study not in (17744445,138885662,162324750,144133560,15744967,33923547,71924100,86803240,41008148,127313418,185592680,142362112,144024400,127413603,205649164,95457728,192562407,121332964,39432304) and score > 0.3) as new_table\", \n",
    "              properties={\"user\": secret['username'],\n",
    "                          \"password\": secret['password']}, \n",
    "              predicates=testing_new_predicates))\n",
    "\n",
    "df \\\n",
    "        .repartition(384)\\\n",
    "        .dropDuplicates()\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/mid/work_concept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15125fc0-2d70-456a-bc9e-f1f70bb03041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # mid.work_keyword_concept\n",
    "\n",
    "# df = (spark.read\n",
    "#       .jdbc(\n",
    "#               url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "#               table=\"(select distinct paper_id from mid.work_keyword_concept) as new_table\", \n",
    "#               properties={\"user\": secret['username'],\n",
    "#                           \"password\": secret['password']}, \n",
    "#               predicates=testing_new_predicates))\n",
    "\n",
    "# df \\\n",
    "#         .repartition(384)\\\n",
    "#         .dropDuplicates()\\\n",
    "#         .write.mode('overwrite') \\\n",
    "#         .parquet(f\"{database_copy_save_path}/mid/work_keyword_concept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53794305-03f7-4ff7-a1ba-e034690cd578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.citation\n",
    "\n",
    "df = (spark.read\n",
    "      .jdbc(\n",
    "              url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "              table=\"(select distinct paper_id, paper_reference_id from mid.citation where paper_reference_id is not null) as new_table\", \n",
    "              properties={\"user\": secret['username'],\n",
    "                          \"password\": secret['password']}, \n",
    "              predicates=testing_new_predicates))\n",
    "\n",
    "df \\\n",
    "        .repartition(384)\\\n",
    "        .dropDuplicates()\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/mid/citation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8dab8a2-108f-482c-a6cf-4530ccc5846b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting predicates for reading tables\n",
    "df = (spark.read\n",
    "    .format(\"postgresql\")\n",
    "    .option(\"dbtable\", \"(SELECT histogram_bounds::text::bigint[] FROM pg_stats WHERE tablename = 'author' AND attname = 'author_id') new_table\")\n",
    "    .option(\"host\", secret['host'])\n",
    "    .option(\"port\", secret['port'])\n",
    "    .option(\"database\", secret['dbname'])\n",
    "    .option(\"user\", secret['username'])\n",
    "    .option(\"password\", secret['password'])\n",
    "    .load())\n",
    "\n",
    "author_id_predicates = df.collect()[0][0]\n",
    "author_id_predicates = [x for x in author_id_predicates if x >= 5000000000]\n",
    "\n",
    "if len(author_id_predicates) == 125:\n",
    "    final_predicates = []\n",
    "    final_predicates.append(f\"author_id >= 5000000000 and author_id < {author_id_predicates[2]}\")\n",
    "    for i in range(2, len(author_id_predicates[:-3]), 3):\n",
    "        final_predicates.append(f\"author_id >= {author_id_predicates[i]} and author_id < {author_id_predicates[i+3]}\")\n",
    "    final_predicates.append(f\"author_id >= {author_id_predicates[-2]}\")\n",
    "elif len(author_id_predicates) <= 35:\n",
    "    final_predicates = []\n",
    "    final_predicates.append(f\"author_id >= 5000000000 and author_id < {author_id_predicates[0]}\")\n",
    "    for i in range(len(author_id_predicates[:-1])):\n",
    "        final_predicates.append(f\"author_id >= {author_id_predicates[i]} and author_id < {author_id_predicates[i+1]}\")\n",
    "    final_predicates.append(f\"author_id >= {author_id_predicates[-1]}\")\n",
    "print(len(final_predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63299573-0fd2-48b6-95e4-6ef8acea3b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad78f1ba-23ff-458e-8c4f-75a8fa13dd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.author\n",
    "df = (spark.read\n",
    "      .jdbc(\n",
    "              url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "              table=\"(select distinct author_id, display_name, merge_into_id from mid.author where author_id >= 5000000000) as new_table\", \n",
    "              properties={\"user\": secret['username'],\n",
    "                          \"password\": secret['password']}, \n",
    "              predicates=final_predicates))\n",
    "\n",
    "df \\\n",
    "        .repartition(384)\\\n",
    "        .dropDuplicates()\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/mid/author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f2e8cb-4a66-41f5-b4ad-2d7d28e0e0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orcid.openalex_authorships\n",
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select distinct paper_id, author_sequence_number, orcid, random_num from orcid.openalex_authorships) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"partitionColumn\", \"random_num\")\n",
    "        .option(\"lowerBound\", \"0\")\n",
    "        .option(\"upperBound\", \"50\")\n",
    "        .option(\"numPartitions\", \"26\")\n",
    "        .option(\"fetchsize\", \"200\").load())\n",
    "\n",
    "df \\\n",
    "        .repartition(384).write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/orcid/openalex_authorships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249cf3ab-009b-49c5-9c54-c1e91ee9cbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orcid.add_orcid\n",
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select distinct work_author_id, new_orcid, request_date from orcid.add_orcid) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password']).load())\n",
    "\n",
    "df \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{database_copy_save_path}/orcid/add_orcid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e6247d-203d-461e-983d-354b8f91cbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ORCID and extended attributes tables can be configured later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35b8e3a8-9212-4e73-8457-6c336eceb648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mid.author_orcid\n",
    "\n",
    "# df = (spark.read\n",
    "#       .jdbc(\n",
    "#               url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "#               table=\"(select distinct author_id, orcid, evidence from mid.author_orcid where author_id > 5000000000) as new_table\", \n",
    "#               properties={\"user\": secret['username'],\n",
    "#                           \"password\": secret['password']}, \n",
    "#               predicates=final_predicates))\n",
    "\n",
    "# df \\\n",
    "#         .write.mode('overwrite') \\\n",
    "#         .parquet(f\"{temp_save_path}author_orcid_temp\")\n",
    "\n",
    "# spark.read.parquet(f\"{temp_save_path}author_orcid_temp\")\\\n",
    "#     .repartition(256).write.mode('overwrite') \\\n",
    "#     .parquet(f\"{temp_save_path}author_orcid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18d7243f-01ff-476d-99fc-596cfbee19d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# legacy.mag_main_author_extended_attributes\n",
    "\n",
    "# df = (spark.read\n",
    "#       .jdbc(\n",
    "#               url=f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\",\n",
    "#               table=\"(select author_id, attribute_value as alternate_name from legacy.mag_main_author_extended_attributes where author_id > 5000000000 and attribute_type=1) as new_table\", \n",
    "#               properties={\"user\": secret['username'],\n",
    "#                           \"password\": secret['password']}, \n",
    "#               predicates=final_predicates))\n",
    "\n",
    "# df \\\n",
    "#         .write.mode('overwrite') \\\n",
    "#         .parquet(f\"{temp_save_path}author_alternate_names_temp\")\n",
    "\n",
    "# spark.read.parquet(f\"{temp_save_path}author_alternate_names_temp\")\\\n",
    "#     .repartition(256).write.mode('overwrite') \\\n",
    "#     .parquet(f\"{temp_save_path}author_alternate_names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d03c95-802a-4184-9412-55f2f2dacff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating new features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee54d97-d48c-415b-9492-b9fc22331a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "work =spark.read.parquet(f\"{database_copy_save_path}/mid/work\").dropDuplicates().filter(F.col('merge_into_id').isNull()).select('paper_id')\n",
    "work.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9859dd68-2b5e-42a0-9ac7-3e4508e03d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "affiliations = spark.read.parquet(f\"{database_copy_save_path}/mid/affiliation\").dropDuplicates() \\\n",
    "    .join(work, how='inner', on='paper_id') \\\n",
    "    .withColumn('work_author_id', F.concat_ws(\"_\", F.col('paper_id'), F.col('author_sequence_number'))) \\\n",
    "    .fillna(-1, subset=['author_id']) \\\n",
    "    .filter(F.col('author_id')==-1)\n",
    "affiliations.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432502c4-3082-449c-995e-6083553bb9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('work_author_id').orderBy(F.col('author_name_len').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce56bfc-5d89-4ba2-8679-4784fb8c3b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coauthors = affiliations.dropDuplicates() \\\n",
    "    .dropDuplicates(subset=['work_author_id','paper_id','original_author']) \\\n",
    "    .withColumn('author_name_len', F.length(F.col('original_author'))) \\\n",
    "    .withColumn('work_author_rank', F.row_number().over(w1)) \\\n",
    "    .filter(F.col('work_author_rank')==1) \\\n",
    "    .groupBy('paper_id').agg(F.collect_set(F.col('original_author')).alias('coauthors'))\n",
    "coauthors.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5639aa-97e7-40af-8f78-4095bcde72cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "work_concept = spark.read.parquet(f\"{database_copy_save_path}/mid/work_concept\").dropDuplicates() \\\n",
    "    .join(affiliations.select('paper_id').dropDuplicates(), on='paper_id', how='inner') \\\n",
    "    .groupby('paper_id').agg(F.collect_set(F.col('field_of_study')).alias('concepts'))\n",
    "work_concept.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a483ddd-147d-4e76-8489-9996c0d81588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "citation = spark.read.parquet(f\"{database_copy_save_path}/mid/citation\").dropDuplicates()\\\n",
    "    .join(affiliations.select('paper_id').dropDuplicates(), on='paper_id', how='inner') \\\n",
    "    .groupBy('paper_id').agg(F.collect_set(F.col('paper_reference_id')).alias('citations'))\n",
    "citation.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c061b8-36d3-45d1-bc61-df5d62a66245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Looking at current authors table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e23a37-007c-4341-865b-d4dd3417148f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# author = spark.read.parquet(f\"{temp_save_path}author\")\n",
    "# author.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29060584-5509-4ddd-8cbf-5e3434614f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# author_orcid = spark.read.parquet(f\"{temp_save_path}author_orcid\").dropDuplicates()\n",
    "# author_orcid.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff46ee4d-59ef-490c-84fe-e65456300492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# alternate_names = spark.read.parquet(f\"{temp_save_path}author_alternate_names\") \\\n",
    "#     .groupBy('author_id').agg(F.collect_set(F.col('alternate_name')).alias('alternate_names'))\n",
    "# alternate_names.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c02fc114-eb5b-4432-8a7d-5e42828b66a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Getting authors that need to go through AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab91183-2882-49e0-954e-f7b6f6f9b2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_orcid_from_list(orcid_list):\n",
    "    if isinstance(orcid_list, list):\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    elif isinstance(orcid_list, set):\n",
    "        orcid_list = list(orcid_list)\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    else:\n",
    "        orcid = ''\n",
    "    return orcid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbb4682-4939-4ad1-8011-3d255a921cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# work_author_id, original_author, orcid, concepts, institutions, citations, coauthors, created_date\n",
    "affiliations \\\n",
    "    .groupBy('work_author_id','original_author','author_id','paper_id')\\\n",
    "    .agg(F.collect_set(F.col('affiliation_id')).alias('institutions'), \n",
    "         F.collect_set(F.col('original_orcid')).alias('orcid')) \\\n",
    "    .withColumn('final_orcid', get_orcid_from_list(F.col('orcid'))) \\\n",
    "    .withColumn('author_name_len', F.length(F.col('original_author'))) \\\n",
    "    .withColumn('work_author_rank', F.row_number().over(w1)) \\\n",
    "    .filter(F.col('work_author_rank')==1) \\\n",
    "    .join(work_concept, how='left', on='paper_id') \\\n",
    "    .join(citation, how='left', on='paper_id') \\\n",
    "    .join(coauthors, how='left', on='paper_id') \\\n",
    "    .withColumn(\"created_date\", F.current_timestamp()) \\\n",
    "    .select(\"work_author_id\", \"original_author\", F.col(\"final_orcid\").alias('orcid'), \"concepts\", \"institutions\", \"citations\", \"coauthors\", \"created_date\") \\\n",
    "    .write.mode('overwrite')\\\n",
    "    .parquet(f\"{prod_save_path}/input_data_for_AND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb7f962-b55a-4ceb-9b05-1381ca0b1dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AND_LoadPostgresData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
