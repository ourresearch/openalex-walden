{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b0e79e-542c-43fd-a1d6-1bf1fc87084f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b116e50-81fd-4943-bd42-7b3d5864dde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, pipeline, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "MODEL_PATH = \"/Volumes/openalex/works/models/topic-classification-title-abstract\"\n",
    "\n",
    "class ModelCache:\n",
    "    model = None\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        if cls.model is not None:\n",
    "            return\n",
    "\n",
    "        # Determine available GPU count\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        if num_devices == 0:\n",
    "            cls.assigned_device = -1\n",
    "        else:\n",
    "            # Assign device using pid hash\n",
    "            cls.assigned_device = os.getpid() % num_devices\n",
    "\n",
    "        cls.model = pipeline(\n",
    "            task = \"text-classification\",\n",
    "            model = MODEL_PATH,\n",
    "            device = cls.assigned_device,  # âœ… Assign GPU or CPU (-1)\n",
    "            # device_map=\"auto\",\n",
    "            top_k=3,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            truncation = True,\n",
    "            max_length = 512\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c44046a-c315-4339-af7f-7a58e0f1c98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load input data and cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aeedb97-b253-4bae-a25e-f78d7bce04f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.table(\"openalex.works.work_topics_frontfill_input\")\n",
    "      .select(\"work_id\", \"title\", \"abstract\", \"journal_name\")\n",
    "      .limit(6700000)\n",
    "      .repartition(384)\n",
    ")\n",
    "\n",
    "print(f\"Input Row Count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a40957e-f211-4beb-9944-c31c8647097d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Secondary Fill - create abstract from Journal Name and Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9f1fc6-145f-4895-900f-2f7881a6a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.sql(\"\"\"\n",
    "# SELECT --format_number(count(*),0)\n",
    "#     id as work_id,\n",
    "#     title,\n",
    "#     concat('[Journal Name] ', primary_location.source.display_name, '\\n[Key Concepts] ', \n",
    "#       concat_ws(', ', slice(concepts.display_name,1,3))) as abstract,\n",
    "#     primary_location.source.display_name as journal_name\n",
    "# FROM openalex.works.openalex_works\n",
    "# LEFT ANTI JOIN openalex.works.work_topics_lm_output lm ON id = lm.work_id\n",
    "# WHERE (topics IS NULL OR size(topics) = 0)\n",
    "#     AND id > 6600000000\n",
    "#     AND length(title) > 10\n",
    "#     AND length(abstract) > 30\n",
    "# LIMIT 6760000;\n",
    "# \"\"\").repartition(384).cache()\n",
    "# print(f\"Input Row Count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08a8c7fd-8e94-4151-bf89-e55c7682aeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbafcfd-f49d-442c-858c-39554058b2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from topic_predictor import *\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "def process_partition(rows, batch_size=BATCH_SIZE):\n",
    "    ModelCache.load()\n",
    "    model = ModelCache.model\n",
    "\n",
    "    batch_rows = []\n",
    "    batch_texts = []\n",
    "\n",
    "    def yield_batch(rows_batch, texts_batch):\n",
    "        try:\n",
    "            batch_outputs = model(texts_batch)\n",
    "        except Exception as e:\n",
    "            batch_outputs = [[] for _ in texts_batch]  # fail-safe: empty predictions\n",
    "\n",
    "        for row, output in zip(rows_batch, batch_outputs):\n",
    "            row_dict = row.asDict()\n",
    "            lm_output = [\n",
    "                {\n",
    "                    \"topic_id\": 10000 + int(topic[\"label\"].split(\":\")[0]),\n",
    "                    \"score\": float(topic[\"score\"])\n",
    "                }\n",
    "                for topic in output\n",
    "            ] if output else None\n",
    "\n",
    "            row_dict[\"lm_topics\"] = lm_output\n",
    "            yield row_dict\n",
    "\n",
    "    for row in rows:\n",
    "        if row is None:\n",
    "            continue\n",
    "\n",
    "        title = clean_title(row['title']) or \"\"\n",
    "        abstract = clean_abstract(row['abstract']) or \"\"\n",
    "        full_text = f\"[CLS]<TITLE> {title.strip()} <ABSTRACT> {abstract.strip()} [SEP]\"\n",
    "\n",
    "        batch_rows.append(row)\n",
    "        batch_texts.append(full_text)\n",
    "\n",
    "        if len(batch_texts) >= batch_size:\n",
    "            yield from yield_batch(batch_rows, batch_texts)\n",
    "            batch_rows = []\n",
    "            batch_texts = []\n",
    "\n",
    "    # Process remaining rows\n",
    "    if batch_rows:\n",
    "        yield from yield_batch(batch_rows, batch_texts)\n",
    "\n",
    "topic_struct = StructType([\n",
    "    StructField(\"topic_id\", IntegerType(), True),\n",
    "    StructField(\"score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"work_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True),\n",
    "    StructField(\"journal_name\", StringType(), True),\n",
    "    StructField(\"lm_topics\", ArrayType(topic_struct), True)\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "res_rdd = df.select(\"work_id\", \"title\", \"abstract\", \"journal_name\").rdd.mapPartitions(process_partition)\n",
    "res_df = spark.createDataFrame(res_rdd, output_schema).cache()\n",
    "output_count = res_df.count()\n",
    "print(f\"Output Row count: {output_count}\")\n",
    "\n",
    "res_df = (res_df.select(\"work_id\", \"title\", \"abstract\", \"journal_name\", \"lm_topics\")\n",
    "                .withColumn(\"lm_primary_topic\", col(\"lm_topics\")[0])\n",
    "                .withColumn(\"source\", lit(\"bert_lm\"))\n",
    "                .withColumn(\"created_timestamp\", current_timestamp()))\n",
    "res_df.write.mode(\"append\").saveAsTable(\"openalex.works.work_topics_lm_output\")\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "print(f\"Total runtime: {runtime:.4f} seconds\")\n",
    "print(f\"Total throughput: {output_count / (runtime):.4f} inferences/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48482aa7-8554-4b1b-a69a-c74d18754f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM openalex.works.work_topics_lm_output;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924a858f-9113-49bc-86cd-471a20a0132f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform `lm_output` to OpenAlex Topics Structs, Insert to frontfill if does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064bd237-5f2b-491c-88d4-250136ed1176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO openalex.works.work_topics_frontfill AS target\n",
    "USING (\n",
    "  WITH topics_metadata AS (\n",
    "    SELECT\n",
    "      topic_id,\n",
    "      t.display_name,\n",
    "      NAMED_STRUCT(\n",
    "        'id', concat('https://openalex.org/subfields/', s.subfield_id),\n",
    "        'display_name', s.display_name\n",
    "      ) AS subfield,\n",
    "      NAMED_STRUCT(\n",
    "        'id', concat('https://openalex.org/fields/', f.field_id),\n",
    "        'display_name', f.display_name\n",
    "      ) AS field,\n",
    "      NAMED_STRUCT(\n",
    "        'id', concat('https://openalex.org/domains/', d.domain_id),\n",
    "        'display_name', d.display_name\n",
    "      ) AS domain\n",
    "    FROM openalex.common.topics t\n",
    "    JOIN openalex.common.subfields s USING (subfield_id)\n",
    "    JOIN openalex.common.fields f USING (field_id)\n",
    "    JOIN openalex.common.domains d USING (domain_id)\n",
    "  ),\n",
    "\n",
    "  lm_output_exploded AS (\n",
    "    SELECT \n",
    "      work_id,\n",
    "      explode(lm_topics) AS result,\n",
    "      source,\n",
    "      created_timestamp\n",
    "    FROM openalex.works.work_topics_lm_output\n",
    "  )\n",
    "\n",
    "  SELECT\n",
    "    work_id,\n",
    "    slice(array_sort(\n",
    "      array_agg(\n",
    "        NAMED_STRUCT(\n",
    "          'id', concat('https://openalex.org/T', result.topic_id),\n",
    "          'display_name', tm.display_name,\n",
    "          'score', result.score,\n",
    "          'subfield', tm.subfield,\n",
    "          'field', tm.field,\n",
    "          'domain', tm.domain\n",
    "        )\n",
    "      ),\n",
    "      (left, right) -> CASE\n",
    "        WHEN left.score > right.score THEN -1\n",
    "        WHEN left.score < right.score THEN 1\n",
    "        ELSE 0\n",
    "      END\n",
    "    ), 1, 3) AS topics,\n",
    "    first(wt.source) AS source,\n",
    "    max(wt.created_timestamp) AS created_datetime,\n",
    "    max(wt.created_timestamp) AS updated_datetime\n",
    "  FROM lm_output_exploded wt\n",
    "  JOIN topics_metadata tm ON tm.topic_id = result.topic_id\n",
    "  GROUP BY work_id\n",
    ") AS source\n",
    "ON target.work_id = source.work_id\n",
    "\n",
    "-- Insert only if the work_id does not exist\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "  work_id,\n",
    "  topics,\n",
    "  source,\n",
    "  created_datetime,\n",
    "  updated_datetime\n",
    ") VALUES (\n",
    "  source.work_id,\n",
    "  source.topics,\n",
    "  source.source,\n",
    "  source.created_datetime,\n",
    "  source.updated_datetime\n",
    ");\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6025229697112346,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "topics_frontfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
