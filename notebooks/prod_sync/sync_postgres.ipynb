{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42a3d7f1-244d-4bdc-8788-9011ac8ef270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get secrets for postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3371c306-2397-4d9e-a466-8e2f489b6bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "secret = {'username': dbutils.secrets.get(scope = \"postgres-works\", key = \"user\"),\n",
    "        'password': dbutils.secrets.get(scope = \"postgres-works\", key = \"password\"),\n",
    "        'host': dbutils.secrets.get(scope = \"postgres-works\", key = \"host\"),\n",
    "        'dbname': dbutils.secrets.get(scope = \"postgres-works\", key = \"dbname\"),\n",
    "        'port': dbutils.secrets.get(scope = \"postgres-works\", key = \"port\"),\n",
    "        'engine': dbutils.secrets.get(scope = \"postgres-works\", key = \"engine\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57d17603-59c1-410e-b6ab-1b5b7991863c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sync `openalex.mid.work_concept`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35c1af9-9cf0-49f8-a0d1-06ae068630b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select * from mid.work_concept) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"partitionColumn\", \"paper_id\")\n",
    "        .option(\"lowerBound\", \"0\")\n",
    "        .option(\"upperBound\", \"4624002859\")\n",
    "        .option(\"numPartitions\", \"512\").load())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"openalex.mid.work_concept\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413e84dd-c6a4-41d8-b6c9-06ce40137e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select * from mid.work_topic) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"partitionColumn\", \"paper_id\")\n",
    "        .option(\"lowerBound\", \"0\")\n",
    "        .option(\"upperBound\", \"4524002859\")\n",
    "        .option(\"numPartitions\", \"512\").load())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"openalex.mid.work_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c301fa0-0233-4613-b586-0652a7a96b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select * from mid.author) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"partitionColumn\", \"author_id\")\n",
    "        .option(\"lowerBound\", \"0\")\n",
    "        .option(\"upperBound\", \"10000109602\")\n",
    "        .option(\"numPartitions\", \"1024\").load())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"openalex.mid.author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312dcfd7-ac22-4ba3-993f-0a5c334dc1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", f\"(select * from mid.work_sdg) as new_table\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"partitionColumn\", \"paper_id\")\n",
    "        .option(\"lowerBound\", \"0\")\n",
    "        .option(\"upperBound\", \"4515768804\")\n",
    "        .option(\"numPartitions\", \"512\").load())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"openalex.mid.work_sdg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef133738-4b59-4e64-b049-67ce52289cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT min(paper_id), max(paper_id), count(*) from openalex_postgres.mid.work_sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3e715f-948e-4817-b894-839404a8b20b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE openalex.mid.author CLUSTER BY (author_id, normalized_title)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7470917978606669,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_postgres",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
