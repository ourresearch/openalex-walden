{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone Parseland Parsing Job\n",
    "\n",
    "Reads new HTML records from `openalex.taxicab.taxicab_results`, calls the Parseland API to extract\n",
    "structured metadata (authors, URLs, license, version, abstract), and appends results to\n",
    "`openalex.parseland.parsed_pages`.\n",
    "\n",
    "**input**: `openalex.taxicab.taxicab_results` (HTML records not yet in `parsed_pages`)\n",
    "\n",
    "**process**: Parseland API on ECS (GET /parseland/{taxicab_id})\n",
    "\n",
    "**output**: `openalex.parseland.parsed_pages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSELAND_URL = \"http://parseland-load-balancer-667160048.us-east-1.elb.amazonaws.com/parseland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_schema = T.StructType([\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"is_corresponding\", T.BooleanType(), True),\n",
    "    T.StructField(\"affiliations\", T.ArrayType(\n",
    "        T.StructType([\n",
    "            T.StructField(\"name\", T.StringType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "url_schema = T.StructType([\n",
    "    T.StructField(\"url\", T.StringType(), True),\n",
    "    T.StructField(\"content_type\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "parsed_pages_schema = T.StructType([\n",
    "    T.StructField(\"taxicab_id\", T.StringType(), True),\n",
    "    T.StructField(\"url\", T.StringType(), True),\n",
    "    T.StructField(\"resolved_url\", T.StringType(), True),\n",
    "    T.StructField(\"native_id\", T.StringType(), True),\n",
    "    T.StructField(\"native_id_namespace\", T.StringType(), True),\n",
    "    T.StructField(\"authors\", T.ArrayType(author_schema), True),\n",
    "    T.StructField(\"urls\", T.ArrayType(url_schema), True),\n",
    "    T.StructField(\"license\", T.StringType(), True),\n",
    "    T.StructField(\"version\", T.StringType(), True),\n",
    "    T.StructField(\"abstract\", T.StringType(), True),\n",
    "    T.StructField(\"had_error\", T.BooleanType(), True),\n",
    "    T.StructField(\"parsed_date\", T.TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS openalex.parseland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS openalex.parseland.parsed_pages (\n",
    "  taxicab_id STRING,\n",
    "  url STRING,\n",
    "  resolved_url STRING,\n",
    "  native_id STRING,\n",
    "  native_id_namespace STRING,\n",
    "  authors ARRAY<STRUCT<name: STRING, is_corresponding: BOOLEAN, affiliations: ARRAY<STRUCT<name: STRING>>>>,\n",
    "  urls ARRAY<STRUCT<url: STRING, content_type: STRING>>,\n",
    "  license STRING,\n",
    "  version STRING,\n",
    "  abstract STRING,\n",
    "  had_error BOOLEAN,\n",
    "  parsed_date TIMESTAMP\n",
    ")\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new HTML records not yet parsed\n",
    "taxicab_results = (\n",
    "    spark.read.table(\"openalex.taxicab.taxicab_results\")\n",
    "    .filter(\n",
    "        F.col(\"taxicab_id\").isNotNull() &\n",
    "        F.col(\"content_type\").contains(\"html\")\n",
    "    )\n",
    "    .select(\"taxicab_id\", \"url\", \"resolved_url\", \"native_id\", \"native_id_namespace\")\n",
    ")\n",
    "\n",
    "already_parsed = (\n",
    "    spark.read.table(\"openalex.parseland.parsed_pages\")\n",
    "    .select(\"taxicab_id\")\n",
    ")\n",
    "\n",
    "to_parse = taxicab_results.join(already_parsed, \"taxicab_id\", \"left_anti\")\n",
    "\n",
    "to_parse_count = to_parse.count()\n",
    "print(f\"Records to parse: {to_parse_count:,}\")\n",
    "\n",
    "if to_parse_count == 0:\n",
    "    dbutils.notebook.exit(\"No new records to parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_parse_pd = to_parse.toPandas()\n",
    "records = to_parse_pd.to_dict('records')\n",
    "print(f\"Collected {len(records):,} records for parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session(max_workers):\n",
    "    \"\"\"Create a requests session with proper connection pooling.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        pool_connections=max_workers,\n",
    "        pool_maxsize=max_workers,\n",
    "        max_retries=retries\n",
    "    )\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "def call_parseland(taxicab_id, session, timeout=30):\n",
    "    \"\"\"Call Parseland API for a single taxicab_id.\"\"\"\n",
    "    try:\n",
    "        response = session.get(f\"{PARSELAND_URL}/{taxicab_id}\", timeout=timeout)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                \"authors\": data.get(\"authors\") or [],\n",
    "                \"urls\": data.get(\"urls\") or [],\n",
    "                \"license\": str(data.get(\"license\") or \"\"),\n",
    "                \"version\": str(data.get(\"version\") or \"\"),\n",
    "                \"abstract\": str(data.get(\"abstract\") or \"\"),\n",
    "                \"had_error\": False\n",
    "            }\n",
    "        else:\n",
    "            return {\"authors\": [], \"urls\": [], \"license\": \"\", \"version\": \"\", \"abstract\": \"\", \"had_error\": True}\n",
    "    except Exception as e:\n",
    "        return {\"authors\": [], \"urls\": [], \"license\": \"\", \"version\": \"\", \"abstract\": \"\", \"had_error\": True}\n",
    "\n",
    "\n",
    "def process_record(record, session):\n",
    "    \"\"\"Process a single record: call parseland and combine with metadata.\"\"\"\n",
    "    result = call_parseland(record[\"taxicab_id\"], session)\n",
    "    return {\n",
    "        \"taxicab_id\": record[\"taxicab_id\"],\n",
    "        \"url\": record[\"url\"],\n",
    "        \"resolved_url\": record[\"resolved_url\"],\n",
    "        \"native_id\": record[\"native_id\"],\n",
    "        \"native_id_namespace\": record[\"native_id_namespace\"],\n",
    "        **result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 48\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "session = create_session(MAX_WORKERS)\n",
    "start_time = time.time()\n",
    "total_written = 0\n",
    "batch = []\n",
    "\n",
    "print(f\"Starting Parseland parsing with {MAX_WORKERS} workers\")\n",
    "print(f\"Writing results in batches of {BATCH_SIZE:,}\")\n",
    "\n",
    "def flush_batch(batch):\n",
    "    global total_written\n",
    "    now = datetime.datetime.now(timezone.utc)\n",
    "    for r in batch:\n",
    "        r[\"parsed_date\"] = now\n",
    "    df = spark.createDataFrame(batch, schema=parsed_pages_schema)\n",
    "    df.write.mode(\"append\").format(\"delta\").saveAsTable(\"openalex.parseland.parsed_pages\")\n",
    "    total_written += len(batch)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = total_written / elapsed if elapsed > 0 else 0\n",
    "    print(f\"  >> Flushed {len(batch):,} results ({total_written:,} total, {rate:.1f}/sec)\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(process_record, record, session): record\n",
    "        for record in records\n",
    "    }\n",
    "    for count, future in enumerate(as_completed(futures), 1):\n",
    "        if count % 5000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = count / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(records) - count) / rate / 60 if rate > 0 else 0\n",
    "            print(f\"Processed {count:,}/{len(records):,} ({rate:.1f}/sec, ~{remaining:.1f} min remaining)\")\n",
    "        batch.append(future.result())\n",
    "        if len(batch) >= BATCH_SIZE:\n",
    "            flush_batch(batch)\n",
    "            batch = []\n",
    "\n",
    "if batch:\n",
    "    flush_batch(batch)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nDone: {total_written:,} records parsed in {elapsed/60:.1f} min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}