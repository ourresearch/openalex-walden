{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d65718f-ee9f-4679-883b-d1f31b169f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This pipeline scrapes landing pages and PDFs. The files are stored in Cloudflare R2 and the metadata is saved in a table\n",
    "\n",
    "**input**: recent crossref records, repo records, and PDF urls from landing page records\n",
    "\n",
    "**process**: taxicab API on ECS\n",
    "\n",
    "**output**: file id, url, related ids saved to `openalex.taxicab.taxicab_results`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fcc4d1-c95f-46af-b7a0-50064dcca58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "from urllib3.util.retry import Retry\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "import pandas as pd\n",
    "from datetime import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12d4d8e-9cc9-4d52-9e13-2853d154231f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = \"http://harvester-load-balancer-366186003.us-east-1.elb.amazonaws.com/taxicab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b08aeb-858e-449b-b7e7-14ddbb879c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS openalex.taxicab.taxicab_results (\n",
    "  taxicab_id STRING,\n",
    "  url STRING,\n",
    "  resolved_url STRING,\n",
    "  status_code INT,\n",
    "  content_type STRING,\n",
    "  native_id STRING,\n",
    "  native_id_namespace STRING,\n",
    "  s3_path STRING,\n",
    "  is_soft_block BOOLEAN,\n",
    "  created_date TIMESTAMP,\n",
    "  processed_date TIMESTAMP,\n",
    "  error STRING\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7769bae1-c7d6-451c-acf2-cce57715ce54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_datetime(value):\n",
    "    \"\"\"Convert various date/time formats to datetime.datetime with UTC timezone\"\"\"\n",
    "    if value is None:\n",
    "        return datetime.datetime.now(timezone.utc)\n",
    "    \n",
    "    # Handle pandas Timestamp objects specifically\n",
    "    if hasattr(value, 'to_pydatetime'):\n",
    "        dt = value.to_pydatetime()\n",
    "        if dt.tzinfo is None:\n",
    "            return dt.replace(tzinfo=timezone.utc)\n",
    "        return dt\n",
    "    \n",
    "    # Handle regular datetime objects\n",
    "    if isinstance(value, datetime.datetime):\n",
    "        if value.tzinfo is None:\n",
    "            return value.replace(tzinfo=timezone.utc)\n",
    "        return value\n",
    "    \n",
    "    # Handle date objects (convert to datetime)\n",
    "    if isinstance(value, datetime.date):\n",
    "        return datetime.datetime.combine(value, datetime.time(0, 0, 0, tzinfo=timezone.utc))\n",
    "    \n",
    "    # Handle string dates\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            # Try parsing ISO format first\n",
    "            if 'T' in value or '+' in value or 'Z' in value:\n",
    "                return datetime.datetime.fromisoformat(value.replace('Z', '+00:00'))\n",
    "            else:\n",
    "                # Try pandas to_datetime as fallback, then convert to datetime\n",
    "                pd_timestamp = pd.to_datetime(value)\n",
    "                return pd_timestamp.to_pydatetime().replace(tzinfo=timezone.utc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing date string '{value}': {e}\")\n",
    "            return datetime.datetime.now(timezone.utc)\n",
    "    \n",
    "    # Fallback for any other type\n",
    "    print(f\"Unexpected date type: {type(value)} - {value}\")\n",
    "    return datetime.datetime.now(timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90bc997-92f8-43df-be1d-a03c30b0455f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_processed_field = \"created_date\"\n",
    "post_2023 = convert_to_datetime(datetime.date(2023, 1, 1))\n",
    "print(f\"Using fixed last processed date: {post_2023}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a679de29-b6e1-436c-b410-b001c87186be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# urls to scrape\n",
    "\n",
    "spark.catalog.refreshTable(\"openalex.taxicab.taxicab_results\")\n",
    "\n",
    "taxicab_finished_tasks = (\n",
    "    spark.table(\"openalex.taxicab.taxicab_results\")\n",
    "    .withColumn(\"norm_url\", F.regexp_replace(\"url\", r\"^https?://(dx\\.)?\", \"\"))\n",
    "    .select(\n",
    "        F.lower(F.col(\"native_id\")).alias(\"finished_id\"),\n",
    "        F.col(\"norm_url\").alias(\"finished_url\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "def normalize_created_date(df):\n",
    "    return df.withColumn(\n",
    "        \"created_date\",\n",
    "        F.to_timestamp(\"created_date\")\n",
    "    )\n",
    "\n",
    "taxicab_results = normalize_created_date(\n",
    "    spark.table(\"openalex.taxicab.taxicab_results\").select(\"url\", \"created_date\")\n",
    ")\n",
    "\n",
    "recent_crossref_works = (\n",
    "    normalize_created_date(\n",
    "        spark.read.table(\"openalex.crossref.crossref_works\")\n",
    "        .filter(F.col(last_processed_field) >= F.lit(post_2023))\n",
    "    )\n",
    "    # Explode URLs if a DOI has more than one, or just filter for doi.org\n",
    "    .select(\n",
    "        \"native_id\",\n",
    "        \"native_id_namespace\",\n",
    "        \"created_date\",\n",
    "        F.explode(\"urls\").alias(\"url_struct\")\n",
    "    )\n",
    "    .withColumn(\"url\", F.col(\"url_struct.url\"))\n",
    "    .filter(F.col(\"url\").contains(\"doi.org\"))\n",
    "    # Normalize the source URL exactly like the finished tasks\n",
    "    .withColumn(\"norm_url_src\", F.regexp_replace(\"url\", r\"^https?://(dx\\.)?\", \"\"))\n",
    "    .alias(\"src\")\n",
    "    .join(\n",
    "        taxicab_finished_tasks.alias(\"fin\"),\n",
    "        (F.lower(F.col(\"src.native_id\")) == F.col(\"fin.finished_id\")) &\n",
    "        (F.col(\"src.norm_url_src\") == F.col(\"fin.finished_url\")),\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    .select(\"src.native_id\", \"src.native_id_namespace\", \"src.url\", \"src.created_date\")\n",
    ")\n",
    "\n",
    "recent_pdf_works = None\n",
    "if spark.catalog.tableExists(\"openalex.landing_page.landing_page_works\"):\n",
    "    recent_pdf_works = (\n",
    "        normalize_created_date(\n",
    "            spark.read\n",
    "            .table(\"openalex.landing_page.landing_page_works\")\n",
    "            .filter(F.col(last_processed_field) >= F.date_sub(F.lit(post_2023), 2))\n",
    "            .select(\n",
    "                \"ids\",\n",
    "                \"native_id\",\n",
    "                \"native_id_namespace\",\n",
    "                F.expr(\"get(filter(urls, x -> x.content_type = 'pdf'), 0).url\").alias(\"url\"),\n",
    "                \"created_date\"\n",
    "            )\n",
    "            .withColumn(\"pmh_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'pmh'), 0).id\"))\n",
    "            .withColumn(\"doi_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'doi'), 0).id\"))\n",
    "            # Set priority: PMH first, then DOI, then original\n",
    "            .withColumn(\"final_native_id\",\n",
    "                F.when(F.col(\"pmh_id\").isNotNull(), F.col(\"pmh_id\"))\n",
    "                .when(F.col(\"doi_id\").isNotNull(), F.col(\"doi_id\"))\n",
    "                .otherwise(F.col(\"native_id\")))\n",
    "            .withColumn(\"final_namespace\",\n",
    "                F.when(F.col(\"pmh_id\").isNotNull(), F.lit(\"pmh\"))\n",
    "                .when(F.col(\"doi_id\").isNotNull(), F.lit(\"doi\"))\n",
    "                .otherwise(F.col(\"native_id_namespace\")))\n",
    "            # select final columns\n",
    "            .select(\n",
    "                F.col(\"final_native_id\").alias(\"native_id\"),\n",
    "                F.col(\"final_namespace\").alias(\"native_id_namespace\"),\n",
    "                \"url\",\n",
    "                \"created_date\"\n",
    "            )\n",
    "            .filter(F.col(\"url\").isNotNull())\n",
    "        )\n",
    "        .join(\n",
    "            taxicab_results,\n",
    "            [\"url\"],\n",
    "            \"left_anti\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "recent_repo_works = (\n",
    "    normalize_created_date(\n",
    "        spark.read.table(\"openalex.repo.repo_works\")\n",
    "        .filter(F.col(last_processed_field) >= F.lit(post_2023))\n",
    "        .select(\n",
    "            \"native_id\",\n",
    "            \"native_id_namespace\",\n",
    "            F.slice(\"urls\", 1, 3).alias(\"urls\"),\n",
    "            \"created_date\"\n",
    "        )\n",
    "        .filter(F.col(\"urls\").isNotNull())\n",
    "        .select(\"*\", F.explode(\"urls\").alias(\"url_struct\"))\n",
    "        .select(\n",
    "            \"native_id\",\n",
    "            \"native_id_namespace\",\n",
    "            \"created_date\",\n",
    "            F.col(\"url_struct.url\").alias(\"url\")\n",
    "        )\n",
    "        .filter(~F.col(\"url\").contains(\"doi.org\"))\n",
    "    )\n",
    "    .join(\n",
    "        taxicab_results,\n",
    "        [\"url\"],\n",
    "        \"left_anti\"\n",
    "    )\n",
    ")\n",
    "\n",
    "all_urls = recent_crossref_works\n",
    "if recent_pdf_works is not None:\n",
    "    all_urls = recent_crossref_works.unionByName(\n",
    "        recent_pdf_works,\n",
    "        allowMissingColumns=True\n",
    "    ).unionByName(\n",
    "        recent_repo_works,\n",
    "        allowMissingColumns=True\n",
    "    )\n",
    "else:\n",
    "    all_urls = recent_crossref_works.unionByName(\n",
    "        recent_repo_works,\n",
    "        allowMissingColumns=True\n",
    "    )\n",
    "\n",
    "recent_pdf_count = (recent_pdf_works and recent_pdf_works.count()) or 0\n",
    "print(f\"{recent_crossref_works.count()} crossref URLs, {recent_repo_works.count()} repo URLs, {recent_pdf_count} PDF URLs remaining\")\n",
    "print(f\"{all_urls.count()} total URLs remaining\")\n",
    "\n",
    "all_urls = all_urls.orderBy(\"created_date\").limit(250000)\n",
    "\n",
    "current_date = datetime.datetime.now(timezone.utc)\n",
    "\n",
    "# modify the DataFrame to replace extreme dates with current date\n",
    "bounded_all_urls = all_urls.withColumn(\n",
    "    \"created_date\",\n",
    "    F.when(\n",
    "        F.col(\"created_date\").isNull() |\n",
    "        (F.year(F.col(\"created_date\")) < 1900) | \n",
    "        (F.year(F.col(\"created_date\")) > 2100),\n",
    "        F.lit(current_date)\n",
    "    ).otherwise(F.col(\"created_date\"))\n",
    ")\n",
    "\n",
    "all_urls_pd = bounded_all_urls.toPandas()\n",
    "\n",
    "# convert to a JSON-compatible list and filter out None values\n",
    "jsonUrls = []\n",
    "for row in all_urls_pd.to_dict('records'):\n",
    "    if row[\"url\"] is not None:\n",
    "        formatted_date = convert_to_datetime(row[\"created_date\"])\n",
    "        \n",
    "        entry = {\n",
    "            \"url\": row[\"url\"],\n",
    "            \"created_date\": formatted_date,\n",
    "            \"native_id\": row.get(\"native_id\", \"\"),\n",
    "            \"native_id_namespace\": row.get(\"native_id_namespace\", \"\")\n",
    "        }\n",
    "        jsonUrls.append(entry)\n",
    "\n",
    "total_urls = len(jsonUrls)\n",
    "pdf_urls = sum(1 for url in jsonUrls if '.pdf' in url['url'].lower())\n",
    "doi_urls = sum(1 for url in jsonUrls if 'doi.org' in url['url'].lower())\n",
    "other_urls = total_urls - pdf_urls - doi_urls\n",
    "\n",
    "print(f\"Harvesting {total_urls} URLs ({pdf_urls} PDFs, {doi_urls} DOIs, {other_urls} other URLs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4172f8a-cff0-4d9d-a67a-9bdbfcdc17f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result schema\n",
    "\n",
    "results_schema = T.StructType([\n",
    "    T.StructField(\"taxicab_id\", T.StringType(), True),\n",
    "    T.StructField(\"url\", T.StringType(), True),\n",
    "    T.StructField(\"resolved_url\", T.StringType(), True),\n",
    "    T.StructField(\"status_code\", T.IntegerType(), True),\n",
    "    T.StructField(\"content_type\", T.StringType(), True),\n",
    "    T.StructField(\"native_id\", T.StringType(), True),\n",
    "    T.StructField(\"native_id_namespace\", T.StringType(), True),\n",
    "    T.StructField(\"s3_path\", T.StringType(), True),\n",
    "    T.StructField(\"is_soft_block\", T.BooleanType(), True),\n",
    "    T.StructField(\"created_date\", T.TimestampType(), True),\n",
    "    T.StructField(\"processed_date\", T.TimestampType(), True),\n",
    "    T.StructField(\"error\", T.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f1a34f-f248-4bf2-b742-38b48a08e85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for existing scrape using taxicab API\n",
    "def get_existing_scrape(url_data):\n",
    "    \"\"\"\n",
    "    Check if the DOI already exists in the Taxicab API before sending to harvester.\n",
    "    Matches the URL in url_data with URLs in either HTML or PDF lists.\n",
    "    Returns the matching result if available, None otherwise.\n",
    "    \"\"\"\n",
    "    native_id = url_data.get(\"native_id\")\n",
    "    native_id_namespace = url_data.get(\"native_id_namespace\")\n",
    "    url_to_match = url_data.get(\"url\")\n",
    "    \n",
    "    try:\n",
    "        api_url = f\"{ENDPOINT}/{native_id_namespace}/{native_id}\"\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # check HTML list for matching URL\n",
    "            if response_data.get(\"html\") and len(response_data[\"html\"]) > 0:\n",
    "                for html_result in response_data[\"html\"]:\n",
    "                    if html_result.get(\"url\") == url_to_match:\n",
    "                        print(f\"Found existing HTML for {url_to_match} with native_id {native_id} and native_id_namespace {native_id_namespace}\")\n",
    "                        return {\n",
    "                            \"taxicab_id\": html_result[\"id\"],\n",
    "                            \"url\": url_data[\"url\"],\n",
    "                            \"resolved_url\": html_result.get(\"resolved_url\"),\n",
    "                            \"status_code\": 200,\n",
    "                            \"content_type\": \"text/html\",\n",
    "                            \"created_date\": url_data[\"created_date\"],\n",
    "                            \"native_id\": html_result[\"native_id\"],\n",
    "                            \"native_id_namespace\": html_result[\"native_id_namespace\"],\n",
    "                            \"s3_path\": html_result.get(\"s3_path\"),\n",
    "                            \"is_soft_block\": False,\n",
    "                            \"error\": None\n",
    "                        }\n",
    "            \n",
    "            # no match in HTML, check PDF list\n",
    "            if response_data.get(\"pdf\") and len(response_data[\"pdf\"]) > 0:\n",
    "                for pdf_result in response_data[\"pdf\"]:\n",
    "                    print(f\"Found existing PDF for {url_to_match} with native_id {native_id} and native_id_namespace {native_id_namespace}\")\n",
    "                    if pdf_result.get(\"url\") == url_to_match:\n",
    "                        return {\n",
    "                            \"taxicab_id\": pdf_result[\"id\"],\n",
    "                            \"url\": url_data[\"url\"],\n",
    "                            \"resolved_url\": pdf_result.get(\"resolved_url\"),\n",
    "                            \"status_code\": 200,\n",
    "                            \"content_type\": \"application/pdf\",\n",
    "                            \"created_date\": url_data[\"created_date\"],\n",
    "                            \"native_id\": pdf_result[\"native_id\"],\n",
    "                            \"native_id_namespace\": pdf_result[\"native_id_namespace\"],\n",
    "                            \"s3_path\": pdf_result.get(\"s3_path\"),\n",
    "                            \"is_soft_block\": False,\n",
    "                            \"error\": None\n",
    "                        }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {native_id_namespace} in API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbc20f0-156f-4a0f-9272-97fc2bb5d70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# process single url\n",
    "\n",
    "def process_url(url_data):\n",
    "    \"\"\"\n",
    "    Main function to process urls.\n",
    "    \"\"\"\n",
    "    # check if it was already scraped in unpaywall, or in this process earlier\n",
    "    existing_scrape = get_existing_scrape(url_data)\n",
    "    if existing_scrape:\n",
    "        print(f\"Found existing HTML or PDF for {url_data.get('native_id')} url {url_data.get('url')}, skipping harvest\")\n",
    "        return existing_scrape\n",
    "    \n",
    "    # clean native_id in case it contains doi.org\n",
    "    native_id = url_data.get(\"native_id\", \"\")\n",
    "    if native_id and \"https://doi.org/\" in native_id:\n",
    "        native_id = native_id.replace(\"https://doi.org/\", \"\")\n",
    "    \n",
    "    # proceed with original harvester process \n",
    "    try:\n",
    "        payload = {\n",
    "            \"url\": url_data.get(\"url\"),\n",
    "            \"native_id\": native_id,\n",
    "            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\")\n",
    "        }\n",
    "        \n",
    "        response = requests.post(ENDPOINT, json=payload)\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        print(response_data.get(\"id\"))\n",
    "        print(response_data.get(\"native_id\"))\n",
    "        print(response_data.get(\"native_id_namespace\"))\n",
    "        \n",
    "        return {\n",
    "            \"taxicab_id\": response_data.get(\"id\"),\n",
    "            \"url\": url_data.get(\"url\"),\n",
    "            \"harvester_id\": response_data.get(\"id\"),\n",
    "            \"status_code\": response_data.get(\"status_code\"),\n",
    "            \"resolved_url\": response_data.get(\"resolved_url\"),\n",
    "            \"content_type\": response_data.get(\"content_type\"),\n",
    "            \"created_date\": url_data[\"created_date\"],\n",
    "            \"native_id\": response_data.get(\"native_id\"),\n",
    "            \"native_id_namespace\": response_data.get(\"native_id_namespace\"),\n",
    "            \"s3_path\": response_data.get(\"s3_path\"),\n",
    "            \"is_soft_block\": response_data.get(\"is_soft_block\", False),\n",
    "            \"error\": None\n",
    "        }\n",
    "    \n",
    "    # something went wrong\n",
    "    except requests.RequestException as e:\n",
    "        return {\n",
    "            \"taxicab_id\": None,\n",
    "            \"url\": url_data.get(\"url\"),\n",
    "            \"status_code\": getattr(e.response, 'status_code', 0),\n",
    "            \"resolved_url\": None,\n",
    "            \"content_type\": None,\n",
    "            \"created_date\": url_data[\"created_date\"],\n",
    "            \"native_id\": native_id,\n",
    "            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n",
    "            \"s3_path\": None,\n",
    "            \"is_soft_block\": False,\n",
    "            \"error\": str(e),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689a3e63-4a4b-4604-bc8d-823ac7561ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run all urls in a threadpool\n",
    "\n",
    "def process_urls_with_threadpool(url_list, max_workers):\n",
    "    \"\"\"\n",
    "    Process URLs using a ThreadPoolExecutor to parallelize requests.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # configure session with retry mechanism for better reliability\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"POST\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        pool_connections=120,\n",
    "        pool_maxsize=120,\n",
    "        max_retries=retries\n",
    "    )\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    # helper function to clean DOI prefixes\n",
    "    def clean_doi(native_id):\n",
    "        if native_id and isinstance(native_id, str):\n",
    "            if \"https://doi.org/\" in native_id:\n",
    "                return native_id.replace(\"https://doi.org/\", \"\")\n",
    "        return native_id\n",
    "    \n",
    "    # process function that includes the session\n",
    "    def process_url_with_session(url_data):\n",
    "        if \"native_id\" in url_data and url_data[\"native_id\"]:\n",
    "            url_data = url_data.copy()\n",
    "            url_data[\"native_id\"] = clean_doi(url_data[\"native_id\"])\n",
    "\n",
    "        try:\n",
    "            result = process_url(url_data)\n",
    "            print(f\"Processed {url_data.get('url')} - Status: {result.get('status_code')}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url_data.get('url')}: {str(e)}\")\n",
    "            return {\n",
    "                \"taxicab_id\": None,\n",
    "                \"url\": url_data.get('url'),\n",
    "                \"status_code\": 0,\n",
    "                \"resolved_url\": None,\n",
    "                \"content_type\": None,\n",
    "                \"created_date\": url_data[\"created_date\"],\n",
    "                \"native_id\": url_data.get(\"native_id\"),\n",
    "                \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n",
    "                \"s3_path\": None,\n",
    "                \"is_soft_block\": False,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "    \n",
    "    print(f\"Starting ThreadPool with {max_workers} workers to process {len(url_list)} URLs\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process URLs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # submit all tasks and map them to their original URLs\n",
    "        future_to_url = {executor.submit(process_url_with_session, url_data): url_data for url_data in url_list}\n",
    "\n",
    "        count = 0\n",
    "        total = len(future_to_url)\n",
    "        \n",
    "        # process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            count += 1\n",
    "            print(f\"Processed {count}/{total}\")\n",
    "            \n",
    "            url_data = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"URL {url_data.get('url')} generated an exception: {exc}\")\n",
    "                original_native_id = url_data.get(\"native_id\")\n",
    "                cleaned_native_id = clean_doi(original_native_id)\n",
    "                results.append({\n",
    "                    \"taxicab_id\": None,\n",
    "                    \"url\": url_data.get('url'),\n",
    "                    \"status_code\": 0,\n",
    "                    \"resolved_url\": None,\n",
    "                    \"content_type\": None,\n",
    "                    \"created_date\": url_data[\"created_date\"],\n",
    "                    \"native_id\": cleaned_native_id,\n",
    "                    \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n",
    "                    \"s3_path\": None,\n",
    "                    \"is_soft_block\": False,\n",
    "                    \"error\": str(exc),\n",
    "                })\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"ThreadPool processing completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Processed {len(results)} URLs\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13d7c17-2ed2-40c4-a1e1-46b577bd300f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run it all\n",
    "results = process_urls_with_threadpool(jsonUrls, max_workers=120)\n",
    "\n",
    "processed_date = datetime.datetime.now(timezone.utc)\n",
    "\n",
    "for result in results:\n",
    "    result[\"processed_date\"] = processed_date\n",
    "    \n",
    "    for field_name in [\"created_date\", \"updated_date\"]:\n",
    "        if field_name in result:\n",
    "            result[field_name] = convert_to_datetime(result[field_name])\n",
    "\n",
    "# create DataFrame directly from results and save to table\n",
    "results_df = spark.createDataFrame(results, schema=results_schema)\n",
    "results_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"openalex.taxicab.taxicab_results\")\n",
    "\n",
    "print(f\"Updated {results_df.count()} records in the results table\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Crossref DOI Link Progress ---\n",
    "eligible_links = (\n",
    "    spark.read.table(\"openalex.crossref.crossref_works\")\n",
    "    .filter(F.col(last_processed_field) >= F.lit(post_2023))\n",
    "    .select(\"native_id\", F.explode(\"urls\").alias(\"u\"))\n",
    "    .filter(F.col(\"u.url\").contains(\"doi.org\"))\n",
    "    .withColumn(\"norm_url\", F.regexp_replace(\"u.url\", r\"^https?://(dx\\.)?\", \"\"))\n",
    "    .select(F.lower(F.col(\"native_id\")).alias(\"id\"), \"norm_url\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "total_to_do = eligible_links.count()\n",
    "done_count = taxicab_finished_tasks.join(eligible_links,\n",
    "    (taxicab_finished_tasks.finished_id == eligible_links.id) &\n",
    "    (taxicab_finished_tasks.finished_url == eligible_links.norm_url)).count()\n",
    "\n",
    "print(f\"--- PROGRESS REPORT ---\")\n",
    "print(f\"Total Unique DOI Links to Scrape: {total_to_do:,}\")\n",
    "print(f\"Total Links Completed:           {done_count:,}\")\n",
    "print(f\"Remaining DOI Links:             {total_to_do - done_count:,}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8281294937844217,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxicab backfill 2023-2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
