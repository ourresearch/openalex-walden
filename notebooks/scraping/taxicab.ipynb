{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d65718f-ee9f-4679-883b-d1f31b169f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This pipeline scrapes landing pages and PDFs. The files are stored in Cloudflare R2 and the metadata is saved in a table\n",
    "\n",
    "**input**: recent crossref records, repo records, and PDF urls from landing page records\n",
    "\n",
    "**process**: taxicab API on ECS\n",
    "\n",
    "**output**: file id, url, related ids saved to `openalex.taxicab.taxicab_results`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fcc4d1-c95f-46af-b7a0-50064dcca58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from concurrent.futures import ThreadPoolExecutor\nimport concurrent.futures\nimport datetime\nimport time\nfrom urllib3.util.retry import Retry\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom datetime import timezone"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12d4d8e-9cc9-4d52-9e13-2853d154231f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = \"http://harvester-load-balancer-366186003.us-east-1.elb.amazonaws.com/taxicab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b08aeb-858e-449b-b7e7-14ddbb879c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS openalex.taxicab.taxicab_results (\n",
    "  taxicab_id STRING,\n",
    "  url STRING,\n",
    "  resolved_url STRING,\n",
    "  status_code INT,\n",
    "  content_type STRING,\n",
    "  native_id STRING,\n",
    "  native_id_namespace STRING,\n",
    "  s3_path STRING,\n",
    "  is_soft_block BOOLEAN,\n",
    "  created_date TIMESTAMP,\n",
    "  processed_date TIMESTAMP,\n",
    "  error STRING\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90bc997-92f8-43df-be1d-a03c30b0455f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "dbutils.widgets.text(\"lookback_days\", \"3\", \"Lookback window (days)\")\nlookback_days = int(dbutils.widgets.get(\"lookback_days\"))\n\nlast_processed_field = \"created_date\"\nlast_processed_date = datetime.datetime.now(timezone.utc) - datetime.timedelta(days=lookback_days)\nprint(f\"Using last processed date ({lookback_days} days ago): {last_processed_date}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4172f8a-cff0-4d9d-a67a-9bdbfcdc17f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# urls to scrape\n\ndbutils.widgets.text(\"url_limit\", \"250000\", \"Max URLs to process\")\nurl_limit = int(dbutils.widgets.get(\"url_limit\"))\n\ndef normalize_created_date(df):\n    return df.withColumn(\n        \"created_date\",\n        F.to_timestamp(\"created_date\")\n    )\n\n# Source 1: Crossref works\nrecent_crossref_works = normalize_created_date(\n    spark.read\n    .table(\"openalex.crossref.crossref_works\")\n    .filter(F.col(last_processed_field) >= F.lit(last_processed_date))\n    .select(\n        \"native_id\",\n        \"native_id_namespace\",\n        F.expr(\"get(filter(urls, x -> x.url like '%doi.org%'), 0).url\").alias(\"url\"),\n        \"created_date\"\n    )\n)\n\n# Source 2: Repo works\nrecent_repo_works = normalize_created_date(\n    spark.read.table(\"openalex.repo.repo_works\")\n    .filter(F.col(last_processed_field) >= F.lit(last_processed_date))\n    .select(\n        \"native_id\",\n        \"native_id_namespace\",\n        F.slice(\"urls\", 1, 3).alias(\"urls\"),\n        \"created_date\"\n    )\n    .filter(F.col(\"urls\").isNotNull())\n    .select(\"*\", F.explode(\"urls\").alias(\"url_struct\"))\n    .select(\n        \"native_id\",\n        \"native_id_namespace\",\n        \"created_date\",\n        F.col(\"url_struct.url\").alias(\"url\")\n    )\n    .filter(~F.col(\"url\").contains(\"doi.org\"))\n)\n\n# Source 3: Landing page PDF URLs\nrecent_pdf_works = normalize_created_date(\n    spark.read\n    .table(\"openalex.landing_page.landing_page_works\")\n    .filter(F.col(last_processed_field) >= F.lit(last_processed_date))\n    .select(\n        \"ids\",\n        \"native_id\",\n        \"native_id_namespace\",\n        F.expr(\"get(filter(urls, x -> x.content_type = 'pdf'), 0).url\").alias(\"url\"),\n        \"created_date\"\n    )\n    .withColumn(\"pmh_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'pmh'), 0).id\"))\n    .withColumn(\"doi_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'doi'), 0).id\"))\n    # Set priority: PMH first, then DOI, then original\n    .withColumn(\"final_native_id\", \n        F.when(F.col(\"pmh_id\").isNotNull(), F.col(\"pmh_id\"))\n        .when(F.col(\"doi_id\").isNotNull(), F.col(\"doi_id\"))\n        .otherwise(F.col(\"native_id\")))\n    .withColumn(\"final_namespace\", \n        F.when(F.col(\"pmh_id\").isNotNull(), F.lit(\"pmh\"))\n        .when(F.col(\"doi_id\").isNotNull(), F.lit(\"doi\"))\n        .otherwise(F.col(\"native_id_namespace\")))\n    # select final columns\n    .select(\n        F.col(\"final_native_id\").alias(\"native_id\"),\n        F.col(\"final_namespace\").alias(\"native_id_namespace\"),\n        \"url\",\n        \"created_date\"\n    )\n    .filter(F.col(\"url\").isNotNull())\n)\n\n# Union all sources, then dedup once against existing results\nall_urls = (\n    recent_crossref_works\n    .unionByName(recent_repo_works)\n    .unionByName(recent_pdf_works)\n)\n\ntaxicab_results = spark.table(\"openalex.taxicab.taxicab_results\").select(\"url\")\n\nall_urls = (\n    all_urls\n    .join(taxicab_results, [\"url\"], \"left_anti\")\n    .orderBy(F.col(\"created_date\").desc())\n    .limit(url_limit)\n)\n\n# Null out extreme dates that would break Arrow/pandas conversion (valid range ~1677-2262)\nall_urls = all_urls.withColumn(\n    \"created_date\",\n    F.when(\n        (F.year(F.col(\"created_date\")) < 1900) | \n        (F.year(F.col(\"created_date\")) > 2100),\n        F.lit(None).cast(\"timestamp\")\n    ).otherwise(F.col(\"created_date\"))\n)\n\nall_urls_pd = all_urls.toPandas()\n\njsonUrls = [\n    {\n        \"url\": row[\"url\"],\n        \"created_date\": row[\"created_date\"],\n        \"native_id\": row.get(\"native_id\", \"\"),\n        \"native_id_namespace\": row.get(\"native_id_namespace\", \"\")\n    }\n    for row in all_urls_pd.to_dict('records')\n    if row[\"url\"] is not None\n]\n\ntotal_urls = len(jsonUrls)\npdf_urls = sum(1 for url in jsonUrls if '.pdf' in url['url'].lower())\ndoi_urls = sum(1 for url in jsonUrls if 'doi.org' in url['url'].lower())\nother_urls = total_urls - pdf_urls - doi_urls\n\nprint(f\"Harvesting {total_urls} URLs ({pdf_urls} PDFs, {doi_urls} DOIs, {other_urls} other URLs)\")"
  },
  {
   "cell_type": "code",
   "source": "# result schema\n\nresults_schema = T.StructType([\n    T.StructField(\"taxicab_id\", T.StringType(), True),\n    T.StructField(\"url\", T.StringType(), True),\n    T.StructField(\"resolved_url\", T.StringType(), True),\n    T.StructField(\"status_code\", T.IntegerType(), True),\n    T.StructField(\"content_type\", T.StringType(), True),\n    T.StructField(\"native_id\", T.StringType(), True),\n    T.StructField(\"native_id_namespace\", T.StringType(), True),\n    T.StructField(\"s3_path\", T.StringType(), True),\n    T.StructField(\"is_soft_block\", T.BooleanType(), True),\n    T.StructField(\"created_date\", T.TimestampType(), True),\n    T.StructField(\"processed_date\", T.TimestampType(), True),\n    T.StructField(\"error\", T.StringType(), True)\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbc20f0-156f-4a0f-9272-97fc2bb5d70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# process single url\n\ndef process_url(url_data, session):\n    \"\"\"\n    Submit a URL to the Taxicab API for scraping.\n    Uses the provided requests.Session for connection pooling.\n    \"\"\"\n    # clean native_id in case it contains doi.org\n    native_id = url_data.get(\"native_id\", \"\")\n    if native_id and \"https://doi.org/\" in native_id:\n        native_id = native_id.replace(\"https://doi.org/\", \"\")\n    \n    try:\n        payload = {\n            \"url\": url_data.get(\"url\"),\n            \"native_id\": native_id,\n            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\")\n        }\n        \n        response = session.post(ENDPOINT, json=payload)\n        response.raise_for_status()\n        response_data = response.json()\n        print(response_data.get(\"id\"))\n        print(response_data.get(\"native_id\"))\n        print(response_data.get(\"native_id_namespace\"))\n        \n        return {\n            \"taxicab_id\": response_data.get(\"id\"),\n            \"url\": url_data.get(\"url\"),\n            \"status_code\": response_data.get(\"status_code\"),\n            \"resolved_url\": response_data.get(\"resolved_url\"),\n            \"content_type\": response_data.get(\"content_type\"),\n            \"created_date\": url_data[\"created_date\"],\n            \"native_id\": response_data.get(\"native_id\"),\n            \"native_id_namespace\": response_data.get(\"native_id_namespace\"),\n            \"s3_path\": response_data.get(\"s3_path\"),\n            \"is_soft_block\": response_data.get(\"is_soft_block\", False),\n            \"error\": None\n        }\n    \n    # something went wrong\n    except requests.RequestException as e:\n        return {\n            \"taxicab_id\": None,\n            \"url\": url_data.get(\"url\"),\n            \"status_code\": getattr(e.response, 'status_code', 0),\n            \"resolved_url\": None,\n            \"content_type\": None,\n            \"created_date\": url_data[\"created_date\"],\n            \"native_id\": native_id,\n            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n            \"s3_path\": None,\n            \"is_soft_block\": False,\n            \"error\": str(e),\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689a3e63-4a4b-4604-bc8d-823ac7561ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# run all urls in a threadpool\n\ndef process_urls_with_threadpool(url_list, max_workers):\n    \"\"\"\n    Process URLs using a ThreadPoolExecutor to parallelize requests.\n    \"\"\"\n    results = []\n    \n    # configure session with retry mechanism for better reliability\n    session = requests.Session()\n    retries = Retry(\n        total=3,\n        backoff_factor=0.5,\n        status_forcelist=[500, 502, 503, 504],\n        allowed_methods=[\"GET\", \"POST\"]\n    )\n    adapter = HTTPAdapter(\n        pool_connections=max_workers,\n        pool_maxsize=max_workers,\n        max_retries=retries\n    )\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n\n    # helper function to clean DOI prefixes\n    def clean_doi(native_id):\n        if native_id and isinstance(native_id, str):\n            if \"https://doi.org/\" in native_id:\n                return native_id.replace(\"https://doi.org/\", \"\")\n        return native_id\n    \n    def submit_url(url_data):\n        if \"native_id\" in url_data and url_data[\"native_id\"]:\n            url_data = url_data.copy()\n            url_data[\"native_id\"] = clean_doi(url_data[\"native_id\"])\n\n        try:\n            result = process_url(url_data, session)\n            print(f\"Processed {url_data.get('url')} - Status: {result.get('status_code')}\")\n            return result\n        except Exception as e:\n            print(f\"Error processing {url_data.get('url')}: {str(e)}\")\n            return {\n                \"taxicab_id\": None,\n                \"url\": url_data.get('url'),\n                \"status_code\": 0,\n                \"resolved_url\": None,\n                \"content_type\": None,\n                \"created_date\": url_data[\"created_date\"],\n                \"native_id\": url_data.get(\"native_id\"),\n                \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n                \"s3_path\": None,\n                \"is_soft_block\": False,\n                \"error\": str(e),\n            }\n    \n    print(f\"Starting ThreadPool with {max_workers} workers to process {len(url_list)} URLs\")\n    start_time = time.time()\n    \n    # Use ThreadPoolExecutor to process URLs in parallel\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # submit all tasks and map them to their original URLs\n        future_to_url = {executor.submit(submit_url, url_data): url_data for url_data in url_list}\n\n        count = 0\n        total = len(future_to_url)\n        \n        # process results as they complete\n        for future in concurrent.futures.as_completed(future_to_url):\n            count += 1\n            print(f\"Processed {count}/{total}\")\n            \n            url_data = future_to_url[future]\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as exc:\n                print(f\"URL {url_data.get('url')} generated an exception: {exc}\")\n                original_native_id = url_data.get(\"native_id\")\n                cleaned_native_id = clean_doi(original_native_id)\n                results.append({\n                    \"taxicab_id\": None,\n                    \"url\": url_data.get('url'),\n                    \"status_code\": 0,\n                    \"resolved_url\": None,\n                    \"content_type\": None,\n                    \"created_date\": url_data[\"created_date\"],\n                    \"native_id\": cleaned_native_id,\n                    \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n                    \"s3_path\": None,\n                    \"is_soft_block\": False,\n                    \"error\": str(exc),\n                })\n    \n    elapsed_time = time.time() - start_time\n    print(f\"ThreadPool processing completed in {elapsed_time:.2f} seconds\")\n    print(f\"Processed {len(results)} URLs\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13d7c17-2ed2-40c4-a1e1-46b577bd300f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# run it all\nresults = process_urls_with_threadpool(jsonUrls, max_workers=120)\n\nprocessed_date = datetime.datetime.now(timezone.utc)\n\nfor result in results:\n    result[\"processed_date\"] = processed_date\n\n# create DataFrame directly from results and save to table\nresults_df = spark.createDataFrame(results, schema=results_schema)\nresults_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"openalex.taxicab.taxicab_results\")\n\nprint(f\"Updated {results_df.count()} records in the results table\")"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8281294937844217,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxicab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}