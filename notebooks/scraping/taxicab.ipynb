{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d65718f-ee9f-4679-883b-d1f31b169f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This pipeline scrapes landing pages and PDFs. The files are stored in Cloudflare R2 and the metadata is saved in a table\n",
    "\n",
    "**input**: recent crossref records, repo records, and PDF urls from landing page records\n",
    "\n",
    "**process**: taxicab API on ECS\n",
    "\n",
    "**output**: file id, url, related ids saved to `openalex.taxicab.taxicab_results`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fcc4d1-c95f-46af-b7a0-50064dcca58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from concurrent.futures import ThreadPoolExecutor\nimport concurrent.futures\nimport datetime\nimport time\nfrom urllib3.util.retry import Retry\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom datetime import timezone"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12d4d8e-9cc9-4d52-9e13-2853d154231f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = \"http://harvester-load-balancer-366186003.us-east-1.elb.amazonaws.com/taxicab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b08aeb-858e-449b-b7e7-14ddbb879c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "%sql\nCREATE TABLE IF NOT EXISTS openalex.taxicab.taxicab_results (\n  taxicab_id STRING,\n  url STRING,\n  resolved_url STRING,\n  status_code INT,\n  content_type STRING,\n  native_id STRING,\n  native_id_namespace STRING,\n  s3_path STRING,\n  is_soft_block BOOLEAN,\n  created_date TIMESTAMP,\n  processed_date TIMESTAMP,\n  error STRING\n)\nUSING DELTA;"
  },
  {
   "cell_type": "code",
   "source": "%sql\nCREATE TABLE IF NOT EXISTS openalex.taxicab.rescrape_queue (\n  native_id STRING,\n  native_id_namespace STRING,\n  created_date TIMESTAMP DEFAULT current_timestamp()\n)\nUSING DELTA;",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# result schema\n\nresults_schema = T.StructType([\n    T.StructField(\"taxicab_id\", T.StringType(), True),\n    T.StructField(\"url\", T.StringType(), True),\n    T.StructField(\"resolved_url\", T.StringType(), True),\n    T.StructField(\"status_code\", T.IntegerType(), True),\n    T.StructField(\"content_type\", T.StringType(), True),\n    T.StructField(\"native_id\", T.StringType(), True),\n    T.StructField(\"native_id_namespace\", T.StringType(), True),\n    T.StructField(\"s3_path\", T.StringType(), True),\n    T.StructField(\"is_soft_block\", T.BooleanType(), True),\n    T.StructField(\"created_date\", T.TimestampType(), True),\n    T.StructField(\"processed_date\", T.TimestampType(), True),\n    T.StructField(\"error\", T.StringType(), True)\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90bc997-92f8-43df-be1d-a03c30b0455f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "dbutils.widgets.text(\"lookback_days\", \"3\", \"Lookback window (days)\")\ndbutils.widgets.text(\"rescrape_queue_only\", \"false\", \"Rescrape queue only (true/false)\")\n\nrescrape_queue_only = dbutils.widgets.get(\"rescrape_queue_only\").strip().lower() == \"true\"\nlookback_days = int(dbutils.widgets.get(\"lookback_days\"))\n\nif rescrape_queue_only:\n    print(\"RESCRAPE MODE: processing queue table\")\nelse:\n    last_processed_date = datetime.datetime.now(timezone.utc) - datetime.timedelta(days=lookback_days)\n    print(f\"Looking back {lookback_days} days from: {last_processed_date}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4172f8a-cff0-4d9d-a67a-9bdbfcdc17f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# urls to scrape\n\ndbutils.widgets.text(\"url_limit\", \"250000\", \"Max URLs to process\")\nurl_limit = int(dbutils.widgets.get(\"url_limit\"))\n\nif rescrape_queue_only:\n    queue_df = spark.read.table(\"openalex.taxicab.rescrape_queue\")\n    queue_count = queue_df.count()\n    if queue_count == 0:\n        dbutils.notebook.exit(\"Queue empty â€” nothing to rescrape\")\n\n    print(f\"Rescrape queue has {queue_count} records\")\n\n    # DOIs: construct URL directly\n    doi_urls = (\n        queue_df.filter(F.col(\"native_id_namespace\") == \"doi\")\n        .withColumn(\"url\", F.concat(F.lit(\"https://doi.org/\"), F.col(\"native_id\")))\n        .select(\"native_id\", \"native_id_namespace\", \"url\")\n    )\n\n    # Non-DOIs: look up most recent URL from taxicab_results\n    non_doi_urls = (\n        queue_df.filter(F.col(\"native_id_namespace\") != \"doi\")\n        .join(\n            spark.read.table(\"openalex.taxicab.taxicab_results\")\n                .select(\"native_id\", \"native_id_namespace\", \"url\")\n                .dropDuplicates([\"native_id\", \"native_id_namespace\", \"url\"]),\n            [\"native_id\", \"native_id_namespace\"], \"inner\"\n        )\n        .select(\"native_id\", \"native_id_namespace\", \"url\")\n    )\n\n    all_urls = doi_urls.unionByName(non_doi_urls).limit(url_limit)\n\n    all_urls_pd = all_urls.toPandas()\n\n    jsonUrls = [\n        {\n            \"url\": row[\"url\"],\n            \"native_id\": row.get(\"native_id\", \"\"),\n            \"native_id_namespace\": row.get(\"native_id_namespace\", \"\")\n        }\n        for row in all_urls_pd.to_dict('records')\n        if row[\"url\"] is not None\n    ]\n\nelse:\n    # Source 1: Crossref works\n    recent_crossref_works = (\n        spark.read\n        .table(\"openalex.crossref.crossref_works\")\n        .filter(F.col(\"created_date\") >= F.lit(last_processed_date))\n        .select(\n            \"native_id\",\n            \"native_id_namespace\",\n            F.expr(\"get(filter(urls, x -> x.url like '%doi.org%'), 0).url\").alias(\"url\"),\n            F.to_timestamp(\"created_date\").alias(\"source_created_date\"),\n        )\n    )\n\n    # Source 2: Repo works\n    recent_repo_works = (\n        spark.read.table(\"openalex.repo.repo_works\")\n        .filter(F.col(\"created_date\") >= F.lit(last_processed_date))\n        .select(\n            \"native_id\",\n            \"native_id_namespace\",\n            F.slice(\"urls\", 1, 3).alias(\"urls\"),\n            F.to_timestamp(\"created_date\").alias(\"source_created_date\"),\n        )\n        .filter(F.col(\"urls\").isNotNull())\n        .select(\"*\", F.explode(\"urls\").alias(\"url_struct\"))\n        .select(\n            \"native_id\",\n            \"native_id_namespace\",\n            \"source_created_date\",\n            F.col(\"url_struct.url\").alias(\"url\")\n        )\n        .filter(~F.col(\"url\").contains(\"doi.org\"))\n    )\n\n    # Source 3: Landing page PDF URLs\n    recent_pdf_works = (\n        spark.read\n        .table(\"openalex.landing_page.landing_page_works\")\n        .filter(F.col(\"created_date\") >= F.lit(last_processed_date))\n        .select(\n            \"ids\",\n            \"native_id\",\n            \"native_id_namespace\",\n            F.expr(\"get(filter(urls, x -> x.content_type = 'pdf'), 0).url\").alias(\"url\"),\n            F.to_timestamp(\"created_date\").alias(\"source_created_date\"),\n        )\n        .withColumn(\"pmh_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'pmh'), 0).id\"))\n        .withColumn(\"doi_id\", F.expr(\"get(filter(ids, x -> x.namespace = 'doi'), 0).id\"))\n        # Set priority: PMH first, then DOI, then original\n        .withColumn(\"final_native_id\", \n            F.when(F.col(\"pmh_id\").isNotNull(), F.col(\"pmh_id\"))\n            .when(F.col(\"doi_id\").isNotNull(), F.col(\"doi_id\"))\n            .otherwise(F.col(\"native_id\")))\n        .withColumn(\"final_namespace\", \n            F.when(F.col(\"pmh_id\").isNotNull(), F.lit(\"pmh\"))\n            .when(F.col(\"doi_id\").isNotNull(), F.lit(\"doi\"))\n            .otherwise(F.col(\"native_id_namespace\")))\n        .select(\n            F.col(\"final_native_id\").alias(\"native_id\"),\n            F.col(\"final_namespace\").alias(\"native_id_namespace\"),\n            \"url\",\n            \"source_created_date\",\n        )\n        .filter(F.col(\"url\").isNotNull())\n    )\n\n    # Union all sources, clean native_id, dedup, order newest first, then drop the ordering column\n    taxicab_results = spark.table(\"openalex.taxicab.taxicab_results\").select(\"url\")\n\n    all_urls = (\n        recent_crossref_works\n        .unionByName(recent_repo_works)\n        .unionByName(recent_pdf_works)\n        .withColumn(\"native_id\", F.regexp_replace(\"native_id\", \"^https://doi\\\\.org/\", \"\"))\n        .join(taxicab_results, [\"url\"], \"left_anti\")\n        .orderBy(F.col(\"source_created_date\").desc())\n        .limit(url_limit)\n        .drop(\"source_created_date\")\n    )\n\n    all_urls_pd = all_urls.toPandas()\n\n    jsonUrls = [\n        {\n            \"url\": row[\"url\"],\n            \"native_id\": row.get(\"native_id\", \"\"),\n            \"native_id_namespace\": row.get(\"native_id_namespace\", \"\")\n        }\n        for row in all_urls_pd.to_dict('records')\n        if row[\"url\"] is not None\n    ]\n\ntotal_urls = len(jsonUrls)\npdf_urls = sum(1 for url in jsonUrls if '.pdf' in url['url'].lower())\ndoi_urls_count = sum(1 for url in jsonUrls if 'doi.org' in url['url'].lower())\nother_urls = total_urls - pdf_urls - doi_urls_count\n\nprint(f\"Harvesting {total_urls} URLs ({pdf_urls} PDFs, {doi_urls_count} DOIs, {other_urls} other URLs)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbc20f0-156f-4a0f-9272-97fc2bb5d70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# process single url\n\ndef process_url(url_data, session):\n    \"\"\"\n    Submit a URL to the Taxicab API for scraping.\n    Uses the provided requests.Session for connection pooling.\n    \"\"\"\n    try:\n        payload = {\n            \"url\": url_data.get(\"url\"),\n            \"native_id\": url_data.get(\"native_id\", \"\"),\n            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\")\n        }\n        \n        response = session.post(ENDPOINT, json=payload)\n        response.raise_for_status()\n        response_data = response.json()\n        print(f\"OK {url_data.get('url')} -> {response_data.get('id')}\")\n        \n        return {\n            \"taxicab_id\": response_data.get(\"id\"),\n            \"url\": url_data.get(\"url\"),\n            \"status_code\": response_data.get(\"status_code\"),\n            \"resolved_url\": response_data.get(\"resolved_url\"),\n            \"content_type\": response_data.get(\"content_type\"),\n            \"native_id\": response_data.get(\"native_id\"),\n            \"native_id_namespace\": response_data.get(\"native_id_namespace\"),\n            \"s3_path\": response_data.get(\"s3_path\"),\n            \"is_soft_block\": response_data.get(\"is_soft_block\", False),\n            \"error\": None\n        }\n    \n    except Exception as e:\n        print(f\"ERR {url_data.get('url')} -> {e}\")\n        return {\n            \"taxicab_id\": None,\n            \"url\": url_data.get(\"url\"),\n            \"status_code\": getattr(getattr(e, 'response', None), 'status_code', 0),\n            \"resolved_url\": None,\n            \"content_type\": None,\n            \"native_id\": url_data.get(\"native_id\", \"\"),\n            \"native_id_namespace\": url_data.get(\"native_id_namespace\", \"\"),\n            \"s3_path\": None,\n            \"is_soft_block\": False,\n            \"error\": str(e),\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689a3e63-4a4b-4604-bc8d-823ac7561ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# run all urls in a threadpool\n\ndef process_urls_with_threadpool(url_list, max_workers):\n    \"\"\"\n    Process URLs using a ThreadPoolExecutor to parallelize requests.\n    \"\"\"\n    # configure session with connection pooling and retry\n    session = requests.Session()\n    retries = Retry(\n        total=3,\n        backoff_factor=0.5,\n        status_forcelist=[500, 502, 503, 504],\n        allowed_methods=[\"GET\", \"POST\"]\n    )\n    adapter = HTTPAdapter(\n        pool_connections=max_workers,\n        pool_maxsize=max_workers,\n        max_retries=retries\n    )\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    \n    print(f\"Starting ThreadPool with {max_workers} workers to process {len(url_list)} URLs\")\n    start_time = time.time()\n    results = []\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_url = {\n            executor.submit(process_url, url_data, session): url_data \n            for url_data in url_list\n        }\n\n        for count, future in enumerate(concurrent.futures.as_completed(future_to_url), 1):\n            if count % 1000 == 0:\n                print(f\"Processed {count}/{len(future_to_url)}\")\n            results.append(future.result())\n    \n    elapsed_time = time.time() - start_time\n    print(f\"ThreadPool processing completed in {elapsed_time:.2f} seconds\")\n    print(f\"Processed {len(results)} URLs\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13d7c17-2ed2-40c4-a1e1-46b577bd300f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# run it all\nresults = process_urls_with_threadpool(jsonUrls, max_workers=120)\n\nnow = datetime.datetime.now(timezone.utc)\n\nfor result in results:\n    result[\"created_date\"] = now\n    result[\"processed_date\"] = now\n\n# create DataFrame directly from results and save to table\nresults_df = spark.createDataFrame(results, schema=results_schema)\nresults_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"openalex.taxicab.taxicab_results\")\n\nprint(f\"Updated {results_df.count()} records in the results table\")\n\nif rescrape_queue_only:\n    spark.sql(\"TRUNCATE TABLE openalex.taxicab.rescrape_queue\")\n    print(\"Rescrape queue cleared\")"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8281294937844217,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxicab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}