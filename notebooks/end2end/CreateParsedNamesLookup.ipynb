{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f887f376-f606-4899-aa3c-ca8fe0b5e43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creates and maintains `openalex.authors.parsed_names_lookup` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b064c1d0-294c-43a8-bae5-fceb544bbfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nameparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3f8c0a-c148-4ef2-b4fd-92556ac3a54c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS identifier('openalex' || :env_suffix || '.authors.parsed_names_lookup') (\n",
    "  raw_author_name STRING,\n",
    "  parsed_name STRUCT<\n",
    "      title: STRING,\n",
    "      first: STRING,\n",
    "      middle: STRING,\n",
    "      last: STRING,\n",
    "      suffix: STRING,\n",
    "      nickname: STRING\n",
    "  >,\n",
    "  created_datetime TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (raw_author_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45696535-4f33-4031-ac13-934ff2679f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Name parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473f82bf-7982-4bf7-80a7-00ea91f02ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nameparser import HumanName\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "dbutils.widgets.text(\"env_suffix\", \"\", \"Environment Suffix\")\n",
    "env_suffix = dbutils.widgets.get(\"env_suffix\")\n",
    "\n",
    "CUTOFF_DATE = \"2025-12-01\"\n",
    "\n",
    "parsed_name_schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('first', StringType(), True),\n",
    "    StructField('middle', StringType(), True),\n",
    "    StructField('last', StringType(), True),\n",
    "    StructField('suffix', StringType(), True),\n",
    "    StructField('nickname', StringType(), True)\n",
    "])\n",
    "\n",
    "# -- Begin CJK support --\n",
    "\n",
    "# Common two-character Chinese surnames (复姓)\n",
    "COMPOUND_SURNAMES = {\n",
    "    '欧阳', '太史', '端木', '上官', '司马', '东方', '独孤', '南宫', '万俟', \n",
    "    '闻人', '夏侯', '诸葛', '尉迟', '公羊', '赫连', '澹台', '皇甫', '宗政',\n",
    "    '濮阳', '公冶', '太叔', '申屠', '公孙', '慕容', '仲孙', '钟离', '长孙',\n",
    "    '宇文', '司徒', '鲜于', '司空', '闾丘', '子车', '亓官', '司寇', '巫马',\n",
    "    '公西', '颛孙', '壤驷', '公良', '漆雕', '乐正', '宰父', '谷梁', '拓跋',\n",
    "    '夹谷', '轩辕', '令狐', '段干', '百里', '呼延', '东郭', '南门', '羊舌',\n",
    "    '微生', '公户', '公玉', '公仪', '梁丘', '公仲', '公上', '公门', '公山',\n",
    "    '公坚', '左丘', '公伯', '西门', '公祖', '第五', '公乘', '贯丘', '公皙',\n",
    "    '南荣', '东里', '东宫', '仲长', '子书', '子桑', '即墨', '达奚', '褚师',\n",
    "    # Traditional variants\n",
    "    '歐陽', '司馬', '東方', '獨孤', '南宮', '諸葛', '尉遲', '赫連', '澹臺',\n",
    "    '皇甫', '濮陽', '慕容', '鍾離', '長孫', '宇文', '鮮于', '閭丘', '顓孫',\n",
    "    '漆雕', '樂正', '穀梁', '拓跋', '夾谷', '軒轅', '段幹', '東郭', '南門',\n",
    "    '羊舌', '梁丘', '左丘', '西門', '東里', '東宮', '仲長'\n",
    "}\n",
    "\n",
    "def is_cjk(char):\n",
    "    \"\"\"Check if a character is CJK (Chinese/Japanese/Korean).\"\"\"\n",
    "    cp = ord(char)\n",
    "    return (\n",
    "        (0x4E00 <= cp <= 0x9FFF) or    # CJK Unified Ideographs\n",
    "        (0x3400 <= cp <= 0x4DBF) or    # CJK Unified Ideographs Extension A\n",
    "        (0x20000 <= cp <= 0x2A6DF) or  # CJK Unified Ideographs Extension B\n",
    "        (0xF900 <= cp <= 0xFAFF) or    # CJK Compatibility Ideographs\n",
    "        (0x2F800 <= cp <= 0x2FA1F)     # CJK Compatibility Ideographs Supplement\n",
    "    )\n",
    "\n",
    "def is_all_cjk(s):\n",
    "    \"\"\"Check if string is entirely CJK characters (ignoring whitespace).\"\"\"\n",
    "    chars = s.replace(' ', '')\n",
    "    return len(chars) > 0 and all(is_cjk(c) for c in chars)\n",
    "\n",
    "def split_chinese_name(name):\n",
    "    \"\"\"\n",
    "    Split a Chinese name written without spaces into (surname, given_name).\n",
    "    Assumes Eastern order (surname first).\n",
    "    \"\"\"\n",
    "    # Check for compound surname first\n",
    "    if len(name) >= 2 and name[:2] in COMPOUND_SURNAMES:\n",
    "        return name[:2], name[2:]\n",
    "    # Default: single-character surname\n",
    "    return name[0], name[1:]\n",
    "\n",
    "# -- End CJK support --\n",
    "\n",
    "\n",
    "def parse_name(name_string):\n",
    "    \"\"\"\n",
    "    Parse a name, with special handling for unsplit Chinese names.\n",
    "    Returns a HumanName object with corrected first/last for Chinese names.\n",
    "    \"\"\"\n",
    "    name_string = name_string.strip()\n",
    "    \n",
    "    # Only special case: All CJK with no spaces - assume Eastern order, split it\n",
    "    if is_all_cjk(name_string) and ' ' not in name_string:\n",
    "        surname, given = split_chinese_name(name_string)\n",
    "        result = HumanName()\n",
    "        result.last = surname\n",
    "        result.first = given\n",
    "        return result\n",
    "    \n",
    "    # Everything else: use standard nameparser (assumes Western order)\n",
    "    return HumanName(name_string)\n",
    "\n",
    "\n",
    "def clean_name_component(s: str) -> str:\n",
    "    \"\"\"Remove diacritics, lowercase, and remove periods.\"\"\"\n",
    "    if s is None or s == '':\n",
    "        return ''\n",
    "    # Remove diacritics (normalize to NFD, remove combining characters)\n",
    "    normalized = unicodedata.normalize('NFD', s)\n",
    "    without_diacritics = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')\n",
    "    # Lowercase and remove periods\n",
    "    cleaned = without_diacritics.lower().replace('.', '')\n",
    "    # Collapse multiple spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def parse_human_name(name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a name using nameparser.HumanName and return cleaned components.\n",
    "    Includes special handling for CJK names without spaces.\n",
    "    Returns dict with: title, first, middle, last, suffix, nickname\n",
    "    All values are cleaned: diacritics removed, lowercased, periods removed.\n",
    "    \"\"\"\n",
    "    if name is None or not isinstance(name, str) or name.strip() == '':\n",
    "        return {\n",
    "            'title': '',\n",
    "            'first': '',\n",
    "            'middle': '',\n",
    "            'last': '',\n",
    "            'suffix': '',\n",
    "            'nickname': ''\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        parsed = parse_name(name)  # use our CJK-aware parser\n",
    "        return {\n",
    "            'title': clean_name_component(parsed.title),\n",
    "            'first': clean_name_component(parsed.first),\n",
    "            'middle': clean_name_component(parsed.middle),\n",
    "            'last': clean_name_component(parsed.last),\n",
    "            'suffix': clean_name_component(parsed.suffix),\n",
    "            'nickname': clean_name_component(parsed.nickname)\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            'title': '',\n",
    "            'first': '',\n",
    "            'middle': '',\n",
    "            'last': '',\n",
    "            'suffix': '',\n",
    "            'nickname': ''\n",
    "        }\n",
    "\n",
    "@F.pandas_udf(parsed_name_schema)\n",
    "def parse_names_batch(names: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Vectorized UDF to parse a batch of names.\"\"\"\n",
    "    results = [parse_human_name(n) for n in names]\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a077634-de80-427f-b3d2-66496200632f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Get new names to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7546d13-b20b-44cf-a221-578a0b246bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_names_df = spark.sql(f\"\"\"\n",
    "    WITH distinct_names AS (\n",
    "        SELECT DISTINCT TRIM(author.name) AS raw_author_name\n",
    "        FROM openalex{env_suffix}.works.locations_mapped\n",
    "        LATERAL VIEW explode(authors) AS author\n",
    "        WHERE author.name IS NOT NULL \n",
    "          AND TRIM(author.name) != ''\n",
    "          AND openalex_updated_dt >= '{CUTOFF_DATE}'\n",
    "        \n",
    "        UNION\n",
    "        \n",
    "        SELECT DISTINCT TRIM(longest_name) AS raw_author_name\n",
    "        FROM openalex.authors.openalex_authors\n",
    "        WHERE longest_name IS NOT NULL \n",
    "          AND TRIM(longest_name) != ''\n",
    "    )\n",
    "    SELECT dn.raw_author_name\n",
    "    FROM distinct_names dn\n",
    "    LEFT ANTI JOIN openalex{env_suffix}.authors.parsed_names_lookup existing\n",
    "        ON dn.raw_author_name = existing.raw_author_name\n",
    "\"\"\")\n",
    "\n",
    "new_count = new_names_df.count()\n",
    "print(f\"Found {new_count:,} new distinct author names to parse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56eb1c3-2a10-4125-81a9-952c125170ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6cb711-cbd5-4886-bb5a-9fd2336bd774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if new_count > 0:\n",
    "    parsed_df = new_names_df.withColumn(\n",
    "        \"parsed_name\", parse_names_batch(F.col(\"raw_author_name\"))\n",
    "    ).withColumn(\n",
    "        \"created_datetime\", F.current_timestamp()\n",
    "    )\n",
    "    \n",
    "    parsed_df.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "        f\"openalex{env_suffix}.authors.parsed_names_lookup\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Added {new_count:,} parsed names to lookup table\")\n",
    "else:\n",
    "    print(\"No new names to parse\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5594533721578024,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateParsedNamesLookup",
   "widgets": {
    "env_suffix": {
     "currentValue": "",
     "nuid": "5db93d33-3ea0-43af-a400-3b7b0870f2a1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Environment Suffix",
      "name": "env_suffix",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Environment Suffix",
      "name": "env_suffix",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
