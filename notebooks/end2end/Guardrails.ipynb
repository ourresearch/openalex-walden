{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Guardrails\n\n## Overview\n\nThis notebook performs data quality checks after CreateWorksEnriched. If any check fails and the override is not set, the pipeline will fail and block downstream tasks.\n\n## Checks\n\n1. **Author IDs Present in Recently Created Works**: All authorships with an author object must have an author ID\n2. **Records Changed in Last Day**: No more than 4 million records should have an `updated_date` in the last day\n\n## Parameters\n\n- `guardrails_override`: Set to `\"true\"` to skip guardrail failures and allow pipeline to continue"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get widget parameters\n",
    "dbutils.widgets.text(\"guardrails_override\", \"false\")\n",
    "dbutils.widgets.text(\"env_suffix\", \"\")\n",
    "\n",
    "guardrails_override = dbutils.widgets.get(\"guardrails_override\").lower() == \"true\"\n",
    "env_suffix = dbutils.widgets.get(\"env_suffix\")\n",
    "\n",
    "print(f\"Guardrails override: {guardrails_override}\")\n",
    "print(f\"Environment suffix: '{env_suffix}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Track all check results\n",
    "check_results = []\n",
    "\n",
    "def record_check(name: str, passed: bool, details: str):\n",
    "    \"\"\"Record a guardrail check result.\"\"\"\n",
    "    status = \"PASSED\" if passed else \"FAILED\"\n",
    "    check_results.append({\"name\": name, \"passed\": passed, \"details\": details})\n",
    "    print(f\"[{status}] {name}: {details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Check 1: Author IDs Present in Recently Created Works\n\nVerify that all authorships with an author object have an author ID. This check runs on new works (id >= 7 billion) created in the recent batch (last 1 day). Legacy works that are re-surfaced with old IDs are excluded since they have a known limitation with author ID inheritance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for missing author IDs in recent batch\n# A work fails this check if it has an authorship with an author object but no author.id\n# Note: We exclude authorships with empty raw_author_name since those can't be matched\n\nmissing_author_ids_query = f\"\"\"\nWITH recent_works AS (\n    SELECT id, authorships, created_date\n    FROM openalex{env_suffix}.works.openalex_works\n    WHERE id >= 7000000000\n      AND created_date >= current_date() - INTERVAL 1 DAY\n      AND authorships IS NOT NULL\n      AND SIZE(authorships) > 0\n),\nworks_with_missing_author_ids AS (\n    SELECT \n        id,\n        SIZE(authorships) as total_authorships,\n        SIZE(FILTER(authorships, a -> \n            a.author IS NOT NULL \n            AND a.raw_author_name IS NOT NULL \n            AND TRIM(a.raw_author_name) != ''\n            AND (a.author.id IS NULL OR a.author.id = '')\n        )) as missing_author_id_count\n    FROM recent_works\n)\nSELECT \n    COUNT(*) as total_recent_works,\n    SUM(CASE WHEN missing_author_id_count > 0 THEN 1 ELSE 0 END) as works_with_missing_author_ids,\n    SUM(missing_author_id_count) as total_missing_author_ids,\n    SUM(total_authorships) as total_authorships_checked\nFROM works_with_missing_author_ids\n\"\"\"\n\nresult = spark.sql(missing_author_ids_query).collect()[0]\n\ntotal_recent_works = result[\"total_recent_works\"]\nworks_with_missing = result[\"works_with_missing_author_ids\"]\ntotal_missing = result[\"total_missing_author_ids\"]\ntotal_authorships = result[\"total_authorships_checked\"]\n\nprint(f\"Recent works checked: {total_recent_works:,}\")\nprint(f\"Total authorships checked: {total_authorships:,}\")\nprint(f\"Works with missing author IDs: {works_with_missing:,}\")\nprint(f\"Total missing author IDs: {total_missing:,}\")\n\n# Calculate percentage\nif total_authorships and total_authorships > 0:\n    missing_pct = (total_missing / total_authorships) * 100\nelse:\n    missing_pct = 0\n\n# Check passes if no missing author IDs (0 tolerance)\ncheck_passed = (works_with_missing == 0 or works_with_missing is None)\n\nrecord_check(\n    \"Author IDs Present in Recently Created Works\",\n    check_passed,\n    f\"{works_with_missing:,} works have authorships with missing author IDs ({missing_pct:.4f}% of authorships)\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show sample of works with missing author IDs for debugging (if any)\nif works_with_missing and works_with_missing > 0:\n    sample_query = f\"\"\"\n    WITH recent_works AS (\n        SELECT id, authorships, created_date, title\n        FROM openalex{env_suffix}.works.openalex_works\n        WHERE id >= 7000000000\n      AND created_date >= current_date() - INTERVAL 1 DAY\n          AND authorships IS NOT NULL\n          AND SIZE(authorships) > 0\n    )\n    SELECT \n        id,\n        title,\n        SIZE(authorships) as total_authorships,\n        SIZE(FILTER(authorships, a -> \n            a.author IS NOT NULL \n            AND a.raw_author_name IS NOT NULL \n            AND TRIM(a.raw_author_name) != ''\n            AND (a.author.id IS NULL OR a.author.id = '')\n        )) as missing_author_id_count,\n        TRANSFORM(\n            FILTER(authorships, a -> \n                a.author IS NOT NULL \n                AND a.raw_author_name IS NOT NULL \n                AND TRIM(a.raw_author_name) != ''\n                AND (a.author.id IS NULL OR a.author.id = '')\n            ),\n            a -> a.raw_author_name\n        ) as authors_missing_ids\n    FROM recent_works\n    WHERE SIZE(FILTER(authorships, a -> \n        a.author IS NOT NULL \n        AND a.raw_author_name IS NOT NULL \n        AND TRIM(a.raw_author_name) != ''\n        AND (a.author.id IS NULL OR a.author.id = '')\n    )) > 0\n    LIMIT 10\n    \"\"\"\n    print(\"\\nSample works with missing author IDs:\")\n    display(spark.sql(sample_query))"
  },
  {
   "cell_type": "markdown",
   "source": "## Check 2: Records Changed in Last Day\n\nVerify that no more than 4 million records have an `updated_date` in the last day. A spike above this threshold may indicate an unintended bulk update.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "MAX_RECORDS_CHANGED = 4_000_000\n\nrecords_changed_query = f\"\"\"\nSELECT COUNT(*) as records_changed\nFROM openalex{env_suffix}.works.openalex_works\nWHERE updated_date >= current_date() - INTERVAL 1 DAY\n\"\"\"\n\nresult = spark.sql(records_changed_query).collect()[0]\nrecords_changed = result[\"records_changed\"]\n\nprint(f\"Records changed in last day: {records_changed:,}\")\nprint(f\"Threshold: {MAX_RECORDS_CHANGED:,}\")\n\ncheck_passed = records_changed <= MAX_RECORDS_CHANGED\n\nrecord_check(\n    \"Records Changed in Last Day\",\n    check_passed,\n    f\"{records_changed:,} records changed (threshold: {MAX_RECORDS_CHANGED:,})\"\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Final Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize all check results\n",
    "print(\"=\"*60)\n",
    "print(\"GUARDRAILS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "failed_checks = [c for c in check_results if not c[\"passed\"]]\n",
    "passed_checks = [c for c in check_results if c[\"passed\"]]\n",
    "\n",
    "print(f\"\\nTotal checks: {len(check_results)}\")\n",
    "print(f\"Passed: {len(passed_checks)}\")\n",
    "print(f\"Failed: {len(failed_checks)}\")\n",
    "\n",
    "if failed_checks:\n",
    "    print(\"\\nFailed checks:\")\n",
    "    for check in failed_checks:\n",
    "        print(f\"  - {check['name']}: {check['details']}\")\n",
    "\n",
    "print(f\"\\nOverride enabled: {guardrails_override}\")\n",
    "\n",
    "# Determine final outcome\n",
    "if failed_checks and not guardrails_override:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GUARDRAILS FAILED - Pipeline will be blocked\")\n",
    "    print(\"Set guardrails_override=true to bypass (use with caution)\")\n",
    "    print(\"=\"*60)\n",
    "    raise Exception(f\"Guardrails failed: {len(failed_checks)} check(s) failed. Set guardrails_override=true to bypass.\")\n",
    "elif failed_checks and guardrails_override:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: Guardrails failed but override is enabled\")\n",
    "    print(\"Pipeline will continue - review failed checks!\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GUARDRAILS PASSED - Pipeline may proceed\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}