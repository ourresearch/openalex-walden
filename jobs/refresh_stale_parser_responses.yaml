# Refresh Stale Parser Responses
#
# Maintenance job to fix records in taxicab_enriched_new where cached parser_response
# is stale. Calls Parseland API directly and updates the cached response.
#
# Usage:
#   databricks bundle deploy -t prod
#   databricks bundle run Refresh_Stale_Parser_Responses
#
# Or run via Databricks UI after deploying.

resources:
  jobs:
    Refresh_Stale_Parser_Responses:
      name: Refresh Stale Parser Responses
      description: >
        Maintenance job to refresh stale parser_response cache in taxicab_enriched_new.
        Run manually when needed to fix records with missing affiliations.
        Supports two filter modes: date_range (BETWEEN) or before_cutoff (< cutoff_date).
      email_notifications:
        on_failure:
          - jason@ourresearch.org
      tasks:
        - task_key: refresh_parser_responses
          notebook_task:
            notebook_path: notebooks/maintenance/RefreshStaleParserResponses
            source: GIT
            base_parameters:
              filter_mode: "before_cutoff"
              cutoff_date: "2025-12-01"
              start_date: "2025-12-27"
              end_date: "2026-01-03"
              publisher_filter: "10.1016/%"
              batch_size: "5000"
              max_workers: "150"
              dry_run: "false"
          job_cluster_key: maintenance_cluster
          timeout_seconds: 86400  # 24 hours (larger dataset)
      job_clusters:
        - job_cluster_key: maintenance_cluster
          new_cluster:
            spark_version: 16.4.x-scala2.12
            aws_attributes:
              first_on_demand: 1
              availability: SPOT_WITH_FALLBACK
              zone_id: us-east-1f
              spot_bid_price_percent: 100
            node_type_id: m5.xlarge
            driver_node_type_id: m5.xlarge
            enable_elastic_disk: true
            data_security_mode: USER_ISOLATION
            runtime_engine: STANDARD
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
      git_source:
        git_url: https://github.com/ourresearch/openalex-walden.git
        git_provider: gitHub
        git_branch: main
      parameters:
        - name: filter_mode
          default: "before_cutoff"
        - name: cutoff_date
          default: "2025-12-01"
        - name: start_date
          default: "2025-12-27"
        - name: end_date
          default: "2026-01-03"
        - name: publisher_filter
          default: "10.1016/%"
        - name: batch_size
          default: "5000"
        - name: max_workers
          default: "150"
        - name: dry_run
          default: "false"
